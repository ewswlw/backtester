{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import vectorbt as vbt\n",
    "import io\n",
    "import sys\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "import math\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras not available, LSTM model will be skipped\n",
      "PyWavelets not available, wavelet features will be skipped\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import Ridge, ElasticNet, Lasso, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Deep learning imports - using keras directly instead of tensorflow.keras\n",
    "try:\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense, Dropout\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    KERAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    KERAS_AVAILABLE = False\n",
    "    print(\"Keras not available, LSTM model will be skipped\")\n",
    "\n",
    "# Optional wavelet transform\n",
    "try:\n",
    "    import pywt\n",
    "    WAVELETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WAVELETS_AVAILABLE = False\n",
    "    print(\"PyWavelets not available, wavelet features will be skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Functions\n",
    "\n",
    "def calculate_rsi(series, window=14):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    print(f\"  Calculating RSI with {window}-day window for {series.name}\")\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "\n",
    "    avg_gain = gain.rolling(window).mean()\n",
    "    avg_loss = loss.rolling(window).mean()\n",
    "\n",
    "    rs = avg_gain / avg_loss.replace(0, 1e-9)  # Avoid division by zero\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def create_enhanced_features(df):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### ENHANCED FEATURE ENGINEERING ###\")\n",
    "    print(\"=\"*80)\n",
    "    original_cols = df.columns.tolist()\n",
    "    feature_count = len(df.columns)\n",
    "\n",
    "    # 1.1 Multi-timeframe features - expanded range\n",
    "    for window in [3, 5, 10, 15, 20, 30, 60, 90, 120, 250]:\n",
    "        print(f\"Creating {window}-day features...\")\n",
    "\n",
    "        # Moving averages of key indicators\n",
    "        for col in ['us_ig_oas', 'us_hy_oas', 'cad_oas', 'vix']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_ma{window}'] = df[col].rolling(window).mean()\n",
    "\n",
    "        # Momentum indicators\n",
    "        for col in ['us_ig_oas', 'us_hy_oas', 'cad_oas', 'cad_ig_er_index', 'us_ig_er_index']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_mom{window}'] = df[col].pct_change(window)\n",
    "\n",
    "        # Volatility features\n",
    "        if 'cad_ig_er_index' in df.columns:\n",
    "            df[f'er_vol_{window}d'] = df['cad_ig_er_index'].pct_change().rolling(window).std()\n",
    "        if 'us_ig_oas' in df.columns:\n",
    "            df[f'oas_vol_{window}d'] = df['us_ig_oas'].pct_change().rolling(window).std()\n",
    "\n",
    "        # Spread ratios and differences\n",
    "        if window >= 10:\n",
    "            if all(col in df.columns for col in ['us_hy_oas', 'us_ig_oas']):\n",
    "                ma_cols = [f'{col}_ma{window}' for col in ['us_hy_oas', 'us_ig_oas']]\n",
    "                if all(col in df.columns for col in ma_cols):\n",
    "                    df[f'hy_ig_oas_ratio_{window}d'] = df[f'us_hy_oas_ma{window}'] / df[f'us_ig_oas_ma{window}']\n",
    "\n",
    "            if all(col in df.columns for col in ['cad_oas', 'us_ig_oas']):\n",
    "                ma_cols = [f'{col}_ma{window}' for col in ['cad_oas', 'us_ig_oas']]\n",
    "                if all(col in df.columns for col in ma_cols):\n",
    "                    df[f'cad_us_oas_diff_{window}d'] = df[f'cad_oas_ma{window}'] - df[f'us_ig_oas_ma{window}']\n",
    "\n",
    "    # 1.2 Rate environment features\n",
    "    print(\"Creating enhanced rate environment features...\")\n",
    "    if 'us_3m_10y' in df.columns:\n",
    "        df['curve_slope'] = df['us_3m_10y']\n",
    "        df['curve_slope_change_20d'] = df['curve_slope'].diff(20)\n",
    "        df['curve_slope_change_60d'] = df['curve_slope'].diff(60)\n",
    "\n",
    "        # Slope momentum and acceleration\n",
    "        df['curve_slope_mom'] = df['curve_slope'].pct_change(20)\n",
    "        df['curve_slope_accel'] = df['curve_slope_mom'].diff(20)\n",
    "\n",
    "        # Curve regimes\n",
    "        df['flat_curve_regime'] = (df['curve_slope'].abs() < 0.5).astype(int)\n",
    "        df['steep_curve_regime'] = (df['curve_slope'] > 1.0).astype(int)\n",
    "        df['inverted_curve_regime'] = (df['curve_slope'] < 0).astype(int)\n",
    "\n",
    "    # 1.3 Market regime indicators\n",
    "    print(\"Creating market regime indicators...\")\n",
    "    # Volatility regimes\n",
    "    if 'vix' in df.columns:\n",
    "        df['vix_ma60'] = df['vix'].rolling(60).mean()\n",
    "        df['high_vol_regime'] = (df['vix'] > df['vix_ma60']).astype(int)\n",
    "        df['extreme_vol_regime'] = (df['vix'] > df['vix_ma60'] * 1.5).astype(int)\n",
    "        df['vol_momentum'] = (df['vix'].pct_change(20) > 0).astype(int)\n",
    "\n",
    "    # Credit spread regimes\n",
    "    if 'us_ig_oas' in df.columns:\n",
    "        df['ig_oas_ma60'] = df['us_ig_oas'].rolling(60).mean()\n",
    "        df['widening_spreads'] = (df['us_ig_oas'] > df['ig_oas_ma60']).astype(int)\n",
    "        df['oas_momentum'] = (df['us_ig_oas'].pct_change(20) > 0).astype(int)\n",
    "\n",
    "    # Combined regimes\n",
    "    if all(col in df.columns for col in ['high_vol_regime', 'widening_spreads']):\n",
    "        df['vol_spread_regime'] = df['high_vol_regime'] * df['widening_spreads']\n",
    "\n",
    "    if all(col in df.columns for col in ['inverted_curve_regime', 'high_vol_regime']):\n",
    "        df['yield_vol_regime'] = df['inverted_curve_regime'] * df['high_vol_regime']\n",
    "\n",
    "    # 1.4 Technical indicators\n",
    "    print(\"Creating technical indicators...\")\n",
    "    # RSI for multiple timeframes\n",
    "    for window in [14, 30, 60]:\n",
    "        if 'cad_ig_er_index' in df.columns:\n",
    "            df[f'cad_er_rsi_{window}'] = calculate_rsi(df['cad_ig_er_index'], window)\n",
    "        if 'us_ig_er_index' in df.columns:\n",
    "            df[f'us_er_rsi_{window}'] = calculate_rsi(df['us_ig_er_index'], window)\n",
    "        if 'us_ig_oas' in df.columns:\n",
    "            df[f'oas_rsi_{window}'] = calculate_rsi(df['us_ig_oas'], window)\n",
    "\n",
    "    # 1.5 Lagged features\n",
    "    print(\"Creating lagged relationships...\")\n",
    "    for lag in [1, 2, 3, 5]:\n",
    "        for col in ['us_ig_oas', 'vix', 'us_ig_er_index', 'cad_ig_er_index']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "                df[f'{col}_pct_lag{lag}'] = df[col].pct_change(lag).shift(1)\n",
    "\n",
    "    # 1.6 Non-linear transformations and interactions\n",
    "    print(\"Creating non-linear transformations and interactions...\")\n",
    "    for col in ['us_ig_oas', 'us_hy_oas', 'cad_oas', 'vix']:\n",
    "        if col in df.columns and df[col].min() > 0:\n",
    "            df[f'{col}_log'] = np.log(df[col])\n",
    "            df[f'{col}_sqrt'] = np.sqrt(df[col])\n",
    "            df[f'{col}_squared'] = df[col] ** 2\n",
    "\n",
    "    # Cross-asset interactions\n",
    "    if all(col in df.columns for col in ['us_ig_oas', 'vix']):\n",
    "        df['oas_vix_interaction'] = df['us_ig_oas'] * df['vix']\n",
    "\n",
    "    if all(col in df.columns for col in ['us_ig_oas', 'curve_slope']):\n",
    "        df['oas_slope_interaction'] = df['us_ig_oas'] * df['curve_slope']\n",
    "\n",
    "    if all(col in df.columns for col in ['vix', 'curve_slope']):\n",
    "        df['vix_slope_interaction'] = df['vix'] * df['curve_slope']\n",
    "\n",
    "    # Recent trend changes vs longer-term trends\n",
    "    if all(col in df.columns for col in ['us_ig_oas_mom10', 'us_ig_oas_mom60']):\n",
    "        df['oas_trend_divergence'] = df['us_ig_oas_mom10'] - df['us_ig_oas_mom60']\n",
    "\n",
    "    if all(col in df.columns for col in ['cad_ig_er_index_mom10', 'cad_ig_er_index_mom60']):\n",
    "        df['er_trend_divergence'] = df['cad_ig_er_index_mom10'] - df['cad_ig_er_index_mom60']\n",
    "\n",
    "    # 1.7 Z-scores and normalized indicators\n",
    "    print(\"Creating z-score features...\")\n",
    "    for col in ['us_ig_oas', 'us_hy_oas', 'vix', 'curve_slope']:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_zscore_60d'] = (df[col] - df[col].rolling(60).mean()) / df[col].rolling(60).std().replace(0, 1e-9)\n",
    "            df[f'{col}_zscore_250d'] = (df[col] - df[col].rolling(250).mean()) / df[col].rolling(250).std().replace(0, 1e-9)\n",
    "\n",
    "    # 1.8 Economic indicator interactions (if available)\n",
    "    if 'us_economic_regime' in df.columns:\n",
    "        print(\"Creating economic regime interactions...\")\n",
    "        if 'us_ig_oas' in df.columns:\n",
    "            df['regime_oas'] = df['us_economic_regime'] * df['us_ig_oas']\n",
    "\n",
    "        if 'curve_slope' in df.columns:\n",
    "            df['regime_curve'] = df['us_economic_regime'] * df['curve_slope']\n",
    "\n",
    "        if 'us_growth_surprises' in df.columns and 'us_ig_oas' in df.columns:\n",
    "            df['growth_surprise_oas'] = df['us_growth_surprises'] * df['us_ig_oas']\n",
    "\n",
    "            if 'vix' in df.columns:\n",
    "                df['surprise_vol_interaction'] = df['us_growth_surprises'] * df['vix']\n",
    "\n",
    "        if 'us_inflation_surprises' in df.columns and 'us_ig_oas' in df.columns:\n",
    "            df['inflation_surprise_oas'] = df['us_inflation_surprises'] * df['us_ig_oas']\n",
    "\n",
    "            if 'curve_slope' in df.columns:\n",
    "                df['inflation_curve_interaction'] = df['us_inflation_surprises'] * df['curve_slope']\n",
    "\n",
    "    # 1.9 NEW: Autocorrelation features\n",
    "    print(\"Creating autocorrelation features...\")\n",
    "    for col in ['us_ig_oas', 'us_hy_oas', 'cad_oas', 'cad_ig_er_index']:\n",
    "        if col in df.columns:\n",
    "            for lag in [5, 10, 20]:\n",
    "                # Calculate autocorrelation on rolling window\n",
    "                try:\n",
    "                    df[f'{col}_autocorr_{lag}'] = df[col].rolling(window=lag*2).apply(\n",
    "                        lambda x: x.autocorr(lag=lag) if len(x.dropna()) > lag else np.nan,\n",
    "                        raw=False\n",
    "                    )\n",
    "                    print(f\"  Created autocorrelation feature: {col}_autocorr_{lag}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error creating autocorrelation for {col}_autocorr_{lag}: {e}\")\n",
    "\n",
    "    # 1.10 NEW: Exponentially weighted features\n",
    "    print(\"Creating exponentially weighted features...\")\n",
    "    for span in [10, 30, 60]:\n",
    "        for col in ['us_ig_oas', 'us_hy_oas', 'vix', 'curve_slope']:\n",
    "            if col in df.columns:\n",
    "                # EWM means and standard deviations\n",
    "                df[f'{col}_ewm{span}'] = df[col].ewm(span=span).mean()\n",
    "                df[f'{col}_ewm{span}_std'] = df[col].ewm(span=span).std()\n",
    "\n",
    "                # EWM-based signals\n",
    "                if span == 10:  # Only for the fast EWM\n",
    "                    df[f'{col}_ewm_signal'] = (df[col] > df[f'{col}_ewm{span}']).astype(float)\n",
    "\n",
    "        # Cross-EWM signals (fast vs slow)\n",
    "        if span == 60:  # Only for the slow EWM\n",
    "            for col in ['us_ig_oas', 'us_hy_oas', 'vix', 'curve_slope']:\n",
    "                if col in df.columns and f'{col}_ewm10' in df.columns and f'{col}_ewm{span}' in df.columns:\n",
    "                    # Crossover signals\n",
    "                    df[f'{col}_ewm_crossover'] = (df[f'{col}_ewm10'] > df[f'{col}_ewm{span}']).astype(float)\n",
    "\n",
    "                    # Divergence strength\n",
    "                    df[f'{col}_ewm_divergence'] = (\n",
    "                        (df[f'{col}_ewm10'] - df[f'{col}_ewm{span}']) / df[f'{col}_ewm{span}']\n",
    "                    )\n",
    "\n",
    "    # 1.11 NEW: Wavelet transform features (if available)\n",
    "    if WAVELETS_AVAILABLE:\n",
    "        print(\"Creating wavelet transform features...\")\n",
    "        for col in ['us_ig_oas', 'vix', 'cad_ig_er_index']:\n",
    "            if col in df.columns:\n",
    "                # Ensure no NaNs for wavelet transform\n",
    "                series = df[col].fillna(method='ffill').fillna(method='bfill').values\n",
    "\n",
    "                # Apply wavelet transform\n",
    "                try:\n",
    "                    # Use db4 wavelet with 3 levels of decomposition\n",
    "                    coeffs = pywt.wavedec(series, 'db4', level=3)\n",
    "\n",
    "                    # Reconstruct approximation and detail coefficients\n",
    "                    for i, coef in enumerate(coeffs):\n",
    "                        if i == 0:\n",
    "                            # Approximation coefficients at the highest level\n",
    "                            df[f'{col}_wavelet_a{i}'] = pywt.upcoef('a', coef, 'db4', level=3, take=len(series))\n",
    "                        else:\n",
    "                            # Detail coefficients at each level\n",
    "                            df[f'{col}_wavelet_d{i}'] = pywt.upcoef('d', coef, 'db4', level=4-i, take=len(series))\n",
    "\n",
    "                    print(f\"  Created wavelet features for {col}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Warning: Wavelet transform failed for {col}: {e}\")\n",
    "\n",
    "    new_features = len(df.columns) - feature_count\n",
    "    print(f\"\\nFeature engineering complete: {new_features} new features created\")\n",
    "    print(f\"Original features: {feature_count}, Total features now: {len(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def create_return_targets(df):\n",
    "    \"\"\"Create return target variables for multiple horizons\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### TARGET ENGINEERING ###\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Creating return target variables...\")\n",
    "\n",
    "    # Multiple horizon returns\n",
    "    for horizon in [1, 3, 5, 10, 20, 30, 60]:\n",
    "        # Forward returns\n",
    "        if 'cad_ig_er_index' in df.columns:\n",
    "            df[f'target_return_{horizon}d'] = df['cad_ig_er_index'].pct_change(periods=horizon).shift(-horizon)\n",
    "\n",
    "            # Log statistics\n",
    "            valid_data = df[f'target_return_{horizon}d'].dropna()\n",
    "            print(f\"  Target {horizon}-day return:\")\n",
    "            print(f\"    Mean: {valid_data.mean():.6f}\")\n",
    "            print(f\"    Std Dev: {valid_data.std():.6f}\")\n",
    "            print(f\"    Min: {valid_data.min():.6f}\")\n",
    "            print(f\"    Max: {valid_data.max():.6f}\")\n",
    "            print(f\"    Positive returns: {valid_data.gt(0).mean()*100:.1f}%\")\n",
    "            print(f\"    Negative returns: {valid_data.lt(0).mean()*100:.1f}%\")\n",
    "        else:\n",
    "            print(f\"ERROR: Cannot create return targets - 'cad_ig_er_index' not found in dataframe\")\n",
    "\n",
    "    # Target correlations\n",
    "    return_targets = [col for col in df.columns if col.startswith('target_return_')]\n",
    "    if len(return_targets) > 1:\n",
    "        target_corr = df[return_targets].corr()\n",
    "        print(\"\\nTarget return correlation matrix:\")\n",
    "        print(target_corr.round(2))\n",
    "\n",
    "        # Print correlation between short and long-term returns\n",
    "        if 'target_return_5d' in df.columns and 'target_return_20d' in df.columns:\n",
    "            corr_5d_20d = df['target_return_5d'].corr(df['target_return_20d'])\n",
    "            print(f\"\\nCorrelation between 5-day and 20-day returns: {corr_5d_20d:.4f}\")\n",
    "\n",
    "        if 'target_return_5d' in df.columns and 'target_return_60d' in df.columns:\n",
    "            corr_5d_60d = df['target_return_5d'].corr(df['target_return_60d'])\n",
    "            print(f\"Correlation between 5-day and 60-day returns: {corr_5d_60d:.4f}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def stable_feature_selection(X, y, n_iterations=50, sample_fraction=0.75, top_n=40):\n",
    "    \"\"\"\n",
    "    Stability-based feature selection using multiple iterations of Lasso\n",
    "    with subsampling for more robust selection.\n",
    "    \"\"\"\n",
    "    print(f\"Performing stability-based feature selection with {n_iterations} iterations...\")\n",
    "    print(f\"  Sample fraction: {sample_fraction}, Target top features: {top_n}\")\n",
    "    feature_importance = np.zeros(X.shape[1])\n",
    "\n",
    "    # Track both feature importance and selection frequency\n",
    "    selection_freq = np.zeros(X.shape[1])\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        if i % 10 == 0 or i == n_iterations - 1:\n",
    "            print(f\"  Iteration {i+1}/{n_iterations}...\")\n",
    "\n",
    "        # Sample data\n",
    "        sample_idx = np.random.choice(\n",
    "            np.arange(X.shape[0]),\n",
    "            size=int(X.shape[0] * sample_fraction),\n",
    "            replace=False\n",
    "        )\n",
    "        X_sample, y_sample = X.iloc[sample_idx], y.iloc[sample_idx]\n",
    "\n",
    "        # Fit model - try both Lasso and Random Forest for diversity\n",
    "        if i % 2 == 0:\n",
    "            # Lasso with cross-validation\n",
    "            try:\n",
    "                model = LassoCV(cv=5, random_state=i, max_iter=10000).fit(X_sample, y_sample)\n",
    "                importance = np.abs(model.coef_)\n",
    "                selected = importance > 0\n",
    "                print(f\"    Lasso selected {selected.sum()} features\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error in Lasso: {e}\")\n",
    "                # Fallback to Ridge which always works\n",
    "                model = Ridge(alpha=1.0, random_state=i).fit(X_sample, y_sample)\n",
    "                importance = np.abs(model.coef_)\n",
    "                # Select top 30% of features\n",
    "                threshold = np.percentile(importance, 70)\n",
    "                selected = importance > threshold\n",
    "                print(f\"    Fallback to Ridge - selected {selected.sum()} features\")\n",
    "        else:\n",
    "            # Random Forest\n",
    "            try:\n",
    "                model = RandomForestRegressor(\n",
    "                    n_estimators=100, max_depth=5, random_state=i).fit(X_sample, y_sample)\n",
    "                importance = model.feature_importances_\n",
    "                # Select top 50% of features from RF\n",
    "                threshold = np.percentile(importance, 50)\n",
    "                selected = importance > threshold\n",
    "                print(f\"    Random Forest selected {selected.sum()} features\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error in Random Forest: {e}\")\n",
    "                # Fallback to Ridge\n",
    "                model = Ridge(alpha=1.0, random_state=i).fit(X_sample, y_sample)\n",
    "                importance = np.abs(model.coef_)\n",
    "                threshold = np.percentile(importance, 70)\n",
    "                selected = importance > threshold\n",
    "                print(f\"    Fallback to Ridge - selected {selected.sum()} features\")\n",
    "\n",
    "        # Update importance and frequency\n",
    "        feature_importance += importance / n_iterations\n",
    "        selection_freq += selected / n_iterations\n",
    "\n",
    "    # Combine importance and frequency scores\n",
    "    combined_score = feature_importance * np.sqrt(selection_freq)\n",
    "\n",
    "    # Create feature ranking\n",
    "    feature_ranks = pd.Series(combined_score, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "    # Print selection frequency statistics\n",
    "    selection_freq_series = pd.Series(selection_freq, index=X.columns)\n",
    "    selection_stats = selection_freq_series.describe()\n",
    "    print(\"\\nFeature selection frequency statistics:\")\n",
    "    print(f\"  Mean selection rate: {selection_stats['mean']:.4f}\")\n",
    "    print(f\"  Min selection rate: {selection_stats['min']:.4f}\")\n",
    "    print(f\"  Max selection rate: {selection_stats['max']:.4f}\")\n",
    "    print(f\"  25th percentile: {selection_stats['25%']:.4f}\")\n",
    "    print(f\"  50th percentile: {selection_stats['50%']:.4f}\")\n",
    "    print(f\"  75th percentile: {selection_stats['75%']:.4f}\")\n",
    "\n",
    "    # Print top features\n",
    "    print(\"\\nTop 15 selected features:\")\n",
    "    for i, (feature, score) in enumerate(feature_ranks.head(15).items()):\n",
    "        freq = selection_freq[X.columns.get_loc(feature)] * 100\n",
    "        print(f\"  {i+1}. {feature}: score={score:.6f}, selected in {freq:.1f}% of iterations\")\n",
    "\n",
    "    return feature_ranks.head(top_n).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regime Identification\n",
    "\n",
    "def identify_improved_regimes(df, n_regimes=3):\n",
    "    \"\"\"\n",
    "    Identify market regimes using Gaussian Mixture Model\n",
    "    with smoothed transitions between regimes.\n",
    "    \"\"\"\n",
    "    print(\"Identifying market regimes using Gaussian Mixture Model...\")\n",
    "    print(f\"  Number of regimes to identify: {n_regimes}\")\n",
    "\n",
    "    # 1. Create regime-specific features\n",
    "    regime_features = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Extract key indicators and their rates of change\n",
    "    key_indicators = ['vix', 'us_ig_oas', 'curve_slope']\n",
    "    windows = [20, 60]\n",
    "\n",
    "    # Calculate moving averages and momentum for key indicators\n",
    "    for indicator in key_indicators:\n",
    "        if indicator in df.columns:\n",
    "            # Moving averages for different timescales\n",
    "            for window in windows:\n",
    "                regime_features[f'{indicator}_ma{window}'] = df[indicator].rolling(window).mean()\n",
    "                print(f\"  Created feature: {indicator}_ma{window}\")\n",
    "\n",
    "            # Add momentum indicators\n",
    "            regime_features[f'{indicator}_mom20'] = df[indicator].pct_change(20)\n",
    "            print(f\"  Created feature: {indicator}_mom20\")\n",
    "\n",
    "            # Add volatility\n",
    "            regime_features[f'{indicator}_vol20'] = df[indicator].rolling(20).std()\n",
    "            print(f\"  Created feature: {indicator}_vol20\")\n",
    "\n",
    "    # Add specific regime indicators\n",
    "    if 'curve_slope' in df.columns:\n",
    "        # Curve steepness/flatness\n",
    "        regime_features['curve_steepness'] = df['curve_slope']\n",
    "        print(\"  Added curve steepness feature\")\n",
    "\n",
    "    # Risk aversion indicator - simplified calculation to avoid indentation issues\n",
    "    if 'vix' in df.columns and 'us_ig_oas' in df.columns:\n",
    "        vix_ratio = df['vix'].rolling(30).mean() / df['vix'].rolling(252).mean()\n",
    "        oas_ratio = df['us_ig_oas'].rolling(30).mean() / df['us_ig_oas'].rolling(252).mean()\n",
    "        regime_features['risk_aversion'] = (vix_ratio + oas_ratio) / 2\n",
    "        print(\"  Added risk aversion feature\")\n",
    "\n",
    "    # Fill missing values for clustering\n",
    "    regime_features = regime_features.fillna(method='bfill').fillna(method='ffill')\n",
    "    print(f\"  Created {len(regime_features.columns)} features for regime identification\")\n",
    "\n",
    "    # 2. Apply dimensionality reduction to regime features\n",
    "    print(\"Applying PCA for dimensionality reduction...\")\n",
    "    regime_features_clean = regime_features.dropna()\n",
    "    print(f\"  {len(regime_features_clean)} valid samples for regime identification\")\n",
    "\n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    regime_data_scaled = scaler.fit_transform(regime_features_clean)\n",
    "\n",
    "    # Apply PCA to reduce dimensionality\n",
    "    n_components = min(5, regime_data_scaled.shape[1])\n",
    "    pca = PCA(n_components=n_components)\n",
    "    regime_data_pca = pca.fit_transform(regime_data_scaled)\n",
    "\n",
    "    # Print explained variance\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(f\"  PCA explained variance ratios: {explained_variance.round(3)}\")\n",
    "    print(f\"  Total explained variance: {explained_variance.sum():.2%}\")\n",
    "    print(f\"  Reduced dimensionality from {regime_data_scaled.shape[1]} to {n_components} features\")\n",
    "\n",
    "    # 3. Apply Gaussian Mixture Model for regime identification\n",
    "    print(f\"Fitting Gaussian Mixture Model with {n_regimes} components...\")\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=n_regimes,\n",
    "        covariance_type='full',\n",
    "        random_state=42,\n",
    "        n_init=10\n",
    "    )\n",
    "\n",
    "    # Fit the model and predict regimes\n",
    "    gmm_regimes = gmm.fit_predict(regime_data_pca)\n",
    "    regime_probs = gmm.predict_proba(regime_data_pca)\n",
    "\n",
    "    # Get BIC and AIC\n",
    "    bic = gmm.bic(regime_data_pca)\n",
    "    aic = gmm.aic(regime_data_pca)\n",
    "    print(f\"  Model fit statistics: BIC={bic:.2f}, AIC={aic:.2f}\")\n",
    "\n",
    "    # 4. Create results dataframe\n",
    "    regime_df = pd.DataFrame(index=regime_features_clean.index)\n",
    "    regime_df['market_regime'] = gmm_regimes\n",
    "\n",
    "    # Add regime probabilities\n",
    "    for i in range(n_regimes):\n",
    "        regime_df[f'regime_{i}_prob'] = regime_probs[:, i]\n",
    "\n",
    "    # Add regime labels based on characteristics\n",
    "    regime_labels = {}\n",
    "\n",
    "    # Determine regime labels based on average characteristics\n",
    "    # Higher OAS/VIX = stressed, Lower OAS/VIX = favorable\n",
    "    regime_stats = {}\n",
    "\n",
    "    for i in range(n_regimes):\n",
    "        regime_mask = (regime_df['market_regime'] == i)\n",
    "        regime_indices = regime_df.index[regime_mask]\n",
    "\n",
    "        print(f\"\\nAnalyzing characteristics of Regime {i}:\")\n",
    "        print(f\"  Samples in regime: {regime_mask.sum()} ({regime_mask.mean()*100:.1f}%)\")\n",
    "\n",
    "        # Calculate average values\n",
    "        vix_avg = df.loc[regime_indices, 'vix'].mean() if 'vix' in df.columns else 0\n",
    "        oas_avg = df.loc[regime_indices, 'us_ig_oas'].mean() if 'us_ig_oas' in df.columns else 0\n",
    "        curve_avg = df.loc[regime_indices, 'curve_slope'].mean() if 'curve_slope' in df.columns else 0\n",
    "\n",
    "        regime_stats[i] = {\n",
    "            'vix_avg': vix_avg,\n",
    "            'oas_avg': oas_avg,\n",
    "            'curve_avg': curve_avg,\n",
    "            'count': regime_mask.sum(),\n",
    "            'pct': regime_mask.mean()\n",
    "        }\n",
    "\n",
    "        print(f\"  Average VIX: {vix_avg:.2f}\")\n",
    "        print(f\"  Average OAS: {oas_avg:.2f}\")\n",
    "        print(f\"  Average Curve Slope: {curve_avg:.2f}\")\n",
    "\n",
    "    # Sort regimes by stress level (OAS + VIX)\n",
    "    stress_levels = {i: stats['oas_avg'] + stats['vix_avg']/10 for i, stats in regime_stats.items()}\n",
    "    sorted_regimes = sorted(stress_levels.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Assign labels based on sorted stress\n",
    "    regime_labels = {}\n",
    "    regime_labels[sorted_regimes[0][0]] = 'favorable'\n",
    "    regime_labels[sorted_regimes[-1][0]] = 'stressed'\n",
    "\n",
    "    # Middle regimes get 'neutral' label\n",
    "    for i in range(1, len(sorted_regimes) - 1):\n",
    "        regime_labels[sorted_regimes[i][0]] = 'neutral'\n",
    "\n",
    "    # Map numeric regimes to labels\n",
    "    regime_df['regime_label'] = regime_df['market_regime'].map(regime_labels)\n",
    "\n",
    "    print(\"\\nRegime label mapping:\")\n",
    "    for i, label in regime_labels.items():\n",
    "        print(f\"  Regime {i} -> {label}\")\n",
    "\n",
    "    # 5. Add smoothed regime indicators\n",
    "    # Calculate exponentially weighted probabilities for smoother transitions\n",
    "    for i in range(n_regimes):\n",
    "        regime_df[f'regime_{i}_prob_smooth'] = regime_df[f'regime_{i}_prob'].ewm(span=5).mean()\n",
    "\n",
    "    # Calculate regime transition indicators\n",
    "    regime_df['regime_change'] = regime_df['market_regime'].diff().abs() > 0\n",
    "    regime_df['days_in_regime'] = regime_df['regime_change'].cumsum()\n",
    "    regime_df['days_since_change'] = regime_df.groupby('days_in_regime')['regime_change'].cumcount()\n",
    "\n",
    "    # 6. Get overall regime statistics\n",
    "    print(\"\\nOverall regime distribution:\")\n",
    "    regime_counts = regime_df['regime_label'].value_counts(normalize=True)\n",
    "    for label, pct in regime_counts.items():\n",
    "        print(f\"  {label}: {pct*100:.1f}%\")\n",
    "\n",
    "    # 7. Fill regime date mapping for test/train splits\n",
    "    # Fill missing dates with the most common regime\n",
    "    most_common_regime = regime_df['market_regime'].mode()[0]\n",
    "    all_regime_df = pd.DataFrame(index=df.index)\n",
    "    all_regime_df['market_regime'] = most_common_regime\n",
    "    all_regime_df.loc[regime_df.index, 'market_regime'] = regime_df['market_regime']\n",
    "    all_regime_df['regime_label'] = all_regime_df['market_regime'].map(regime_labels)\n",
    "\n",
    "    for i in range(n_regimes):\n",
    "        if f'regime_{i}_prob' in regime_df.columns:\n",
    "            all_regime_df[f'regime_{i}_prob'] = 0.0\n",
    "            all_regime_df.loc[regime_df.index, f'regime_{i}_prob'] = regime_df[f'regime_{i}_prob']\n",
    "\n",
    "    # Print information about filled values\n",
    "    filled_count = len(all_regime_df) - len(regime_df)\n",
    "    if filled_count > 0:\n",
    "        print(f\"\\nFilled {filled_count} missing dates with most common regime ({most_common_regime})\")\n",
    "\n",
    "    return all_regime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building Functions\n",
    "\n",
    "def build_traditional_models(X_train, y_train):\n",
    "    \"\"\"Build traditional regression models\"\"\"\n",
    "    print(\"Training traditional models...\")\n",
    "\n",
    "    models = {}\n",
    "    model_params = {\n",
    "        'ridge': {\n",
    "            'alpha': 1.0,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        'elastic': {\n",
    "            'alpha': 0.01,\n",
    "            'l1_ratio': 0.5,\n",
    "            'max_iter': 10000,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        'rf': {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 4,\n",
    "            'min_samples_leaf': 10,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        'gbr': {\n",
    "            'n_estimators': 100,\n",
    "            'learning_rate': 0.01,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.8,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        'xgb': {\n",
    "            'n_estimators': 100,\n",
    "            'learning_rate': 0.01,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Ridge regression\n",
    "    print(\"  Training Ridge regression...\")\n",
    "    models['ridge'] = Ridge(**model_params['ridge'])\n",
    "    models['ridge'].fit(X_train, y_train)\n",
    "\n",
    "    # Print coefficients\n",
    "    ridge_coefs = pd.Series(models['ridge'].coef_, index=X_train.columns)\n",
    "    top_ridge_coefs = ridge_coefs.abs().sort_values(ascending=False)\n",
    "    print(f\"    Top 5 Ridge coefficients (absolute):\")\n",
    "    for i, (feat, coef) in enumerate(top_ridge_coefs.head(5).items()):\n",
    "        actual_coef = ridge_coefs[feat]\n",
    "        print(f\"      {feat}: {actual_coef:.6f}\")\n",
    "\n",
    "    # ElasticNet\n",
    "    print(\"  Training ElasticNet...\")\n",
    "    models['elastic'] = ElasticNet(**model_params['elastic'])\n",
    "    models['elastic'].fit(X_train, y_train)\n",
    "\n",
    "    # Print non-zero coefficients\n",
    "    elastic_coefs = pd.Series(models['elastic'].coef_, index=X_train.columns)\n",
    "    nonzero_coefs = elastic_coefs[elastic_coefs != 0]\n",
    "    print(f\"    ElasticNet selected {len(nonzero_coefs)} non-zero features\")\n",
    "    if len(nonzero_coefs) > 0:\n",
    "        top_elastic_coefs = nonzero_coefs.abs().sort_values(ascending=False)\n",
    "        print(f\"    Top 5 ElasticNet coefficients (absolute):\")\n",
    "        for i, (feat, coef) in enumerate(top_elastic_coefs.head(5).items()):\n",
    "            actual_coef = elastic_coefs[feat]\n",
    "            print(f\"      {feat}: {actual_coef:.6f}\")\n",
    "\n",
    "    # Random Forest\n",
    "    print(\"  Training Random Forest...\")\n",
    "    models['rf'] = RandomForestRegressor(**model_params['rf'])\n",
    "    models['rf'].fit(X_train, y_train)\n",
    "\n",
    "    # Print feature importance\n",
    "    rf_importance = pd.Series(models['rf'].feature_importances_, index=X_train.columns)\n",
    "    top_rf_importance = rf_importance.sort_values(ascending=False)\n",
    "    print(f\"    Top 5 Random Forest important features:\")\n",
    "    for i, (feat, imp) in enumerate(top_rf_importance.head(5).items()):\n",
    "        print(f\"      {feat}: {imp:.6f}\")\n",
    "\n",
    "    # Gradient Boosting\n",
    "    print(\"  Training Gradient Boosting...\")\n",
    "    models['gbr'] = GradientBoostingRegressor(**model_params['gbr'])\n",
    "    models['gbr'].fit(X_train, y_train)\n",
    "\n",
    "    # Print feature importance\n",
    "    gbr_importance = pd.Series(models['gbr'].feature_importances_, index=X_train.columns)\n",
    "    top_gbr_importance = gbr_importance.sort_values(ascending=False)\n",
    "    print(f\"    Top 5 Gradient Boosting important features:\")\n",
    "    for i, (feat, imp) in enumerate(top_gbr_importance.head(5).items()):\n",
    "        print(f\"      {feat}: {imp:.6f}\")\n",
    "\n",
    "    # XGBoost\n",
    "    print(\"  Training XGBoost...\")\n",
    "    models['xgb'] = XGBRegressor(**model_params['xgb'])\n",
    "    models['xgb'].fit(X_train, y_train)\n",
    "\n",
    "    # Print feature importance\n",
    "    xgb_importance = pd.Series(models['xgb'].feature_importances_, index=X_train.columns)\n",
    "    top_xgb_importance = xgb_importance.sort_values(ascending=False)\n",
    "    print(f\"    Top 5 XGBoost important features:\")\n",
    "    for i, (feat, imp) in enumerate(top_xgb_importance.head(5).items()):\n",
    "        print(f\"      {feat}: {imp:.6f}\")\n",
    "\n",
    "    print(f\"  Trained {len(models)} traditional models\")\n",
    "    return models\n",
    "\n",
    "def build_regime_aware_models(X_train, y_train, regimes_train):\n",
    "    \"\"\"Build regime-specific models\"\"\"\n",
    "    print(\"Training regime-specific models...\")\n",
    "\n",
    "    regime_models = {}\n",
    "\n",
    "    # For each regime, train specialized models\n",
    "    for regime in regimes_train['regime_label'].dropna().unique():\n",
    "        # Create mask for this regime\n",
    "        regime_mask = (regimes_train['regime_label'] == regime)\n",
    "        regime_count = regime_mask.sum()\n",
    "\n",
    "        if regime_count >= 100:  # Only train if we have sufficient data\n",
    "            print(f\"  Training models for '{regime}' regime ({regime_count} samples)...\")\n",
    "\n",
    "            # Get regime-specific data\n",
    "            X_regime = X_train[regime_mask]\n",
    "            y_regime = y_train[regime_mask]\n",
    "\n",
    "            # Ridge model for this regime\n",
    "            print(f\"    Training Ridge model for {regime} regime...\")\n",
    "            ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "            ridge_model.fit(X_regime, y_regime)\n",
    "            regime_models[f'ridge_regime_{regime}'] = ridge_model\n",
    "\n",
    "            # Print top coefficients\n",
    "            ridge_coefs = pd.Series(ridge_model.coef_, index=X_train.columns)\n",
    "            top_ridge_coefs = ridge_coefs.abs().sort_values(ascending=False)\n",
    "            print(f\"      Top 3 Ridge coefficients for {regime} regime:\")\n",
    "            for i, (feat, coef) in enumerate(top_ridge_coefs.head(3).items()):\n",
    "                actual_coef = ridge_coefs[feat]\n",
    "                print(f\"        {feat}: {actual_coef:.6f}\")\n",
    "\n",
    "            # Random Forest model for this regime\n",
    "            print(f\"    Training Random Forest model for {regime} regime...\")\n",
    "            rf_model = RandomForestRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=3,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=42\n",
    "            )\n",
    "            rf_model.fit(X_regime, y_regime)\n",
    "            regime_models[f'rf_regime_{regime}'] = rf_model\n",
    "\n",
    "            # Print feature importance\n",
    "            rf_importance = pd.Series(rf_model.feature_importances_, index=X_train.columns)\n",
    "            top_rf_importance = rf_importance.sort_values(ascending=False)\n",
    "            print(f\"      Top 3 Random Forest important features for {regime} regime:\")\n",
    "            for i, (feat, imp) in enumerate(top_rf_importance.head(3).items()):\n",
    "                print(f\"        {feat}: {imp:.6f}\")\n",
    "        else:\n",
    "            print(f\"  Skipping '{regime}' regime - insufficient samples ({regime_count} < 100)\")\n",
    "\n",
    "    print(f\"  Trained {len(regime_models)} regime-specific models\")\n",
    "    return regime_models\n",
    "\n",
    "def build_advanced_models(X_train, y_train):\n",
    "    \"\"\"Build advanced ensemble and neural network models\"\"\"\n",
    "    print(\"Training advanced models...\")\n",
    "\n",
    "    advanced_models = {}\n",
    "\n",
    "    # 1. Stacking ensemble\n",
    "    print(\"  Training stacking ensemble model...\")\n",
    "    base_models = [\n",
    "        ('ridge', Ridge(alpha=1.0, random_state=42)),\n",
    "        ('elastic', ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000, random_state=42)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=50, max_depth=3, random_state=42)),\n",
    "        ('gbr', GradientBoostingRegressor(n_estimators=50, learning_rate=0.01, random_state=42))\n",
    "    ]\n",
    "\n",
    "    meta_learner = Ridge(alpha=1.0)\n",
    "\n",
    "    stacking_model = StackingRegressor(\n",
    "        estimators=base_models,\n",
    "        final_estimator=meta_learner,\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        stacking_model.fit(X_train, y_train)\n",
    "        advanced_models['stacking'] = stacking_model\n",
    "        print(\"    Stacking ensemble model trained successfully\")\n",
    "\n",
    "        # Try to extract feature importance from the meta-learner\n",
    "        try:\n",
    "            if hasattr(meta_learner, 'coef_'):\n",
    "                meta_coefs = meta_learner.coef_\n",
    "                print(f\"    Meta-learner coefficients: {meta_coefs}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    Error training stacking model: {e}\")\n",
    "\n",
    "    # 2. Neural network model\n",
    "    print(\"  Training neural network model...\")\n",
    "    nn_model = MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.0001,\n",
    "        batch_size=32,\n",
    "        learning_rate='adaptive',\n",
    "        max_iter=1000,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        nn_model.fit(X_train, y_train)\n",
    "        advanced_models['nn'] = nn_model\n",
    "        print(f\"    Neural network model trained successfully\")\n",
    "        print(f\"    Training iterations: {nn_model.n_iter_}\")\n",
    "        print(f\"    Final loss: {nn_model.loss_:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Error training neural network model: {e}\")\n",
    "\n",
    "    # 3. SVR model\n",
    "    print(\"  Training SVR model...\")\n",
    "    svr_model = SVR(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        epsilon=0.1,\n",
    "        gamma='scale'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        svr_model.fit(X_train, y_train)\n",
    "        advanced_models['svr'] = svr_model\n",
    "        print(\"    SVR model trained successfully\")\n",
    "        print(f\"    Support vectors: {svr_model.support_vectors_.shape[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Error training SVR model: {e}\")\n",
    "\n",
    "    print(f\"  Trained {len(advanced_models)} advanced models\")\n",
    "    return advanced_models\n",
    "\n",
    "def build_lstm_model(X_train, y_train, lookback=20):\n",
    "    \"\"\"Build and train LSTM model for sequence prediction\"\"\"\n",
    "    if not KERAS_AVAILABLE:\n",
    "        print(\"Keras not available, skipping LSTM model\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Training LSTM model with {lookback}-day lookback window...\")\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X_train_values = X_train.values\n",
    "    y_train_values = y_train.values\n",
    "\n",
    "    # Check if we have enough data\n",
    "    if len(X_train_values) <= lookback:\n",
    "        print(\"  Not enough data for LSTM training\")\n",
    "        return None, None\n",
    "\n",
    "    # Create sequences for LSTM\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "\n",
    "    for i in range(len(X_train_values) - lookback):\n",
    "        X_sequences.append(X_train_values[i:i+lookback])\n",
    "        y_sequences.append(y_train_values[i+lookback])\n",
    "\n",
    "    X_sequences = np.array(X_sequences)\n",
    "    y_sequences = np.array(y_sequences)\n",
    "\n",
    "    print(f\"  LSTM input shape: {X_sequences.shape}\")\n",
    "    print(f\"  LSTM output shape: {y_sequences.shape}\")\n",
    "\n",
    "    # Define LSTM model\n",
    "    print(\"  Defining LSTM architecture...\")\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=(lookback, X_train.shape[1])),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Train with early stopping\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    try:\n",
    "        print(\"  Training LSTM model...\")\n",
    "        history = model.fit(\n",
    "            X_sequences, y_sequences,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        # Print training history\n",
    "        print(\"\\n  LSTM training history:\")\n",
    "        for i, (train_loss, val_loss) in enumerate(zip(history.history['loss'], history.history['val_loss'])):\n",
    "            if i % 10 == 0 or i == len(history.history['loss']) - 1:\n",
    "                print(f\"    Epoch {i+1}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}\")\n",
    "\n",
    "        print(\"  LSTM model training complete\")\n",
    "        return model, history\n",
    "    except Exception as e:\n",
    "        print(f\"  Error training LSTM model: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and Evaluation Functions\n",
    "\n",
    "def predict_with_lstm(model, X_test, lookback=20):\n",
    "    \"\"\"Generate predictions with LSTM model\"\"\"\n",
    "    if model is None:\n",
    "        return np.zeros(len(X_test))\n",
    "\n",
    "    print(f\"Generating predictions with LSTM model (lookback={lookback})...\")\n",
    "\n",
    "    # Convert to numpy\n",
    "    X_test_values = X_test.values\n",
    "\n",
    "    # Check if we have enough data\n",
    "    if len(X_test_values) <= lookback:\n",
    "        print(\"  Not enough test data for LSTM prediction\")\n",
    "        return np.zeros(len(X_test))\n",
    "\n",
    "    # Create sequences for prediction\n",
    "    X_sequences = []\n",
    "    for i in range(len(X_test_values) - lookback):\n",
    "        X_sequences.append(X_test_values[i:i+lookback])\n",
    "\n",
    "    X_sequences = np.array(X_sequences)\n",
    "    print(f\"  Test sequences shape: {X_sequences.shape}\")\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"  Making LSTM predictions...\")\n",
    "    predictions = model.predict(X_sequences, verbose=1)\n",
    "\n",
    "    # Print prediction statistics\n",
    "    print(f\"  Prediction statistics: mean={predictions.mean():.6f}, std={predictions.std():.6f}\")\n",
    "    print(f\"  Prediction range: [{predictions.min():.6f}, {predictions.max():.6f}]\")\n",
    "\n",
    "    # Pad with zeros for the first lookback points\n",
    "    padded_predictions = np.zeros(len(X_test))\n",
    "    padded_predictions[lookback:] = predictions.flatten()\n",
    "\n",
    "    return padded_predictions\n",
    "\n",
    "def create_regime_ensemble_predictions(models, X_test, regimes_test):\n",
    "    \"\"\"Create ensemble predictions using regime-specific models\"\"\"\n",
    "    print(\"Creating regime-based ensemble predictions...\")\n",
    "\n",
    "    # Initialize predictions\n",
    "    regime_preds = np.zeros(len(X_test))\n",
    "    used_regime_model = np.zeros(len(X_test), dtype=bool)\n",
    "\n",
    "    # Get unique regimes\n",
    "    unique_regimes = regimes_test['regime_label'].dropna().unique()\n",
    "    print(f\"  Test set contains {len(unique_regimes)} regimes: {', '.join(unique_regimes)}\")\n",
    "\n",
    "    # Track regime model usage\n",
    "    regime_model_usage = {regime: 0 for regime in unique_regimes}\n",
    "    total_samples = len(X_test)\n",
    "\n",
    "    # For each test sample, apply the appropriate regime model if available\n",
    "    for i, (idx, regime) in enumerate(zip(X_test.index, regimes_test['regime_label'])):\n",
    "        if pd.isna(regime):\n",
    "            continue\n",
    "\n",
    "        # Try ridge model first\n",
    "        ridge_regime_key = f'ridge_regime_{regime}'\n",
    "        if ridge_regime_key in models:\n",
    "            regime_preds[i] = models[ridge_regime_key].predict(X_test.iloc[[i]])[0]\n",
    "            used_regime_model[i] = True\n",
    "            regime_model_usage[regime] += 1\n",
    "        # Try RF model next\n",
    "        else:\n",
    "            rf_regime_key = f'rf_regime_{regime}'\n",
    "            if rf_regime_key in models:\n",
    "                regime_preds[i] = models[rf_regime_key].predict(X_test.iloc[[i]])[0]\n",
    "                used_regime_model[i] = True\n",
    "                regime_model_usage[regime] += 1\n",
    "\n",
    "    # Print regime model usage statistics\n",
    "    print(\"  Regime model usage:\")\n",
    "    for regime, count in regime_model_usage.items():\n",
    "        pct = (count / total_samples) * 100\n",
    "        print(f\"    {regime}: {count} samples ({pct:.1f}%)\")\n",
    "\n",
    "    # For samples where no regime model was available, use global ridge model\n",
    "    global_model_count = (~used_regime_model).sum()\n",
    "    if global_model_count > 0:\n",
    "        print(f\"  Using global Ridge model for {global_model_count} samples ({global_model_count/total_samples*100:.1f}%)\")\n",
    "        if 'ridge' in models:\n",
    "            regime_preds[~used_regime_model] = models['ridge'].predict(X_test.iloc[~used_regime_model])\n",
    "        else:\n",
    "            print(\"  WARNING: No global Ridge model available\")\n",
    "\n",
    "    # Print prediction statistics\n",
    "    print(f\"  Prediction statistics: mean={regime_preds.mean():.6f}, std={regime_preds.std():.6f}\")\n",
    "    print(f\"  Prediction range: [{regime_preds.min():.6f}, {regime_preds.max():.6f}]\")\n",
    "\n",
    "    return regime_preds\n",
    "\n",
    "def create_adaptive_ensemble(all_predictions, y_test_dict, horizons, best_models):\n",
    "    \"\"\"Create adaptive multi-horizon ensemble with dynamic weighting\"\"\"\n",
    "    print(\"Creating adaptive multi-horizon ensemble...\")\n",
    "\n",
    "    # Get first prediction length (they might differ due to LSTM)\n",
    "    first_horizon = horizons[0]\n",
    "    first_model = best_models[first_horizon]\n",
    "    pred_length = len(all_predictions[first_horizon][first_model])\n",
    "\n",
    "    # Initialize weights and predictions\n",
    "    model_weights = {}\n",
    "    total_weight = 0\n",
    "    ensemble_preds = np.zeros(pred_length)\n",
    "\n",
    "    # Calculate weights based on directional accuracy and horizon\n",
    "    print(\"\\nCalculating model weights based on performance:\")\n",
    "    for horizon in horizons:\n",
    "        model_name = best_models[horizon]\n",
    "        predictions = all_predictions[horizon][model_name]\n",
    "        print(f\"  Using {model_name} for {horizon}-day horizon\")\n",
    "\n",
    "        # Verification length (might be shorter for LSTM)\n",
    "        verify_length = min(len(predictions), len(y_test_dict[horizon]))\n",
    "\n",
    "        # Calculate directional accuracy (key metric for trading)\n",
    "        dir_acc = np.mean(\n",
    "            (y_test_dict[horizon][:verify_length] > 0) ==\n",
    "            (predictions[:verify_length] > 0)\n",
    "        )\n",
    "\n",
    "        # Weight by directional accuracy and inverse horizon\n",
    "        raw_weight = dir_acc * (1.0 / horizon)\n",
    "        model_weights[f'{horizon}d_{model_name}'] = raw_weight\n",
    "        total_weight += raw_weight\n",
    "\n",
    "        print(f\"    Directional accuracy: {dir_acc:.4f}\")\n",
    "        print(f\"    Raw weight: {raw_weight:.4f}\")\n",
    "\n",
    "        # Add weighted prediction\n",
    "        if verify_length < pred_length:\n",
    "            # Pad predictions if needed\n",
    "            padded_preds = np.zeros(pred_length)\n",
    "            padded_preds[:verify_length] = predictions[:verify_length]\n",
    "            ensemble_preds += padded_preds * raw_weight\n",
    "            print(f\"    Note: Padded predictions from length {verify_length} to {pred_length}\")\n",
    "        else:\n",
    "            ensemble_preds += predictions * raw_weight\n",
    "\n",
    "    # Normalize weights\n",
    "    for key in model_weights:\n",
    "        model_weights[key] /= total_weight\n",
    "\n",
    "    # Normalize ensemble predictions\n",
    "    ensemble_preds /= total_weight\n",
    "\n",
    "    # Print weights\n",
    "    print(\"\\nFinal model weights in ensemble:\")\n",
    "    for model, weight in sorted(model_weights.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {model}: {weight:.4f} ({weight*100:.1f}%)\")\n",
    "\n",
    "    # Print prediction statistics\n",
    "    print(f\"\\nEnsemble prediction statistics:\")\n",
    "    print(f\"  Mean: {ensemble_preds.mean():.6f}\")\n",
    "    print(f\"  Std Dev: {ensemble_preds.std():.6f}\")\n",
    "    print(f\"  Range: [{ensemble_preds.min():.6f}, {ensemble_preds.max():.6f}]\")\n",
    "    print(f\"  Positive predictions: {(ensemble_preds > 0).mean()*100:.1f}%\")\n",
    "\n",
    "    return ensemble_preds, model_weights\n",
    "\n",
    "def evaluate_model_detailed(y_true, y_pred, model_name):\n",
    "    \"\"\"Comprehensive model evaluation with detailed metrics\"\"\"\n",
    "    # Basic metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    # Directional accuracy\n",
    "    dir_acc = np.mean((y_true > 0) == (y_pred > 0))\n",
    "\n",
    "    # Separate directional accuracy for positive/negative actual returns\n",
    "    pos_mask = y_true > 0\n",
    "    neg_mask = y_true <= 0\n",
    "\n",
    "    pos_dir_acc = np.mean((y_pred[pos_mask] > 0)) if np.any(pos_mask) else np.nan\n",
    "    neg_dir_acc = np.mean((y_pred[neg_mask] <= 0)) if np.any(neg_mask) else np.nan\n",
    "\n",
    "    # Spearman rank correlation\n",
    "    spearman_corr, spearman_pval = spearmanr(y_true, y_pred)\n",
    "\n",
    "    # Profit simulation (simple strategy: long when prediction > 0, short when < 0)\n",
    "    signals = np.sign(y_pred)\n",
    "    returns = signals * y_true\n",
    "\n",
    "    # Profit factor: sum of positive returns divided by absolute sum of negative returns\n",
    "    pos_returns = returns[returns > 0].sum()\n",
    "    neg_returns = np.abs(returns[returns < 0].sum())\n",
    "    profit_factor = pos_returns / neg_returns if neg_returns != 0 else np.inf\n",
    "\n",
    "    # Sharpe ratio (simplified)\n",
    "    mean_return = returns.mean()\n",
    "    std_return = returns.std()\n",
    "    sharpe = (mean_return / std_return) * np.sqrt(252) if std_return != 0 else 0\n",
    "\n",
    "    # Win rate\n",
    "    win_rate = np.mean(returns > 0)\n",
    "\n",
    "    # Print detailed results\n",
    "    print(f\"\\nDetailed evaluation for {model_name}:\")\n",
    "    print(f\"  MSE: {mse:.8f}\")\n",
    "    print(f\"  RMSE: {rmse:.6f}\")\n",
    "    print(f\"  MAE: {mae:.6f}\")\n",
    "    print(f\"  R: {r2:.4f}\")\n",
    "\n",
    "    print(f\"  Directional Accuracy: {dir_acc:.4f}\")\n",
    "    print(f\"    - For actual positive returns: {pos_dir_acc:.4f}\")\n",
    "    print(f\"    - For actual negative returns: {neg_dir_acc:.4f}\")\n",
    "\n",
    "    print(f\"  Spearman Correlation: {spearman_corr:.4f} (p={spearman_pval:.4f})\")\n",
    "\n",
    "    print(f\"  Trading Performance:\")\n",
    "    print(f\"    - Win Rate: {win_rate:.4f}\")\n",
    "    print(f\"    - Profit Factor: {profit_factor:.4f}\")\n",
    "    print(f\"    - Sharpe Ratio: {sharpe:.4f}\")\n",
    "    print(f\"    - Mean Return: {mean_return:.6f}\")\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'mae': mae,\n",
    "        'dir_acc': dir_acc,\n",
    "        'pos_dir_acc': pos_dir_acc,\n",
    "        'neg_dir_acc': neg_dir_acc,\n",
    "        'spearman_corr': spearman_corr,\n",
    "        'spearman_pval': spearman_pval,\n",
    "        'profit_factor': profit_factor,\n",
    "        'win_rate': win_rate,\n",
    "        'sharpe': sharpe,\n",
    "        'mean_return': mean_return\n",
    "    }\n",
    "\n",
    "def analyze_feature_importance(models, feature_names):\n",
    "    \"\"\"Analyze feature importance from multiple models\"\"\"\n",
    "    if 'rf' in models:\n",
    "        # Get feature importance from Random Forest\n",
    "        rf_model = models['rf']\n",
    "        rf_importance = pd.Series(rf_model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "        print(\"\\nTop 15 features from Random Forest:\")\n",
    "        for i, (feat, imp) in enumerate(rf_importance.head(15).items()):\n",
    "            print(f\"  {i+1}. {feat}: {imp:.4f}\")\n",
    "\n",
    "    if 'gbr' in models:\n",
    "        # Get feature importance from Gradient Boosting\n",
    "        gb_model = models['gbr']\n",
    "        gb_importance = pd.Series(gb_model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "        print(\"\\nTop 15 features from Gradient Boosting:\")\n",
    "        for i, (feat, imp) in enumerate(gb_importance.head(15).items()):\n",
    "            print(f\"  {i+1}. {feat}: {imp:.4f}\")\n",
    "\n",
    "    if 'xgb' in models:\n",
    "        # Get feature importance from XGBoost\n",
    "        xgb_model = models['xgb']\n",
    "        xgb_importance = pd.Series(xgb_model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "        print(\"\\nTop 15 features from XGBoost:\")\n",
    "        for i, (feat, imp) in enumerate(xgb_importance.head(15).items()):\n",
    "            print(f\"  {i+1}. {feat}: {imp:.4f}\")\n",
    "\n",
    "    if 'elastic' in models:\n",
    "        # Get coefficients from ElasticNet\n",
    "        en_model = models['elastic']\n",
    "        en_coef = pd.Series(en_model.coef_, index=feature_names)\n",
    "        en_importance = en_coef.abs().sort_values(ascending=False)\n",
    "\n",
    "        print(\"\\nTop 15 features from ElasticNet:\")\n",
    "        for i, (feat, imp) in enumerate(en_importance.head(15).items()):\n",
    "            coef_value = en_coef[feat]\n",
    "            print(f\"  {i+1}. {feat}: {coef_value:.6f}\")\n",
    "\n",
    "    # If multiple models available, calculate consensus importance\n",
    "    if set(['rf', 'gbr', 'xgb']).issubset(set(models.keys())):\n",
    "        print(\"\\nConsensus feature importance (top 15):\")\n",
    "\n",
    "        # Get scaled importance from each model\n",
    "        rf_imp = pd.Series(models['rf'].feature_importances_, index=feature_names)\n",
    "        rf_imp = rf_imp / rf_imp.sum()\n",
    "\n",
    "        gb_imp = pd.Series(models['gbr'].feature_importances_, index=feature_names)\n",
    "        gb_imp = gb_imp / gb_imp.sum()\n",
    "\n",
    "        xgb_imp = pd.Series(models['xgb'].feature_importances_, index=feature_names)\n",
    "        xgb_imp = xgb_imp / xgb_imp.sum()\n",
    "\n",
    "        # Calculate consensus score\n",
    "        consensus = (rf_imp + gb_imp + xgb_imp) / 3\n",
    "\n",
    "        # Print top features by consensus\n",
    "        for i, (feat, imp) in enumerate(consensus.sort_values(ascending=False).head(15).items()):\n",
    "            rf_val = rf_imp[feat]\n",
    "            gb_val = gb_imp[feat]\n",
    "            xgb_val = xgb_imp[feat]\n",
    "            print(f\"  {i+1}. {feat}: {imp:.4f} (RF: {rf_val:.4f}, GB: {gb_val:.4f}, XGB: {xgb_val:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### ENHANCED FIXED INCOME RETURN PREDICTION SYSTEM ###\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Load and preprocess data\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### LOADING AND PREPROCESSING DATA ###\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    data_path = 'c:/Users/Eddy/Documents/backtrader/data_pipelines/backtest_data.csv'\n",
    "    df = pd.read_csv(data_path, index_col=0, parse_dates=True)\n",
    "    print(f\"Original data shape: {df.shape}\")\n",
    "    print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"Original columns: {', '.join(df.columns.tolist())}\")\n",
    "\n",
    "    # Check for missing values and basic data quality\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(f\"\\nMissing values check:\")\n",
    "    if missing_values.sum() > 0:\n",
    "        print(f\"Columns with missing values: {missing_values[missing_values > 0]}\")\n",
    "    else:\n",
    "        print(\"No missing values found in original data\")\n",
    "\n",
    "    # Handle missing values\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "    print(\"Missing values filled using forward-fill and backward-fill methods\")\n",
    "\n",
    "    # Enhanced feature engineering\n",
    "    df = create_enhanced_features(df)\n",
    "\n",
    "    # Create return targets\n",
    "    df = create_return_targets(df)\n",
    "\n",
    "    # Clean data\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### DATA CLEANING ###\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Shape before dropping NaN: {df.shape}\")\n",
    "    orig_count = len(df)\n",
    "    df = df.dropna()\n",
    "    dropped_count = orig_count - len(df)\n",
    "    dropped_pct = (dropped_count / orig_count) * 100 if orig_count > 0 else 0\n",
    "    print(f\"Shape after dropping NaN: {df.shape} (dropped {dropped_count} rows, {dropped_pct:.2f}%)\")\n",
    "\n",
    "    # Define primary target and horizons\n",
    "    horizons = [5, 10, 20, 60]  # Added 60-day for longer-term signals\n",
    "    primary_horizon = 5\n",
    "    primary_target = f'target_return_{primary_horizon}d'\n",
    "    print(f\"\\nPrediction horizons: {horizons} days\")\n",
    "    print(f\"Primary prediction horizon: {primary_horizon} days\")\n",
    "\n",
    "    # Define features (exclude targets)\n",
    "    feature_cols = [col for col in df.columns if not col.startswith('target_')]\n",
    "    X = df[feature_cols]\n",
    "    print(f\"\\nTotal features available: {len(feature_cols)}\")\n",
    "\n",
    "    # Display sample of feature names\n",
    "    print(\"Sample of features:\")\n",
    "    sample_features = feature_cols[:5] + feature_cols[-5:] if len(feature_cols) > 10 else feature_cols\n",
    "    for i, feat in enumerate(sample_features):\n",
    "        print(f\"  {i+1}. {feat}\")\n",
    "    if len(feature_cols) > 10:\n",
    "        print(f\"  ... and {len(feature_cols) - 10} more features\")\n",
    "\n",
    "    # Get targets for all horizons\n",
    "    y_dict = {horizon: df[f'target_return_{horizon}d'] for horizon in horizons}\n",
    "\n",
    "    # Print target statistics\n",
    "    print(\"\\nTarget statistics:\")\n",
    "    for horizon, y in y_dict.items():\n",
    "        print(f\"  {horizon}-day returns: mean={y.mean():.6f}, std={y.std():.6f}, min={y.min():.6f}, max={y.max():.6f}\")\n",
    "        print(f\"    Positive returns: {(y > 0).mean()*100:.2f}%\")\n",
    "\n",
    "    # Feature selection - using stability selection\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### FEATURE SELECTION WITH STABILITY SELECTION ###\")\n",
    "    print(\"=\"*80)\n",
    "    selected_features = stable_feature_selection(X, y_dict[primary_horizon], n_iterations=30, sample_fraction=0.8, top_n=50)\n",
    "    X = X[selected_features]\n",
    "    print(f\"Selected {len(selected_features)} features using stability selection\")\n",
    "\n",
    "    # Print all selected features\n",
    "    print(\"\\nAll selected features:\")\n",
    "    for i, feat in enumerate(selected_features):\n",
    "        print(f\"  {i+1}. {feat}\")\n",
    "\n",
    "    # Train/test split - more recent data as test\n",
    "    split_pct = 0.8\n",
    "    split_index = int(split_pct * len(X))\n",
    "    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "    y_train_dict = {h: y.iloc[:split_index] for h, y in y_dict.items()}\n",
    "    y_test_dict = {h: y.iloc[split_index:] for h, y in y_dict.items()}\n",
    "\n",
    "    print(f\"\\nTraining set: {X_train.shape[0]} samples from {X_train.index.min()} to {X_train.index.max()}\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} samples from {X_test.index.min()} to {X_test.index.max()}\")\n",
    "\n",
    "    # Print target statistics for train/test\n",
    "    print(\"\\nTraining set target statistics:\")\n",
    "    for horizon, y_train in y_train_dict.items():\n",
    "        print(f\"  {horizon}-day returns: mean={y_train.mean():.6f}, std={y_train.std():.6f}\")\n",
    "        print(f\"    Positive returns: {(y_train > 0).mean()*100:.2f}%\")\n",
    "\n",
    "    print(\"\\nTest set target statistics:\")\n",
    "    for horizon, y_test in y_test_dict.items():\n",
    "        print(f\"  {horizon}-day returns: mean={y_test.mean():.6f}, std={y_test.std():.6f}\")\n",
    "        print(f\"    Positive returns: {(y_test > 0).mean()*100:.2f}%\")\n",
    "\n",
    "    # Scale features\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### FEATURE SCALING ###\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Using RobustScaler for feature scaling (robust to outliers)\")\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Print scaling statistics\n",
    "    print(\"Scaling summary:\")\n",
    "    scale_sample = np.random.choice(range(X_train.shape[1]), min(5, X_train.shape[1]), replace=False)\n",
    "    for i in scale_sample:\n",
    "        col = X_train.columns[i]\n",
    "        print(f\"  {col}:\")\n",
    "        print(f\"    Before scaling: mean={X_train.iloc[:, i].mean():.4f}, std={X_train.iloc[:, i].std():.4f}\")\n",
    "        print(f\"    After scaling: mean={X_train_scaled[:, i].mean():.4f}, std={X_train_scaled[:, i].std():.4f}\")\n",
    "\n",
    "    # Convert back to DataFrames for easier handling\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "    # Identify market regimes\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### IDENTIFYING MARKET REGIMES ###\")\n",
    "    print(\"=\"*80)\n",
    "    regimes_df = identify_improved_regimes(df)\n",
    "    regimes_train = regimes_df.loc[X_train.index]\n",
    "    regimes_test = regimes_df.loc[X_test.index]\n",
    "\n",
    "    # Print regime distribution\n",
    "    print(\"\\nTraining set regime distribution:\")\n",
    "    train_regime_counts = regimes_train['regime_label'].value_counts(normalize=True)\n",
    "    for regime, count in train_regime_counts.items():\n",
    "        print(f\"  {regime}: {count*100:.2f}%\")\n",
    "\n",
    "    print(\"\\nTest set regime distribution:\")\n",
    "    test_regime_counts = regimes_test['regime_label'].value_counts(normalize=True)\n",
    "    for regime, count in test_regime_counts.items():\n",
    "        print(f\"  {regime}: {count*100:.2f}%\")\n",
    "\n",
    "    # Initialize storage for models and metrics\n",
    "    all_models = {}\n",
    "    all_predictions = {}\n",
    "    model_metrics = {}\n",
    "    best_models_by_horizon = {}\n",
    "    best_metrics_by_horizon = {}\n",
    "\n",
    "    # Build models for each horizon\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### TRAINING MODELS FOR MULTIPLE HORIZONS ###\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for horizon in horizons:\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(f\"## TRAINING MODELS FOR {horizon}-DAY HORIZON ##\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Get target for this horizon\n",
    "        y_train = y_train_dict[horizon]\n",
    "        y_test = y_test_dict[horizon]\n",
    "\n",
    "        # Build traditional models\n",
    "        traditional_models = build_traditional_models(X_train_scaled_df, y_train)\n",
    "\n",
    "        # Build regime-aware models\n",
    "        regime_models = build_regime_aware_models(X_train_scaled_df, y_train, regimes_train)\n",
    "\n",
    "        # Build advanced models\n",
    "        advanced_models = build_advanced_models(X_train_scaled_df, y_train)\n",
    "\n",
    "        # Store all models for this horizon\n",
    "        all_models[horizon] = {**traditional_models, **regime_models, **advanced_models}\n",
    "\n",
    "        # Generate and evaluate predictions\n",
    "        horizon_predictions = {}\n",
    "\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(f\"## EVALUATING MODELS FOR {horizon}-DAY HORIZON ##\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        best_model_name = None\n",
    "        best_dir_acc = -1\n",
    "        best_profit = -1\n",
    "\n",
    "        for model_name, model in all_models[horizon].items():\n",
    "            # Skip regime-specific models in direct evaluation\n",
    "            if model_name.startswith(('ridge_regime_', 'rf_regime_')):\n",
    "                continue\n",
    "\n",
    "            # Generate predictions\n",
    "            print(f\"\\nGenerating predictions for model: {model_name}\")\n",
    "            y_pred = model.predict(X_test_scaled_df)\n",
    "\n",
    "            # Store predictions\n",
    "            horizon_predictions[model_name] = y_pred\n",
    "\n",
    "            # Evaluate model with detailed metrics\n",
    "            metrics = evaluate_model_detailed(y_test, y_pred, f\"{horizon}d {model_name}\")\n",
    "            model_metrics[(horizon, model_name)] = metrics\n",
    "\n",
    "            # Track best model\n",
    "            if metrics['dir_acc'] > best_dir_acc:\n",
    "                best_dir_acc = metrics['dir_acc']\n",
    "                best_model_name = model_name\n",
    "\n",
    "            if metrics['profit_factor'] > best_profit:\n",
    "                best_profit = metrics['profit_factor']\n",
    "\n",
    "        # Create regime-weighted ensemble predictions\n",
    "        if any(k.startswith('ridge_regime_') for k in all_models[horizon].keys()):\n",
    "            print(\"\\nGenerating regime-aware ensemble predictions\")\n",
    "            regime_ensemble_preds = create_regime_ensemble_predictions(\n",
    "                all_models[horizon], X_test_scaled_df, regimes_test)\n",
    "            horizon_predictions['regime_ensemble'] = regime_ensemble_preds\n",
    "\n",
    "            # Evaluate regime ensemble\n",
    "            metrics = evaluate_model_detailed(y_test, regime_ensemble_preds, f\"{horizon}d regime_ensemble\")\n",
    "            model_metrics[(horizon, 'regime_ensemble')] = metrics\n",
    "\n",
    "            # Check if this is the best model\n",
    "            if metrics['dir_acc'] > best_dir_acc:\n",
    "                best_dir_acc = metrics['dir_acc']\n",
    "                best_model_name = 'regime_ensemble'\n",
    "\n",
    "        # Create simple model ensemble (average predictions)\n",
    "        print(\"\\nGenerating model ensemble predictions (average of all models)\")\n",
    "        model_ensemble_preds = np.zeros(len(X_test))\n",
    "        ensemble_count = 0\n",
    "\n",
    "        for model_name, preds in horizon_predictions.items():\n",
    "            if not model_name.startswith('regime'):  # Exclude regime ensemble\n",
    "                model_ensemble_preds += preds\n",
    "                ensemble_count += 1\n",
    "\n",
    "        model_ensemble_preds /= ensemble_count\n",
    "        horizon_predictions['model_ensemble'] = model_ensemble_preds\n",
    "\n",
    "        # Evaluate model ensemble\n",
    "        metrics = evaluate_model_detailed(y_test, model_ensemble_preds, f\"{horizon}d model_ensemble\")\n",
    "        model_metrics[(horizon, 'model_ensemble')] = metrics\n",
    "\n",
    "        # Check if this is the best model\n",
    "        if metrics['dir_acc'] > best_dir_acc:\n",
    "            best_dir_acc = metrics['dir_acc']\n",
    "            best_model_name = 'model_ensemble'\n",
    "\n",
    "        # Store all predictions for this horizon\n",
    "        all_predictions[horizon] = horizon_predictions\n",
    "\n",
    "        # Store best model for this horizon\n",
    "        best_models_by_horizon[horizon] = best_model_name\n",
    "        best_metrics_by_horizon[horizon] = model_metrics[(horizon, best_model_name)]\n",
    "\n",
    "        print(f\"\\nBest model for {horizon}-day horizon: {best_model_name}\")\n",
    "        print(f\"  Directional Accuracy: {best_dir_acc:.4f}\")\n",
    "\n",
    "    # Train LSTM model for sequence prediction (if Keras is available)\n",
    "    if KERAS_AVAILABLE:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"### TRAINING LSTM SEQUENCE MODEL ###\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        lstm_model, lstm_history = build_lstm_model(\n",
    "            X_train_scaled_df, y_train_dict[primary_horizon], lookback=20)\n",
    "\n",
    "        if lstm_model is not None:\n",
    "            # Get LSTM predictions\n",
    "            print(\"\\nGenerating LSTM predictions\")\n",
    "            lstm_preds = predict_with_lstm(lstm_model, X_test_scaled_df, lookback=20)\n",
    "\n",
    "            # Evaluate LSTM\n",
    "            print(\"\\nEvaluating LSTM model:\")\n",
    "            valid_indices = ~np.isnan(lstm_preds)\n",
    "            if np.any(valid_indices):\n",
    "                y_valid = y_test_dict[primary_horizon].iloc[valid_indices]\n",
    "                lstm_valid_preds = lstm_preds[valid_indices]\n",
    "\n",
    "                metrics = evaluate_model_detailed(\n",
    "                    y_valid, lstm_valid_preds, f\"{primary_horizon}d LSTM\"\n",
    "                )\n",
    "                model_metrics[(primary_horizon, 'lstm')] = metrics\n",
    "\n",
    "                # Store LSTM predictions\n",
    "                all_predictions[primary_horizon]['lstm'] = lstm_preds\n",
    "\n",
    "                # Check if LSTM is best for primary horizon\n",
    "                if metrics['dir_acc'] > best_metrics_by_horizon[primary_horizon]['dir_acc']:\n",
    "                    best_models_by_horizon[primary_horizon] = 'lstm'\n",
    "                    best_metrics_by_horizon[primary_horizon] = metrics\n",
    "                    print(f\"LSTM is now the best model for {primary_horizon}-day horizon!\")\n",
    "            else:\n",
    "                print(\"Could not evaluate LSTM - all predictions are NaN\")\n",
    "    else:\n",
    "        print(\"\\nSkipping LSTM model (Keras not available)\")\n",
    "\n",
    "    # Create adaptive multi-horizon ensemble\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### CREATING ADAPTIVE MULTI-HORIZON ENSEMBLE ###\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Print best models for each horizon\n",
    "    print(\"\\nBest model for each horizon:\")\n",
    "    for horizon in horizons:\n",
    "        best_model = best_models_by_horizon[horizon]\n",
    "        best_metrics = best_metrics_by_horizon[horizon]\n",
    "        print(f\"  {horizon}-day horizon: {best_model}\")\n",
    "        print(f\"    Directional Accuracy: {best_metrics['dir_acc']:.4f}\")\n",
    "        print(f\"    R: {best_metrics['r2']:.4f}\")\n",
    "        print(f\"    Profit Factor: {best_metrics['profit_factor']:.4f}\")\n",
    "\n",
    "    # Create adaptive multi-horizon ensemble\n",
    "    adaptive_ensemble_preds, model_weights = create_adaptive_ensemble(\n",
    "        all_predictions, y_test_dict, horizons, best_models_by_horizon)\n",
    "\n",
    "    # Print weights\n",
    "    print(\"\\nAdaptive ensemble weights:\")\n",
    "    for model, weight in sorted(model_weights.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {model}: {weight:.4f}\")\n",
    "\n",
    "    # Evaluate adaptive ensemble on primary horizon\n",
    "    print(\"\\nEvaluating adaptive multi-horizon ensemble:\")\n",
    "    metrics = evaluate_model_detailed(\n",
    "        y_test_dict[primary_horizon],\n",
    "        adaptive_ensemble_preds,\n",
    "        \"Adaptive multi-horizon ensemble\"\n",
    "    )\n",
    "    model_metrics[('adaptive', 'ensemble')] = metrics\n",
    "\n",
    "    # Feature importance analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### FEATURE IMPORTANCE ANALYSIS ###\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    analyze_feature_importance(all_models[primary_horizon], X_train.columns)\n",
    "\n",
    "    # Print final performance summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### FINAL PERFORMANCE SUMMARY ###\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Create summary table of all models for primary horizon\n",
    "    print(f\"\\nPerformance metrics for {primary_horizon}-day horizon models:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Model':<20} {'Dir. Acc.':<10} {'R':<10} {'RMSE':<10} {'MAE':<10} {'Profit Factor':<15} {'Sharpe':<10}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for model_key, metrics in model_metrics.items():\n",
    "        # Only show primary horizon models in summary\n",
    "        if model_key[0] == primary_horizon:\n",
    "            model_name = model_key[1]\n",
    "            print(f\"{model_name:<20} {metrics['dir_acc']:<10.4f} {metrics['r2']:<10.4f} \"\n",
    "                  f\"{metrics['rmse']:<10.6f} {metrics['mae']:<10.6f} {metrics['profit_factor']:<15.4f} \"\n",
    "                  f\"{metrics['sharpe']:<10.4f}\")\n",
    "\n",
    "    # Add adaptive ensemble to table\n",
    "    if ('adaptive', 'ensemble') in model_metrics:\n",
    "        metrics = model_metrics[('adaptive', 'ensemble')]\n",
    "        print(f\"{'adaptive_ensemble':<20} {metrics['dir_acc']:<10.4f} {metrics['r2']:<10.4f} \"\n",
    "              f\"{metrics['rmse']:<10.6f} {metrics['mae']:<10.6f} {metrics['profit_factor']:<15.4f} \"\n",
    "              f\"{metrics['sharpe']:<10.4f}\")\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Directional accuracy comparison across horizons\n",
    "    print(\"\\nDirectional accuracy comparison across horizons:\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Fixed: Avoid nested f-string formatting issues\n",
    "    horizon_headers = \"\".join([f\"{h}-day\".ljust(12) for h in horizons])\n",
    "    print(f\"{'Model':<20} {horizon_headers}\")\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Common models across all horizons\n",
    "    common_models = ['ridge', 'elastic', 'rf', 'xgb', 'model_ensemble', 'regime_ensemble']\n",
    "\n",
    "    for model in common_models:\n",
    "        row = f\"{model:<20} \"\n",
    "        for horizon in horizons:\n",
    "            if (horizon, model) in model_metrics:\n",
    "                row += f\"{model_metrics[(horizon, model)]['dir_acc']:<12.4f}\"\n",
    "            else:\n",
    "                row += f\"{'N/A':<12}\"\n",
    "        print(row)\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Print adaptive ensemble performance\n",
    "    if ('adaptive', 'ensemble') in model_metrics:\n",
    "        metrics = model_metrics[('adaptive', 'ensemble')]\n",
    "\n",
    "        print(\"\\nAdaptive Multi-Horizon Ensemble Final Performance:\")\n",
    "        print(f\"R: {metrics['r2']:.4f}\")\n",
    "        print(f\"RMSE: {metrics['rmse']:.6f}\")\n",
    "        print(f\"Directional Accuracy: {metrics['dir_acc']:.4f}\")\n",
    "        print(f\"Profit Factor: {metrics['profit_factor']:.4f}\")\n",
    "        print(f\"Sharpe Ratio: {metrics['sharpe']:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"### EXECUTION COMPLETE ###\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    return all_models, all_predictions, adaptive_ensemble_preds, model_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "### ENHANCED FIXED INCOME RETURN PREDICTION SYSTEM ###\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "### LOADING AND PREPROCESSING DATA ###\n",
      "================================================================================\n",
      "Original data shape: (5902, 15)\n",
      "Date range: 2002-10-31 00:00:00 to 2025-02-27 00:00:00\n",
      "Original columns: cad_oas, us_hy_oas, us_ig_oas, tsx, vix, us_3m_10y, us_growth_surprises, us_inflation_surprises, us_lei_yoy, us_hard_data_surprises, us_equity_revisions, us_economic_regime, cad_ig_er_index, us_hy_er_index, us_ig_er_index\n",
      "\n",
      "Missing values check:\n",
      "No missing values found in original data\n",
      "Missing values filled using forward-fill and backward-fill methods\n",
      "\n",
      "================================================================================\n",
      "### ENHANCED FEATURE ENGINEERING ###\n",
      "================================================================================\n",
      "Creating 3-day features...\n",
      "Creating 5-day features...\n",
      "Creating 10-day features...\n",
      "Creating 15-day features...\n",
      "Creating 20-day features...\n",
      "Creating 30-day features...\n",
      "Creating 60-day features...\n",
      "Creating 90-day features...\n",
      "Creating 120-day features...\n",
      "Creating 250-day features...\n",
      "Creating enhanced rate environment features...\n",
      "Creating market regime indicators...\n",
      "Creating technical indicators...\n",
      "  Calculating RSI with 14-day window for cad_ig_er_index\n",
      "  Calculating RSI with 14-day window for us_ig_er_index\n",
      "  Calculating RSI with 14-day window for us_ig_oas\n",
      "  Calculating RSI with 30-day window for cad_ig_er_index\n",
      "  Calculating RSI with 30-day window for us_ig_er_index\n",
      "  Calculating RSI with 30-day window for us_ig_oas\n",
      "  Calculating RSI with 60-day window for cad_ig_er_index\n",
      "  Calculating RSI with 60-day window for us_ig_er_index\n",
      "  Calculating RSI with 60-day window for us_ig_oas\n",
      "Creating lagged relationships...\n",
      "Creating non-linear transformations and interactions...\n",
      "Creating z-score features...\n",
      "Creating economic regime interactions...\n",
      "Creating autocorrelation features...\n",
      "  Created autocorrelation feature: us_ig_oas_autocorr_5\n",
      "  Created autocorrelation feature: us_ig_oas_autocorr_10\n",
      "  Created autocorrelation feature: us_ig_oas_autocorr_20\n",
      "  Created autocorrelation feature: us_hy_oas_autocorr_5\n",
      "  Created autocorrelation feature: us_hy_oas_autocorr_10\n",
      "  Created autocorrelation feature: us_hy_oas_autocorr_20\n",
      "  Created autocorrelation feature: cad_oas_autocorr_5\n",
      "  Created autocorrelation feature: cad_oas_autocorr_10\n",
      "  Created autocorrelation feature: cad_oas_autocorr_20\n",
      "  Created autocorrelation feature: cad_ig_er_index_autocorr_5\n",
      "  Created autocorrelation feature: cad_ig_er_index_autocorr_10\n",
      "  Created autocorrelation feature: cad_ig_er_index_autocorr_20\n",
      "Creating exponentially weighted features...\n",
      "\n",
      "Feature engineering complete: 262 new features created\n",
      "Original features: 15, Total features now: 277\n",
      "\n",
      "================================================================================\n",
      "### TARGET ENGINEERING ###\n",
      "================================================================================\n",
      "Creating return target variables...\n",
      "  Target 1-day return:\n",
      "    Mean: 0.000051\n",
      "    Std Dev: 0.000924\n",
      "    Min: -0.018623\n",
      "    Max: 0.007247\n",
      "    Positive returns: 56.3%\n",
      "    Negative returns: 38.3%\n",
      "  Target 3-day return:\n",
      "    Mean: 0.000151\n",
      "    Std Dev: 0.001948\n",
      "    Min: -0.027042\n",
      "    Max: 0.016885\n",
      "    Positive returns: 61.6%\n",
      "    Negative returns: 38.4%\n",
      "  Target 5-day return:\n",
      "    Mean: 0.000253\n",
      "    Std Dev: 0.002848\n",
      "    Min: -0.041640\n",
      "    Max: 0.022801\n",
      "    Positive returns: 62.7%\n",
      "    Negative returns: 37.3%\n",
      "  Target 10-day return:\n",
      "    Mean: 0.000508\n",
      "    Std Dev: 0.004810\n",
      "    Min: -0.071521\n",
      "    Max: 0.032632\n",
      "    Positive returns: 63.7%\n",
      "    Negative returns: 36.3%\n",
      "  Target 20-day return:\n",
      "    Mean: 0.001018\n",
      "    Std Dev: 0.007883\n",
      "    Min: -0.093680\n",
      "    Max: 0.046416\n",
      "    Positive returns: 65.4%\n",
      "    Negative returns: 34.6%\n",
      "  Target 30-day return:\n",
      "    Mean: 0.001531\n",
      "    Std Dev: 0.010087\n",
      "    Min: -0.096046\n",
      "    Max: 0.050912\n",
      "    Positive returns: 66.0%\n",
      "    Negative returns: 34.0%\n",
      "  Target 60-day return:\n",
      "    Mean: 0.003088\n",
      "    Std Dev: 0.015716\n",
      "    Min: -0.094807\n",
      "    Max: 0.084585\n",
      "    Positive returns: 69.3%\n",
      "    Negative returns: 30.7%\n",
      "\n",
      "Target return correlation matrix:\n",
      "                   target_return_1d  target_return_3d  target_return_5d  \\\n",
      "target_return_1d               1.00              0.71              0.60   \n",
      "target_return_3d               0.71              1.00              0.88   \n",
      "target_return_5d               0.60              0.88              1.00   \n",
      "target_return_10d              0.48              0.72              0.85   \n",
      "target_return_20d              0.34              0.52              0.63   \n",
      "target_return_30d              0.26              0.40              0.49   \n",
      "target_return_60d              0.22              0.33              0.39   \n",
      "\n",
      "                   target_return_10d  target_return_20d  target_return_30d  \\\n",
      "target_return_1d                0.48               0.34               0.26   \n",
      "target_return_3d                0.72               0.52               0.40   \n",
      "target_return_5d                0.85               0.63               0.49   \n",
      "target_return_10d               1.00               0.82               0.65   \n",
      "target_return_20d               0.82               1.00               0.89   \n",
      "target_return_30d               0.65               0.89               1.00   \n",
      "target_return_60d               0.51               0.66               0.78   \n",
      "\n",
      "                   target_return_60d  \n",
      "target_return_1d                0.22  \n",
      "target_return_3d                0.33  \n",
      "target_return_5d                0.39  \n",
      "target_return_10d               0.51  \n",
      "target_return_20d               0.66  \n",
      "target_return_30d               0.78  \n",
      "target_return_60d               1.00  \n",
      "\n",
      "Correlation between 5-day and 20-day returns: 0.6301\n",
      "Correlation between 5-day and 60-day returns: 0.3904\n",
      "\n",
      "================================================================================\n",
      "### DATA CLEANING ###\n",
      "================================================================================\n",
      "Shape before dropping NaN: (5902, 284)\n",
      "Shape after dropping NaN: (5546, 284) (dropped 356 rows, 6.03%)\n",
      "\n",
      "Prediction horizons: [5, 10, 20, 60] days\n",
      "Primary prediction horizon: 5 days\n",
      "\n",
      "Total features available: 277\n",
      "Sample of features:\n",
      "  1. cad_oas\n",
      "  2. us_hy_oas\n",
      "  3. us_ig_oas\n",
      "  4. tsx\n",
      "  5. vix\n",
      "  6. us_hy_oas_ewm_divergence\n",
      "  7. vix_ewm_crossover\n",
      "  8. vix_ewm_divergence\n",
      "  9. curve_slope_ewm_crossover\n",
      "  10. curve_slope_ewm_divergence\n",
      "  ... and 267 more features\n",
      "\n",
      "Target statistics:\n",
      "  5-day returns: mean=0.000237, std=0.002903, min=-0.041640, max=0.022801\n",
      "    Positive returns: 62.66%\n",
      "  10-day returns: mean=0.000482, std=0.004916, min=-0.071521, max=0.032632\n",
      "    Positive returns: 63.60%\n",
      "  20-day returns: mean=0.000983, std=0.008065, min=-0.093680, max=0.046416\n",
      "    Positive returns: 65.40%\n",
      "  60-day returns: mean=0.003018, std=0.016075, min=-0.094807, max=0.084585\n",
      "    Positive returns: 68.55%\n",
      "\n",
      "================================================================================\n",
      "### FEATURE SELECTION WITH STABILITY SELECTION ###\n",
      "================================================================================\n",
      "Performing stability-based feature selection with 30 iterations...\n",
      "  Sample fraction: 0.8, Target top features: 50\n",
      "  Iteration 1/30...\n",
      "    Lasso selected 16 features\n",
      "    Random Forest selected 138 features\n",
      "    Lasso selected 8 features\n",
      "    Random Forest selected 138 features\n",
      "    Lasso selected 7 features\n",
      "    Random Forest selected 138 features\n",
      "    Lasso selected 7 features\n",
      "    Random Forest selected 138 features\n",
      "    Lasso selected 8 features\n",
      "    Random Forest selected 138 features\n",
      "  Iteration 11/30...\n",
      "    Lasso selected 9 features\n",
      "    Random Forest selected 138 features\n",
      "    Lasso selected 9 features\n",
      "    Random Forest selected 138 features\n",
      "    Lasso selected 18 features\n",
      "    Random Forest selected 138 features\n",
      "    Lasso selected 10 features\n",
      "    Random Forest selected 138 features\n",
      "    Lasso selected 7 features\n",
      "    Random Forest selected 138 features\n",
      "  Iteration 21/30...\n",
      "    Lasso selected 8 features\n",
      "    Random Forest selected 138 features\n",
      "    Lasso selected 7 features\n",
      "    Random Forest selected 138 features\n",
      "    Lasso selected 9 features\n",
      "    Random Forest selected 138 features\n",
      "    Lasso selected 8 features\n",
      "    Random Forest selected 138 features\n",
      "    Lasso selected 18 features\n",
      "  Iteration 30/30...\n",
      "    Random Forest selected 138 features\n",
      "\n",
      "Feature selection frequency statistics:\n",
      "  Mean selection rate: 0.2670\n",
      "  Min selection rate: 0.0000\n",
      "  Max selection rate: 0.6667\n",
      "  25th percentile: 0.1000\n",
      "  50th percentile: 0.2667\n",
      "  75th percentile: 0.4333\n",
      "\n",
      "Top 15 selected features:\n",
      "  1. us_ig_er_index_mom10: score=0.058897, selected in 50.0% of iterations\n",
      "  2. us_hy_oas_mom10: score=0.022284, selected in 50.0% of iterations\n",
      "  3. er_vol_5d: score=0.021926, selected in 50.0% of iterations\n",
      "  4. vix_ewm_divergence: score=0.018964, selected in 50.0% of iterations\n",
      "  5. hy_ig_oas_ratio_250d: score=0.017715, selected in 50.0% of iterations\n",
      "  6. us_ig_oas_ewm_signal: score=0.010512, selected in 50.0% of iterations\n",
      "  7. us_hy_oas_zscore_60d: score=0.008004, selected in 50.0% of iterations\n",
      "  8. hy_ig_oas_ratio_120d: score=0.007118, selected in 50.0% of iterations\n",
      "  9. us_hy_oas_zscore_250d: score=0.006951, selected in 50.0% of iterations\n",
      "  10. us_ig_oas_mom10: score=0.006185, selected in 50.0% of iterations\n",
      "  11. hy_ig_oas_ratio_90d: score=0.004876, selected in 50.0% of iterations\n",
      "  12. hy_ig_oas_ratio_60d: score=0.004709, selected in 50.0% of iterations\n",
      "  13. vix_zscore_250d: score=0.004653, selected in 50.0% of iterations\n",
      "  14. er_trend_divergence: score=0.004444, selected in 50.0% of iterations\n",
      "  15. us_ig_oas_mom15: score=0.004197, selected in 50.0% of iterations\n",
      "Selected 50 features using stability selection\n",
      "\n",
      "All selected features:\n",
      "  1. us_ig_er_index_mom10\n",
      "  2. us_hy_oas_mom10\n",
      "  3. er_vol_5d\n",
      "  4. vix_ewm_divergence\n",
      "  5. hy_ig_oas_ratio_250d\n",
      "  6. us_ig_oas_ewm_signal\n",
      "  7. us_hy_oas_zscore_60d\n",
      "  8. hy_ig_oas_ratio_120d\n",
      "  9. us_hy_oas_zscore_250d\n",
      "  10. us_ig_oas_mom10\n",
      "  11. hy_ig_oas_ratio_90d\n",
      "  12. hy_ig_oas_ratio_60d\n",
      "  13. vix_zscore_250d\n",
      "  14. er_trend_divergence\n",
      "  15. us_ig_oas_mom15\n",
      "  16. cad_oas_ma120\n",
      "  17. hy_ig_oas_ratio_10d\n",
      "  18. cad_oas_mom5\n",
      "  19. us_ig_oas_mom3\n",
      "  20. cad_ig_er_index_mom3\n",
      "  21. us_ig_er_index_mom3\n",
      "  22. vix_ma30\n",
      "  23. cad_oas_ma20\n",
      "  24. us_ig_er_index_mom15\n",
      "  25. er_vol_250d\n",
      "  26. us_hy_oas_mom20\n",
      "  27. cad_ig_er_index_autocorr_20\n",
      "  28. er_vol_3d\n",
      "  29. vix_ewm30_std\n",
      "  30. cad_er_rsi_14\n",
      "  31. oas_rsi_14\n",
      "  32. vix_ewm10_std\n",
      "  33. er_vol_10d\n",
      "  34. cad_ig_er_index_mom10\n",
      "  35. cad_us_oas_diff_250d\n",
      "  36. us_hy_oas_ewm10_std\n",
      "  37. cad_ig_er_index_pct_lag3\n",
      "  38. cad_oas_mom60\n",
      "  39. vix_ma20\n",
      "  40. regime_oas\n",
      "  41. cad_oas_autocorr_10\n",
      "  42. curve_slope_ewm60\n",
      "  43. vix_ma60\n",
      "  44. us_ig_er_index_mom20\n",
      "  45. curve_slope_ewm10\n",
      "  46. us_ig_oas_autocorr_5\n",
      "  47. hy_ig_oas_ratio_30d\n",
      "  48. hy_ig_oas_ratio_20d\n",
      "  49. cad_ig_er_index_mom5\n",
      "  50. us_ig_er_index_mom5\n",
      "\n",
      "Training set: 4436 samples from 2003-10-13 00:00:00 to 2020-07-24 00:00:00\n",
      "Testing set: 1110 samples from 2020-07-27 00:00:00 to 2024-12-05 00:00:00\n",
      "\n",
      "Training set target statistics:\n",
      "  5-day returns: mean=0.000200, std=0.003116\n",
      "    Positive returns: 62.06%\n",
      "  10-day returns: mean=0.000408, std=0.005291\n",
      "    Positive returns: 62.80%\n",
      "  20-day returns: mean=0.000838, std=0.008727\n",
      "    Positive returns: 64.40%\n",
      "  60-day returns: mean=0.002542, std=0.017488\n",
      "    Positive returns: 66.52%\n",
      "\n",
      "Test set target statistics:\n",
      "  5-day returns: mean=0.000386, std=0.001816\n",
      "    Positive returns: 65.05%\n",
      "  10-day returns: mean=0.000781, std=0.002961\n",
      "    Positive returns: 66.76%\n",
      "  20-day returns: mean=0.001559, std=0.004499\n",
      "    Positive returns: 69.37%\n",
      "  60-day returns: mean=0.004916, std=0.008029\n",
      "    Positive returns: 76.67%\n",
      "\n",
      "================================================================================\n",
      "### FEATURE SCALING ###\n",
      "================================================================================\n",
      "Using RobustScaler for feature scaling (robust to outliers)\n",
      "Scaling summary:\n",
      "  hy_ig_oas_ratio_120d:\n",
      "    Before scaling: mean=3.4220, std=0.3521\n",
      "    After scaling: mean=0.0109, std=0.8436\n",
      "  us_ig_oas_autocorr_5:\n",
      "    Before scaling: mean=0.1166, std=0.6732\n",
      "    After scaling: mean=-0.0862, std=0.5085\n",
      "  cad_oas_ma120:\n",
      "    Before scaling: mean=121.3537, std=52.7219\n",
      "    After scaling: mean=0.0972, std=1.2222\n",
      "  er_trend_divergence:\n",
      "    Before scaling: mean=-0.0018, std=0.0152\n",
      "    After scaling: mean=0.0953, std=1.6688\n",
      "  us_ig_er_index_mom20:\n",
      "    Before scaling: mean=0.0006, std=0.0166\n",
      "    After scaling: mean=-0.1017, std=1.8724\n",
      "\n",
      "================================================================================\n",
      "### IDENTIFYING MARKET REGIMES ###\n",
      "================================================================================\n",
      "Identifying market regimes using Gaussian Mixture Model...\n",
      "  Number of regimes to identify: 3\n",
      "  Created feature: vix_ma20\n",
      "  Created feature: vix_ma60\n",
      "  Created feature: vix_mom20\n",
      "  Created feature: vix_vol20\n",
      "  Created feature: us_ig_oas_ma20\n",
      "  Created feature: us_ig_oas_ma60\n",
      "  Created feature: us_ig_oas_mom20\n",
      "  Created feature: us_ig_oas_vol20\n",
      "  Created feature: curve_slope_ma20\n",
      "  Created feature: curve_slope_ma60\n",
      "  Created feature: curve_slope_mom20\n",
      "  Created feature: curve_slope_vol20\n",
      "  Added curve steepness feature\n",
      "  Added risk aversion feature\n",
      "  Created 14 features for regime identification\n",
      "Applying PCA for dimensionality reduction...\n",
      "  5546 valid samples for regime identification\n",
      "  PCA explained variance ratios: [0.394 0.219 0.132 0.071 0.05 ]\n",
      "  Total explained variance: 86.61%\n",
      "  Reduced dimensionality from 14 to 5 features\n",
      "Fitting Gaussian Mixture Model with 3 components...\n",
      "  Model fit statistics: BIC=46584.31, AIC=46173.82\n",
      "\n",
      "Analyzing characteristics of Regime 0:\n",
      "  Samples in regime: 123 (2.2%)\n",
      "  Average VIX: 15.14\n",
      "  Average OAS: 102.91\n",
      "  Average Curve Slope: 9.53\n",
      "\n",
      "Analyzing characteristics of Regime 1:\n",
      "  Samples in regime: 895 (16.1%)\n",
      "  Average VIX: 32.13\n",
      "  Average OAS: 260.24\n",
      "  Average Curve Slope: 246.67\n",
      "\n",
      "Analyzing characteristics of Regime 2:\n",
      "  Samples in regime: 4528 (81.6%)\n",
      "  Average VIX: 16.38\n",
      "  Average OAS: 124.56\n",
      "  Average Curve Slope: 193.95\n",
      "\n",
      "Regime label mapping:\n",
      "  Regime 0 -> favorable\n",
      "  Regime 1 -> stressed\n",
      "  Regime 2 -> neutral\n",
      "\n",
      "Overall regime distribution:\n",
      "  neutral: 81.6%\n",
      "  stressed: 16.1%\n",
      "  favorable: 2.2%\n",
      "\n",
      "Training set regime distribution:\n",
      "  neutral: 80.14%\n",
      "  stressed: 17.56%\n",
      "  favorable: 2.30%\n",
      "\n",
      "Test set regime distribution:\n",
      "  neutral: 87.66%\n",
      "  stressed: 10.45%\n",
      "  favorable: 1.89%\n",
      "\n",
      "================================================================================\n",
      "### TRAINING MODELS FOR MULTIPLE HORIZONS ###\n",
      "================================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "## TRAINING MODELS FOR 5-DAY HORIZON ##\n",
      "----------------------------------------------------------------------\n",
      "Training traditional models...\n",
      "  Training Ridge regression...\n",
      "    Top 5 Ridge coefficients (absolute):\n",
      "      vix_ma30: -0.003392\n",
      "      vix_ma20: 0.002919\n",
      "      curve_slope_ewm10: -0.002277\n",
      "      curve_slope_ewm60: 0.002061\n",
      "      hy_ig_oas_ratio_10d: 0.001794\n",
      "  Training ElasticNet...\n",
      "    ElasticNet selected 0 non-zero features\n",
      "  Training Random Forest...\n",
      "    Top 5 Random Forest important features:\n",
      "      us_ig_er_index_mom10: 0.169776\n",
      "      er_vol_250d: 0.093734\n",
      "      us_hy_oas_mom10: 0.078360\n",
      "      vix_ewm_divergence: 0.067619\n",
      "      hy_ig_oas_ratio_250d: 0.051995\n",
      "  Training Gradient Boosting...\n",
      "    Top 5 Gradient Boosting important features:\n",
      "      us_ig_er_index_mom10: 0.128944\n",
      "      er_vol_5d: 0.122307\n",
      "      cad_us_oas_diff_250d: 0.094574\n",
      "      vix_ewm_divergence: 0.080299\n",
      "      er_vol_250d: 0.074726\n",
      "  Training XGBoost...\n",
      "    Top 5 XGBoost important features:\n",
      "      us_ig_oas_mom10: 0.050844\n",
      "      cad_us_oas_diff_250d: 0.048172\n",
      "      vix_zscore_250d: 0.046093\n",
      "      hy_ig_oas_ratio_120d: 0.045675\n",
      "      us_ig_oas_mom15: 0.043263\n",
      "  Trained 5 traditional models\n",
      "Training regime-specific models...\n",
      "  Training models for 'neutral' regime (3555 samples)...\n",
      "    Training Ridge model for neutral regime...\n",
      "      Top 3 Ridge coefficients for neutral regime:\n",
      "        curve_slope_ewm10: -0.001342\n",
      "        hy_ig_oas_ratio_10d: 0.001221\n",
      "        curve_slope_ewm60: 0.001072\n",
      "    Training Random Forest model for neutral regime...\n",
      "      Top 3 Random Forest important features for neutral regime:\n",
      "        us_ig_oas_mom10: 0.187379\n",
      "        us_hy_oas_ewm10_std: 0.118503\n",
      "        hy_ig_oas_ratio_120d: 0.101570\n",
      "  Training models for 'favorable' regime (102 samples)...\n",
      "    Training Ridge model for favorable regime...\n",
      "      Top 3 Ridge coefficients for favorable regime:\n",
      "        us_ig_er_index_mom20: -0.000642\n",
      "        us_hy_oas_mom10: -0.000570\n",
      "        us_ig_er_index_mom15: 0.000554\n",
      "    Training Random Forest model for favorable regime...\n",
      "      Top 3 Random Forest important features for favorable regime:\n",
      "        us_ig_er_index_mom3: 0.486058\n",
      "        us_ig_er_index_mom5: 0.159219\n",
      "        er_vol_10d: 0.056117\n",
      "  Training models for 'stressed' regime (779 samples)...\n",
      "    Training Ridge model for stressed regime...\n",
      "      Top 3 Ridge coefficients for stressed regime:\n",
      "        cad_oas_ma20: 0.006182\n",
      "        curve_slope_ewm10: -0.005069\n",
      "        vix_ma30: -0.004901\n",
      "    Training Random Forest model for stressed regime...\n",
      "      Top 3 Random Forest important features for stressed regime:\n",
      "        cad_us_oas_diff_250d: 0.482183\n",
      "        er_vol_250d: 0.147975\n",
      "        us_ig_er_index_mom10: 0.057083\n",
      "  Trained 6 regime-specific models\n",
      "Training advanced models...\n",
      "  Training stacking ensemble model...\n",
      "    Stacking ensemble model trained successfully\n",
      "  Training neural network model...\n",
      "Iteration 1, loss = 0.03907601\n",
      "Validation score: -2500.622947\n",
      "Iteration 2, loss = 0.00658632\n",
      "Validation score: -1458.148200\n",
      "Iteration 3, loss = 0.00537357\n",
      "Validation score: -1477.163879\n",
      "Iteration 4, loss = 0.00413446\n",
      "Validation score: -932.565355\n",
      "Iteration 5, loss = 0.00435874\n",
      "Validation score: -1013.349072\n",
      "Iteration 6, loss = 0.00201857\n",
      "Validation score: -704.923912\n",
      "Iteration 7, loss = 0.00149338\n",
      "Validation score: -493.016750\n",
      "Iteration 8, loss = 0.00220582\n",
      "Validation score: -481.966285\n",
      "Iteration 9, loss = 0.00152177\n",
      "Validation score: -613.594693\n",
      "Iteration 10, loss = 0.00176086\n",
      "Validation score: -328.521878\n",
      "Iteration 11, loss = 0.00110781\n",
      "Validation score: -288.557748\n",
      "Iteration 12, loss = 0.00085589\n",
      "Validation score: -245.431631\n",
      "Iteration 13, loss = 0.00078131\n",
      "Validation score: -281.791931\n",
      "Iteration 14, loss = 0.00077946\n",
      "Validation score: -207.355834\n",
      "Iteration 15, loss = 0.00144768\n",
      "Validation score: -230.050542\n",
      "Iteration 16, loss = 0.00075792\n",
      "Validation score: -200.880316\n",
      "Iteration 17, loss = 0.00072963\n",
      "Validation score: -199.592815\n",
      "Iteration 18, loss = 0.00056670\n",
      "Validation score: -142.801031\n",
      "Iteration 19, loss = 0.00055606\n",
      "Validation score: -157.620878\n",
      "Iteration 20, loss = 0.00047476\n",
      "Validation score: -126.808307\n",
      "Iteration 21, loss = 0.00044983\n",
      "Validation score: -137.960365\n",
      "Iteration 22, loss = 0.00046532\n",
      "Validation score: -141.860841\n",
      "Iteration 23, loss = 0.00062381\n",
      "Validation score: -127.689179\n",
      "Iteration 24, loss = 0.00041864\n",
      "Validation score: -172.283281\n",
      "Iteration 25, loss = 0.00037854\n",
      "Validation score: -99.931673\n",
      "Iteration 26, loss = 0.00041250\n",
      "Validation score: -88.634963\n",
      "Iteration 27, loss = 0.00038360\n",
      "Validation score: -107.039296\n",
      "Iteration 28, loss = 0.00049735\n",
      "Validation score: -152.676688\n",
      "Iteration 29, loss = 0.00073264\n",
      "Validation score: -78.425813\n",
      "Iteration 30, loss = 0.00036470\n",
      "Validation score: -72.696315\n",
      "Iteration 31, loss = 0.00033838\n",
      "Validation score: -61.208896\n",
      "Iteration 32, loss = 0.00030838\n",
      "Validation score: -88.991796\n",
      "Iteration 33, loss = 0.00041600\n",
      "Validation score: -54.596214\n",
      "Iteration 34, loss = 0.00035885\n",
      "Validation score: -161.031218\n",
      "Iteration 35, loss = 0.00026056\n",
      "Validation score: -76.530387\n",
      "Iteration 36, loss = 0.00031317\n",
      "Validation score: -92.509031\n",
      "Iteration 37, loss = 0.00024518\n",
      "Validation score: -45.415248\n",
      "Iteration 38, loss = 0.00021934\n",
      "Validation score: -30.929504\n",
      "Iteration 39, loss = 0.00020444\n",
      "Validation score: -27.292967\n",
      "Iteration 40, loss = 0.00019582\n",
      "Validation score: -25.971900\n",
      "Iteration 41, loss = 0.00019497\n",
      "Validation score: -36.268408\n",
      "Iteration 42, loss = 0.00019699\n",
      "Validation score: -29.217193\n",
      "Iteration 43, loss = 0.00019297\n",
      "Validation score: -23.699894\n",
      "Iteration 44, loss = 0.00018498\n",
      "Validation score: -25.267030\n",
      "Iteration 45, loss = 0.00018776\n",
      "Validation score: -22.607802\n",
      "Iteration 46, loss = 0.00022531\n",
      "Validation score: -95.852814\n",
      "Iteration 47, loss = 0.00029006\n",
      "Validation score: -28.685132\n",
      "Iteration 48, loss = 0.00022045\n",
      "Validation score: -27.439179\n",
      "Iteration 49, loss = 0.00023183\n",
      "Validation score: -24.048587\n",
      "Iteration 50, loss = 0.00016547\n",
      "Validation score: -16.383808\n",
      "Iteration 51, loss = 0.00017884\n",
      "Validation score: -16.617980\n",
      "Iteration 52, loss = 0.00017366\n",
      "Validation score: -26.372169\n",
      "Iteration 53, loss = 0.00016805\n",
      "Validation score: -33.082911\n",
      "Iteration 54, loss = 0.00017152\n",
      "Validation score: -21.219047\n",
      "Iteration 55, loss = 0.00016257\n",
      "Validation score: -32.243785\n",
      "Iteration 56, loss = 0.00018537\n",
      "Validation score: -15.611243\n",
      "Iteration 57, loss = 0.00016292\n",
      "Validation score: -10.210975\n",
      "Iteration 58, loss = 0.00014854\n",
      "Validation score: -9.915264\n",
      "Iteration 59, loss = 0.00013760\n",
      "Validation score: -10.309073\n",
      "Iteration 60, loss = 0.00013571\n",
      "Validation score: -11.877948\n",
      "Iteration 61, loss = 0.00013360\n",
      "Validation score: -8.436858\n",
      "Iteration 62, loss = 0.00012943\n",
      "Validation score: -7.381614\n",
      "Iteration 63, loss = 0.00012156\n",
      "Validation score: -7.059023\n",
      "Iteration 64, loss = 0.00011976\n",
      "Validation score: -6.361763\n",
      "Iteration 65, loss = 0.00012314\n",
      "Validation score: -13.141091\n",
      "Iteration 66, loss = 0.00012909\n",
      "Validation score: -8.326698\n",
      "Iteration 67, loss = 0.00011529\n",
      "Validation score: -10.640375\n",
      "Iteration 68, loss = 0.00012457\n",
      "Validation score: -5.506551\n",
      "Iteration 69, loss = 0.00011061\n",
      "Validation score: -6.492931\n",
      "Iteration 70, loss = 0.00010465\n",
      "Validation score: -4.592719\n",
      "Iteration 71, loss = 0.00010261\n",
      "Validation score: -3.464832\n",
      "Iteration 72, loss = 0.00010091\n",
      "Validation score: -3.985617\n",
      "Iteration 73, loss = 0.00010117\n",
      "Validation score: -3.705969\n",
      "Iteration 74, loss = 0.00009701\n",
      "Validation score: -3.124199\n",
      "Iteration 75, loss = 0.00010684\n",
      "Validation score: -3.764976\n",
      "Iteration 76, loss = 0.00011670\n",
      "Validation score: -17.298718\n",
      "Iteration 77, loss = 0.00010476\n",
      "Validation score: -3.041364\n",
      "Iteration 78, loss = 0.00011259\n",
      "Validation score: -2.460708\n",
      "Iteration 79, loss = 0.00008775\n",
      "Validation score: -0.729512\n",
      "Iteration 80, loss = 0.00008593\n",
      "Validation score: -1.242646\n",
      "Iteration 81, loss = 0.00009195\n",
      "Validation score: -2.050545\n",
      "Iteration 82, loss = 0.00010104\n",
      "Validation score: -1.403484\n",
      "Iteration 83, loss = 0.00007759\n",
      "Validation score: -0.935399\n",
      "Iteration 84, loss = 0.00007412\n",
      "Validation score: -0.211323\n",
      "Iteration 85, loss = 0.00007085\n",
      "Validation score: -0.214232\n",
      "Iteration 86, loss = 0.00006857\n",
      "Validation score: -0.012290\n",
      "Iteration 87, loss = 0.00006736\n",
      "Validation score: -1.756934\n",
      "Iteration 88, loss = 0.00006565\n",
      "Validation score: -0.316295\n",
      "Iteration 89, loss = 0.00006323\n",
      "Validation score: -1.799573\n",
      "Iteration 90, loss = 0.00006073\n",
      "Validation score: -0.296530\n",
      "Iteration 91, loss = 0.00005809\n",
      "Validation score: -0.518274\n",
      "Iteration 92, loss = 0.00005689\n",
      "Validation score: -0.118894\n",
      "Iteration 93, loss = 0.00005540\n",
      "Validation score: -1.405697\n",
      "Iteration 94, loss = 0.00005256\n",
      "Validation score: -0.027323\n",
      "Iteration 95, loss = 0.00005208\n",
      "Validation score: -0.319945\n",
      "Iteration 96, loss = 0.00004974\n",
      "Validation score: 0.002698\n",
      "Iteration 97, loss = 0.00004879\n",
      "Validation score: -1.977004\n",
      "Iteration 98, loss = 0.00004608\n",
      "Validation score: -4.264173\n",
      "Iteration 99, loss = 0.00005890\n",
      "Validation score: -2.050031\n",
      "Iteration 100, loss = 0.00004357\n",
      "Validation score: -1.022496\n",
      "Iteration 101, loss = 0.00004126\n",
      "Validation score: -1.770547\n",
      "Iteration 102, loss = 0.00003893\n",
      "Validation score: -3.523805\n",
      "Iteration 103, loss = 0.00003780\n",
      "Validation score: -2.780723\n",
      "Iteration 104, loss = 0.00003615\n",
      "Validation score: -3.319183\n",
      "Iteration 105, loss = 0.00003554\n",
      "Validation score: -1.117601\n",
      "Iteration 106, loss = 0.00003462\n",
      "Validation score: -1.393214\n",
      "Iteration 107, loss = 0.00003196\n",
      "Validation score: 0.114824\n",
      "Iteration 108, loss = 0.00003278\n",
      "Validation score: -0.939857\n",
      "Iteration 109, loss = 0.00005228\n",
      "Validation score: -16.816808\n",
      "Iteration 110, loss = 0.00005069\n",
      "Validation score: -9.952014\n",
      "Iteration 111, loss = 0.00005054\n",
      "Validation score: -0.371952\n",
      "Iteration 112, loss = 0.00002863\n",
      "Validation score: 0.010538\n",
      "Iteration 113, loss = 0.00002646\n",
      "Validation score: 0.303320\n",
      "Iteration 114, loss = 0.00002480\n",
      "Validation score: 0.447404\n",
      "Iteration 115, loss = 0.00002404\n",
      "Validation score: 0.310626\n",
      "Iteration 116, loss = 0.00002259\n",
      "Validation score: 0.350358\n",
      "Iteration 117, loss = 0.00002153\n",
      "Validation score: 0.388144\n",
      "Iteration 118, loss = 0.00002081\n",
      "Validation score: 0.406093\n",
      "Iteration 119, loss = 0.00001984\n",
      "Validation score: 0.509067\n",
      "Iteration 120, loss = 0.00001887\n",
      "Validation score: 0.425668\n",
      "Iteration 121, loss = 0.00001813\n",
      "Validation score: 0.409802\n",
      "Iteration 122, loss = 0.00001735\n",
      "Validation score: 0.466451\n",
      "Iteration 123, loss = 0.00001738\n",
      "Validation score: 0.372994\n",
      "Iteration 124, loss = 0.00001639\n",
      "Validation score: 0.446388\n",
      "Iteration 125, loss = 0.00001604\n",
      "Validation score: 0.441385\n",
      "Iteration 126, loss = 0.00001458\n",
      "Validation score: 0.503328\n",
      "Iteration 127, loss = 0.00001408\n",
      "Validation score: 0.487228\n",
      "Iteration 128, loss = 0.00001327\n",
      "Validation score: 0.506419\n",
      "Iteration 129, loss = 0.00001243\n",
      "Validation score: 0.540109\n",
      "Iteration 130, loss = 0.00001190\n",
      "Validation score: 0.479683\n",
      "Iteration 131, loss = 0.00001144\n",
      "Validation score: 0.469439\n",
      "Iteration 132, loss = 0.00001127\n",
      "Validation score: 0.350800\n",
      "Iteration 133, loss = 0.00001134\n",
      "Validation score: 0.515642\n",
      "Iteration 134, loss = 0.00001053\n",
      "Validation score: 0.538596\n",
      "Iteration 135, loss = 0.00000972\n",
      "Validation score: 0.439564\n",
      "Iteration 136, loss = 0.00000912\n",
      "Validation score: 0.455728\n",
      "Iteration 137, loss = 0.00000856\n",
      "Validation score: 0.476314\n",
      "Iteration 138, loss = 0.00000798\n",
      "Validation score: 0.520170\n",
      "Iteration 139, loss = 0.00000845\n",
      "Validation score: 0.452688\n",
      "Iteration 140, loss = 0.00000813\n",
      "Validation score: 0.485046\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "    Neural network model trained successfully\n",
      "    Training iterations: 140\n",
      "    Final loss: 0.000008\n",
      "  Training SVR model...\n",
      "    SVR model trained successfully\n",
      "    Support vectors: 0\n",
      "  Trained 3 advanced models\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "## EVALUATING MODELS FOR 5-DAY HORIZON ##\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Generating predictions for model: ridge\n",
      "\n",
      "Detailed evaluation for 5d ridge:\n",
      "  MSE: 0.00000369\n",
      "  RMSE: 0.001921\n",
      "  MAE: 0.001417\n",
      "  R: -0.1194\n",
      "  Directional Accuracy: 0.5982\n",
      "    - For actual positive returns: 0.6828\n",
      "    - For actual negative returns: 0.4407\n",
      "  Spearman Correlation: 0.2505 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.5982\n",
      "    - Profit Factor: 2.0726\n",
      "    - Sharpe Ratio: 4.0659\n",
      "    - Mean Return: 0.000461\n",
      "\n",
      "Generating predictions for model: elastic\n",
      "\n",
      "Detailed evaluation for 5d elastic:\n",
      "  MSE: 0.00000333\n",
      "  RMSE: 0.001825\n",
      "  MAE: 0.001272\n",
      "  R: -0.0105\n",
      "  Directional Accuracy: 0.6505\n",
      "    - For actual positive returns: 1.0000\n",
      "    - For actual negative returns: 0.0000\n",
      "  Spearman Correlation: nan (p=nan)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6505\n",
      "    - Profit Factor: 1.8262\n",
      "    - Sharpe Ratio: 3.3721\n",
      "    - Mean Return: 0.000386\n",
      "\n",
      "Generating predictions for model: rf\n",
      "\n",
      "Detailed evaluation for 5d rf:\n",
      "  MSE: 0.00000298\n",
      "  RMSE: 0.001726\n",
      "  MAE: 0.001206\n",
      "  R: 0.0963\n",
      "  Directional Accuracy: 0.6640\n",
      "    - For actual positive returns: 0.7659\n",
      "    - For actual negative returns: 0.4742\n",
      "  Spearman Correlation: 0.3464 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6640\n",
      "    - Profit Factor: 2.8318\n",
      "    - Sharpe Ratio: 5.7351\n",
      "    - Mean Return: 0.000631\n",
      "\n",
      "Generating predictions for model: gbr\n",
      "\n",
      "Detailed evaluation for 5d gbr:\n",
      "  MSE: 0.00000309\n",
      "  RMSE: 0.001758\n",
      "  MAE: 0.001218\n",
      "  R: 0.0623\n",
      "  Directional Accuracy: 0.6784\n",
      "    - For actual positive returns: 0.8781\n",
      "    - For actual negative returns: 0.3067\n",
      "  Spearman Correlation: 0.3528 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6784\n",
      "    - Profit Factor: 2.6921\n",
      "    - Sharpe Ratio: 5.4695\n",
      "    - Mean Return: 0.000605\n",
      "\n",
      "Generating predictions for model: xgb\n",
      "\n",
      "Detailed evaluation for 5d xgb:\n",
      "  MSE: 0.00000299\n",
      "  RMSE: 0.001729\n",
      "  MAE: 0.001204\n",
      "  R: 0.0927\n",
      "  Directional Accuracy: 0.6676\n",
      "    - For actual positive returns: 0.8338\n",
      "    - For actual negative returns: 0.3582\n",
      "  Spearman Correlation: 0.3603 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6676\n",
      "    - Profit Factor: 3.0311\n",
      "    - Sharpe Ratio: 6.0891\n",
      "    - Mean Return: 0.000665\n",
      "\n",
      "Generating predictions for model: stacking\n",
      "\n",
      "Detailed evaluation for 5d stacking:\n",
      "  MSE: 0.00000332\n",
      "  RMSE: 0.001822\n",
      "  MAE: 0.001270\n",
      "  R: -0.0070\n",
      "  Directional Accuracy: 0.6505\n",
      "    - For actual positive returns: 1.0000\n",
      "    - For actual negative returns: 0.0000\n",
      "  Spearman Correlation: 0.2607 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6505\n",
      "    - Profit Factor: 1.8262\n",
      "    - Sharpe Ratio: 3.3721\n",
      "    - Mean Return: 0.000386\n",
      "\n",
      "Generating predictions for model: nn\n",
      "\n",
      "Detailed evaluation for 5d nn:\n",
      "  MSE: 0.00000498\n",
      "  RMSE: 0.002233\n",
      "  MAE: 0.001632\n",
      "  R: -0.5127\n",
      "  Directional Accuracy: 0.6694\n",
      "    - For actual positive returns: 0.8698\n",
      "    - For actual negative returns: 0.2964\n",
      "  Spearman Correlation: 0.3167 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6694\n",
      "    - Profit Factor: 2.9476\n",
      "    - Sharpe Ratio: 5.9442\n",
      "    - Mean Return: 0.000651\n",
      "\n",
      "Generating predictions for model: svr\n",
      "\n",
      "Detailed evaluation for 5d svr:\n",
      "  MSE: 0.00009944\n",
      "  RMSE: 0.009972\n",
      "  MAE: 0.009807\n",
      "  R: -29.1752\n",
      "  Directional Accuracy: 0.3495\n",
      "    - For actual positive returns: 0.0000\n",
      "    - For actual negative returns: 1.0000\n",
      "  Spearman Correlation: nan (p=nan)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3495\n",
      "    - Profit Factor: 0.5476\n",
      "    - Sharpe Ratio: -3.3721\n",
      "    - Mean Return: -0.000386\n",
      "\n",
      "Generating regime-aware ensemble predictions\n",
      "Creating regime-based ensemble predictions...\n",
      "  Test set contains 3 regimes: neutral, stressed, favorable\n",
      "  Regime model usage:\n",
      "    neutral: 973 samples (87.7%)\n",
      "    stressed: 116 samples (10.5%)\n",
      "    favorable: 21 samples (1.9%)\n",
      "  Prediction statistics: mean=0.000437, std=0.001231\n",
      "  Prediction range: [-0.005630, 0.006651]\n",
      "\n",
      "Detailed evaluation for 5d regime_ensemble:\n",
      "  MSE: 0.00000379\n",
      "  RMSE: 0.001946\n",
      "  MAE: 0.001351\n",
      "  R: -0.1496\n",
      "  Directional Accuracy: 0.6279\n",
      "    - For actual positive returns: 0.8019\n",
      "    - For actual negative returns: 0.3041\n",
      "  Spearman Correlation: 0.2484 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6279\n",
      "    - Profit Factor: 2.2092\n",
      "    - Sharpe Ratio: 4.4124\n",
      "    - Mean Return: 0.000497\n",
      "\n",
      "Generating model ensemble predictions (average of all models)\n",
      "\n",
      "Detailed evaluation for 5d model_ensemble:\n",
      "  MSE: 0.00000452\n",
      "  RMSE: 0.002125\n",
      "  MAE: 0.001694\n",
      "  R: -0.3708\n",
      "  Directional Accuracy: 0.3739\n",
      "    - For actual positive returns: 0.0540\n",
      "    - For actual negative returns: 0.9691\n",
      "  Spearman Correlation: 0.3332 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3739\n",
      "    - Profit Factor: 0.6630\n",
      "    - Sharpe Ratio: -2.3107\n",
      "    - Mean Return: -0.000267\n",
      "\n",
      "Best model for 5-day horizon: gbr\n",
      "  Directional Accuracy: 0.6784\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "## TRAINING MODELS FOR 10-DAY HORIZON ##\n",
      "----------------------------------------------------------------------\n",
      "Training traditional models...\n",
      "  Training Ridge regression...\n",
      "    Top 5 Ridge coefficients (absolute):\n",
      "      vix_ma30: -0.004507\n",
      "      curve_slope_ewm10: -0.004494\n",
      "      curve_slope_ewm60: 0.003903\n",
      "      vix_ma20: 0.003085\n",
      "      hy_ig_oas_ratio_60d: 0.003004\n",
      "  Training ElasticNet...\n",
      "    ElasticNet selected 0 non-zero features\n",
      "  Training Random Forest...\n",
      "    Top 5 Random Forest important features:\n",
      "      er_vol_250d: 0.203909\n",
      "      us_ig_oas_mom10: 0.119746\n",
      "      hy_ig_oas_ratio_120d: 0.100180\n",
      "      us_hy_oas_mom10: 0.069073\n",
      "      cad_us_oas_diff_250d: 0.067609\n",
      "  Training Gradient Boosting...\n",
      "    Top 5 Gradient Boosting important features:\n",
      "      cad_us_oas_diff_250d: 0.206745\n",
      "      us_ig_oas_mom10: 0.138155\n",
      "      er_vol_250d: 0.119813\n",
      "      us_ig_er_index_mom10: 0.067942\n",
      "      hy_ig_oas_ratio_120d: 0.049747\n",
      "  Training XGBoost...\n",
      "    Top 5 XGBoost important features:\n",
      "      us_hy_oas_mom10: 0.078968\n",
      "      cad_us_oas_diff_250d: 0.064208\n",
      "      us_ig_oas_mom10: 0.063257\n",
      "      us_ig_er_index_mom3: 0.052996\n",
      "      vix_zscore_250d: 0.043296\n",
      "  Trained 5 traditional models\n",
      "Training regime-specific models...\n",
      "  Training models for 'neutral' regime (3555 samples)...\n",
      "    Training Ridge model for neutral regime...\n",
      "      Top 3 Ridge coefficients for neutral regime:\n",
      "        curve_slope_ewm10: -0.004298\n",
      "        curve_slope_ewm60: 0.003730\n",
      "        hy_ig_oas_ratio_10d: 0.001445\n",
      "    Training Random Forest model for neutral regime...\n",
      "      Top 3 Random Forest important features for neutral regime:\n",
      "        hy_ig_oas_ratio_120d: 0.242549\n",
      "        us_ig_oas_mom10: 0.139902\n",
      "        us_ig_oas_ewm_signal: 0.087685\n",
      "  Training models for 'favorable' regime (102 samples)...\n",
      "    Training Ridge model for favorable regime...\n",
      "      Top 3 Ridge coefficients for favorable regime:\n",
      "        vix_ewm10_std: 0.001185\n",
      "        er_vol_10d: 0.001065\n",
      "        er_trend_divergence: -0.000965\n",
      "    Training Random Forest model for favorable regime...\n",
      "      Top 3 Random Forest important features for favorable regime:\n",
      "        hy_ig_oas_ratio_250d: 0.145950\n",
      "        cad_oas_ma20: 0.113260\n",
      "        vix_ma30: 0.098287\n",
      "  Training models for 'stressed' regime (779 samples)...\n",
      "    Training Ridge model for stressed regime...\n",
      "      Top 3 Ridge coefficients for stressed regime:\n",
      "        cad_oas_ma20: 0.010707\n",
      "        hy_ig_oas_ratio_250d: 0.007464\n",
      "        cad_oas_mom60: 0.006976\n",
      "    Training Random Forest model for stressed regime...\n",
      "      Top 3 Random Forest important features for stressed regime:\n",
      "        cad_us_oas_diff_250d: 0.558849\n",
      "        er_vol_250d: 0.225698\n",
      "        hy_ig_oas_ratio_120d: 0.089188\n",
      "  Trained 6 regime-specific models\n",
      "Training advanced models...\n",
      "  Training stacking ensemble model...\n",
      "    Stacking ensemble model trained successfully\n",
      "  Training neural network model...\n",
      "Iteration 1, loss = 0.03910213\n",
      "Validation score: -845.995133\n",
      "Iteration 2, loss = 0.00661974\n",
      "Validation score: -496.843590\n",
      "Iteration 3, loss = 0.00540790\n",
      "Validation score: -499.798881\n",
      "Iteration 4, loss = 0.00427235\n",
      "Validation score: -314.336752\n",
      "Iteration 5, loss = 0.00525312\n",
      "Validation score: -352.290173\n",
      "Iteration 6, loss = 0.00197328\n",
      "Validation score: -201.264329\n",
      "Iteration 7, loss = 0.00151506\n",
      "Validation score: -165.376734\n",
      "Iteration 8, loss = 0.00159889\n",
      "Validation score: -146.480682\n",
      "Iteration 9, loss = 0.00108968\n",
      "Validation score: -129.903373\n",
      "Iteration 10, loss = 0.00105119\n",
      "Validation score: -127.556332\n",
      "Iteration 11, loss = 0.00095278\n",
      "Validation score: -120.970453\n",
      "Iteration 12, loss = 0.00081592\n",
      "Validation score: -117.542136\n",
      "Iteration 13, loss = 0.00079994\n",
      "Validation score: -104.778808\n",
      "Iteration 14, loss = 0.00232725\n",
      "Validation score: -299.168626\n",
      "Iteration 15, loss = 0.00578125\n",
      "Validation score: -115.324820\n",
      "Iteration 16, loss = 0.00118915\n",
      "Validation score: -72.703526\n",
      "Iteration 17, loss = 0.00067652\n",
      "Validation score: -54.153786\n",
      "Iteration 18, loss = 0.00056328\n",
      "Validation score: -45.518385\n",
      "Iteration 19, loss = 0.00048158\n",
      "Validation score: -43.898257\n",
      "Iteration 20, loss = 0.00042450\n",
      "Validation score: -33.770455\n",
      "Iteration 21, loss = 0.00041260\n",
      "Validation score: -33.707884\n",
      "Iteration 22, loss = 0.00037753\n",
      "Validation score: -27.304324\n",
      "Iteration 23, loss = 0.00035179\n",
      "Validation score: -27.448395\n",
      "Iteration 24, loss = 0.00032725\n",
      "Validation score: -24.705794\n",
      "Iteration 25, loss = 0.00031068\n",
      "Validation score: -23.647995\n",
      "Iteration 26, loss = 0.00033232\n",
      "Validation score: -18.985357\n",
      "Iteration 27, loss = 0.00028618\n",
      "Validation score: -17.289383\n",
      "Iteration 28, loss = 0.00027342\n",
      "Validation score: -14.337794\n",
      "Iteration 29, loss = 0.00024310\n",
      "Validation score: -14.565788\n",
      "Iteration 30, loss = 0.00023788\n",
      "Validation score: -13.651722\n",
      "Iteration 31, loss = 0.00022752\n",
      "Validation score: -12.631396\n",
      "Iteration 32, loss = 0.00021806\n",
      "Validation score: -11.732145\n",
      "Iteration 33, loss = 0.00020897\n",
      "Validation score: -10.855027\n",
      "Iteration 34, loss = 0.00020363\n",
      "Validation score: -11.634195\n",
      "Iteration 35, loss = 0.00019651\n",
      "Validation score: -11.395776\n",
      "Iteration 36, loss = 0.00019818\n",
      "Validation score: -9.881186\n",
      "Iteration 37, loss = 0.00018977\n",
      "Validation score: -9.628562\n",
      "Iteration 38, loss = 0.00018329\n",
      "Validation score: -8.520761\n",
      "Iteration 39, loss = 0.00018273\n",
      "Validation score: -7.929293\n",
      "Iteration 40, loss = 0.00017687\n",
      "Validation score: -6.870873\n",
      "Iteration 41, loss = 0.00017272\n",
      "Validation score: -6.682186\n",
      "Iteration 42, loss = 0.00016919\n",
      "Validation score: -6.332815\n",
      "Iteration 43, loss = 0.00018764\n",
      "Validation score: -7.323446\n",
      "Iteration 44, loss = 0.00020206\n",
      "Validation score: -8.105631\n",
      "Iteration 45, loss = 0.00017049\n",
      "Validation score: -6.450825\n",
      "Iteration 46, loss = 0.00016400\n",
      "Validation score: -5.614289\n",
      "Iteration 47, loss = 0.00016029\n",
      "Validation score: -5.552825\n",
      "Iteration 48, loss = 0.00015843\n",
      "Validation score: -6.768590\n",
      "Iteration 49, loss = 0.00017751\n",
      "Validation score: -8.677351\n",
      "Iteration 50, loss = 0.00016843\n",
      "Validation score: -5.099656\n",
      "Iteration 51, loss = 0.00019547\n",
      "Validation score: -4.895790\n",
      "Iteration 52, loss = 0.00015212\n",
      "Validation score: -4.126475\n",
      "Iteration 53, loss = 0.00014147\n",
      "Validation score: -2.910161\n",
      "Iteration 54, loss = 0.00014019\n",
      "Validation score: -3.614161\n",
      "Iteration 55, loss = 0.00013423\n",
      "Validation score: -3.053787\n",
      "Iteration 56, loss = 0.00013284\n",
      "Validation score: -2.134108\n",
      "Iteration 57, loss = 0.00012943\n",
      "Validation score: -2.470415\n",
      "Iteration 58, loss = 0.00012541\n",
      "Validation score: -2.126496\n",
      "Iteration 59, loss = 0.00012556\n",
      "Validation score: -2.150024\n",
      "Iteration 60, loss = 0.00012631\n",
      "Validation score: -1.709757\n",
      "Iteration 61, loss = 0.00012902\n",
      "Validation score: -3.760685\n",
      "Iteration 62, loss = 0.00012790\n",
      "Validation score: -1.814612\n",
      "Iteration 63, loss = 0.00012017\n",
      "Validation score: -2.039413\n",
      "Iteration 64, loss = 0.00011698\n",
      "Validation score: -1.932325\n",
      "Iteration 65, loss = 0.00011831\n",
      "Validation score: -2.313182\n",
      "Iteration 66, loss = 0.00012683\n",
      "Validation score: -5.370646\n",
      "Iteration 67, loss = 0.00015606\n",
      "Validation score: -6.827240\n",
      "Iteration 68, loss = 0.00013719\n",
      "Validation score: -2.405760\n",
      "Iteration 69, loss = 0.00011689\n",
      "Validation score: -1.139049\n",
      "Iteration 70, loss = 0.00010761\n",
      "Validation score: -1.996432\n",
      "Iteration 71, loss = 0.00010402\n",
      "Validation score: -1.837222\n",
      "Iteration 72, loss = 0.00010001\n",
      "Validation score: -1.304731\n",
      "Iteration 73, loss = 0.00009911\n",
      "Validation score: -4.434706\n",
      "Iteration 74, loss = 0.00011876\n",
      "Validation score: -0.883439\n",
      "Iteration 75, loss = 0.00010136\n",
      "Validation score: -0.731777\n",
      "Iteration 76, loss = 0.00009721\n",
      "Validation score: -0.938533\n",
      "Iteration 77, loss = 0.00009254\n",
      "Validation score: 0.295928\n",
      "Iteration 78, loss = 0.00008825\n",
      "Validation score: 0.307697\n",
      "Iteration 79, loss = 0.00008761\n",
      "Validation score: 0.386550\n",
      "Iteration 80, loss = 0.00010332\n",
      "Validation score: -0.100822\n",
      "Iteration 81, loss = 0.00008276\n",
      "Validation score: 0.395336\n",
      "Iteration 82, loss = 0.00008029\n",
      "Validation score: 0.423441\n",
      "Iteration 83, loss = 0.00007744\n",
      "Validation score: 0.523067\n",
      "Iteration 84, loss = 0.00007497\n",
      "Validation score: 0.550126\n",
      "Iteration 85, loss = 0.00007290\n",
      "Validation score: 0.495008\n",
      "Iteration 86, loss = 0.00007174\n",
      "Validation score: 0.471992\n",
      "Iteration 87, loss = 0.00006963\n",
      "Validation score: 0.521551\n",
      "Iteration 88, loss = 0.00006827\n",
      "Validation score: 0.615018\n",
      "Iteration 89, loss = 0.00007019\n",
      "Validation score: -0.136124\n",
      "Iteration 90, loss = 0.00006730\n",
      "Validation score: 0.074598\n",
      "Iteration 91, loss = 0.00006189\n",
      "Validation score: 0.232219\n",
      "Iteration 92, loss = 0.00005978\n",
      "Validation score: 0.573661\n",
      "Iteration 93, loss = 0.00006196\n",
      "Validation score: -0.541993\n",
      "Iteration 94, loss = 0.00005930\n",
      "Validation score: -0.195346\n",
      "Iteration 95, loss = 0.00006214\n",
      "Validation score: 0.431947\n",
      "Iteration 96, loss = 0.00005633\n",
      "Validation score: 0.382757\n",
      "Iteration 97, loss = 0.00005373\n",
      "Validation score: 0.625981\n",
      "Iteration 98, loss = 0.00005506\n",
      "Validation score: -0.000290\n",
      "Iteration 99, loss = 0.00005228\n",
      "Validation score: -0.213345\n",
      "Iteration 100, loss = 0.00005377\n",
      "Validation score: -0.254391\n",
      "Iteration 101, loss = 0.00005052\n",
      "Validation score: -0.043694\n",
      "Iteration 102, loss = 0.00004452\n",
      "Validation score: 0.368030\n",
      "Iteration 103, loss = 0.00004438\n",
      "Validation score: 0.008354\n",
      "Iteration 104, loss = 0.00005673\n",
      "Validation score: 0.462758\n",
      "Iteration 105, loss = 0.00004198\n",
      "Validation score: 0.392689\n",
      "Iteration 106, loss = 0.00003852\n",
      "Validation score: 0.509221\n",
      "Iteration 107, loss = 0.00003678\n",
      "Validation score: 0.672766\n",
      "Iteration 108, loss = 0.00003486\n",
      "Validation score: 0.523318\n",
      "Iteration 109, loss = 0.00003315\n",
      "Validation score: 0.653118\n",
      "Iteration 110, loss = 0.00003286\n",
      "Validation score: 0.563818\n",
      "Iteration 111, loss = 0.00003220\n",
      "Validation score: 0.686496\n",
      "Iteration 112, loss = 0.00003029\n",
      "Validation score: 0.672715\n",
      "Iteration 113, loss = 0.00002909\n",
      "Validation score: 0.583352\n",
      "Iteration 114, loss = 0.00002947\n",
      "Validation score: 0.394936\n",
      "Iteration 115, loss = 0.00002692\n",
      "Validation score: 0.685679\n",
      "Iteration 116, loss = 0.00002536\n",
      "Validation score: 0.673726\n",
      "Iteration 117, loss = 0.00002526\n",
      "Validation score: 0.266229\n",
      "Iteration 118, loss = 0.00002743\n",
      "Validation score: -0.047184\n",
      "Iteration 119, loss = 0.00002591\n",
      "Validation score: 0.032121\n",
      "Iteration 120, loss = 0.00002703\n",
      "Validation score: 0.178127\n",
      "Iteration 121, loss = 0.00002504\n",
      "Validation score: 0.402552\n",
      "Iteration 122, loss = 0.00002491\n",
      "Validation score: -0.042837\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "    Neural network model trained successfully\n",
      "    Training iterations: 122\n",
      "    Final loss: 0.000025\n",
      "  Training SVR model...\n",
      "    SVR model trained successfully\n",
      "    Support vectors: 0\n",
      "  Trained 3 advanced models\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "## EVALUATING MODELS FOR 10-DAY HORIZON ##\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Generating predictions for model: ridge\n",
      "\n",
      "Detailed evaluation for 10d ridge:\n",
      "  MSE: 0.00001140\n",
      "  RMSE: 0.003376\n",
      "  MAE: 0.002493\n",
      "  R: -0.3012\n",
      "  Directional Accuracy: 0.6000\n",
      "    - For actual positive returns: 0.7058\n",
      "    - For actual negative returns: 0.3875\n",
      "  Spearman Correlation: 0.2154 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6000\n",
      "    - Profit Factor: 1.9384\n",
      "    - Sharpe Ratio: 3.7792\n",
      "    - Mean Return: 0.000709\n",
      "\n",
      "Generating predictions for model: elastic\n",
      "\n",
      "Detailed evaluation for 10d elastic:\n",
      "  MSE: 0.00000890\n",
      "  RMSE: 0.002983\n",
      "  MAE: 0.002116\n",
      "  R: -0.0159\n",
      "  Directional Accuracy: 0.6676\n",
      "    - For actual positive returns: 1.0000\n",
      "    - For actual negative returns: 0.0000\n",
      "  Spearman Correlation: nan (p=nan)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6676\n",
      "    - Profit Factor: 2.0851\n",
      "    - Sharpe Ratio: 4.1875\n",
      "    - Mean Return: 0.000781\n",
      "\n",
      "Generating predictions for model: rf\n",
      "\n",
      "Detailed evaluation for 10d rf:\n",
      "  MSE: 0.00000996\n",
      "  RMSE: 0.003156\n",
      "  MAE: 0.002240\n",
      "  R: -0.1372\n",
      "  Directional Accuracy: 0.6685\n",
      "    - For actual positive returns: 0.7490\n",
      "    - For actual negative returns: 0.5068\n",
      "  Spearman Correlation: 0.3172 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6685\n",
      "    - Profit Factor: 2.7308\n",
      "    - Sharpe Ratio: 5.6714\n",
      "    - Mean Return: 0.001030\n",
      "\n",
      "Generating predictions for model: gbr\n",
      "\n",
      "Detailed evaluation for 10d gbr:\n",
      "  MSE: 0.00001228\n",
      "  RMSE: 0.003505\n",
      "  MAE: 0.002270\n",
      "  R: -0.4025\n",
      "  Directional Accuracy: 0.6541\n",
      "    - For actual positive returns: 0.7395\n",
      "    - For actual negative returns: 0.4824\n",
      "  Spearman Correlation: 0.3035 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6541\n",
      "    - Profit Factor: 2.4076\n",
      "    - Sharpe Ratio: 4.9843\n",
      "    - Mean Return: 0.000917\n",
      "\n",
      "Generating predictions for model: xgb\n",
      "\n",
      "Detailed evaluation for 10d xgb:\n",
      "  MSE: 0.00000952\n",
      "  RMSE: 0.003086\n",
      "  MAE: 0.002129\n",
      "  R: -0.0873\n",
      "  Directional Accuracy: 0.6748\n",
      "    - For actual positive returns: 0.8178\n",
      "    - For actual negative returns: 0.3875\n",
      "  Spearman Correlation: 0.3233 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6748\n",
      "    - Profit Factor: 2.6201\n",
      "    - Sharpe Ratio: 5.4469\n",
      "    - Mean Return: 0.000994\n",
      "\n",
      "Generating predictions for model: stacking\n",
      "\n",
      "Detailed evaluation for 10d stacking:\n",
      "  MSE: 0.00000880\n",
      "  RMSE: 0.002967\n",
      "  MAE: 0.002105\n",
      "  R: -0.0052\n",
      "  Directional Accuracy: 0.6676\n",
      "    - For actual positive returns: 1.0000\n",
      "    - For actual negative returns: 0.0000\n",
      "  Spearman Correlation: 0.2143 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6676\n",
      "    - Profit Factor: 2.0851\n",
      "    - Sharpe Ratio: 4.1875\n",
      "    - Mean Return: 0.000781\n",
      "\n",
      "Generating predictions for model: nn\n",
      "\n",
      "Detailed evaluation for 10d nn:\n",
      "  MSE: 0.00001315\n",
      "  RMSE: 0.003626\n",
      "  MAE: 0.002510\n",
      "  R: -0.5008\n",
      "  Directional Accuracy: 0.6541\n",
      "    - For actual positive returns: 0.7490\n",
      "    - For actual negative returns: 0.4634\n",
      "  Spearman Correlation: 0.2726 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6541\n",
      "    - Profit Factor: 2.3769\n",
      "    - Sharpe Ratio: 4.9138\n",
      "    - Mean Return: 0.000905\n",
      "\n",
      "Generating predictions for model: svr\n",
      "\n",
      "Detailed evaluation for 10d svr:\n",
      "  MSE: 0.00041783\n",
      "  RMSE: 0.020441\n",
      "  MAE: 0.020226\n",
      "  R: -46.7025\n",
      "  Directional Accuracy: 0.3324\n",
      "    - For actual positive returns: 0.0000\n",
      "    - For actual negative returns: 1.0000\n",
      "  Spearman Correlation: nan (p=nan)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3324\n",
      "    - Profit Factor: 0.4796\n",
      "    - Sharpe Ratio: -4.1875\n",
      "    - Mean Return: -0.000781\n",
      "\n",
      "Generating regime-aware ensemble predictions\n",
      "Creating regime-based ensemble predictions...\n",
      "  Test set contains 3 regimes: neutral, stressed, favorable\n",
      "  Regime model usage:\n",
      "    neutral: 973 samples (87.7%)\n",
      "    stressed: 116 samples (10.5%)\n",
      "    favorable: 21 samples (1.9%)\n",
      "  Prediction statistics: mean=0.000535, std=0.002187\n",
      "  Prediction range: [-0.017310, 0.013001]\n",
      "\n",
      "Detailed evaluation for 10d regime_ensemble:\n",
      "  MSE: 0.00001145\n",
      "  RMSE: 0.003383\n",
      "  MAE: 0.002388\n",
      "  R: -0.3069\n",
      "  Directional Accuracy: 0.6027\n",
      "    - For actual positive returns: 0.7126\n",
      "    - For actual negative returns: 0.3821\n",
      "  Spearman Correlation: 0.1734 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6027\n",
      "    - Profit Factor: 2.1007\n",
      "    - Sharpe Ratio: 4.2291\n",
      "    - Mean Return: 0.000788\n",
      "\n",
      "Generating model ensemble predictions (average of all models)\n",
      "\n",
      "Detailed evaluation for 10d model_ensemble:\n",
      "  MSE: 0.00001603\n",
      "  RMSE: 0.004004\n",
      "  MAE: 0.003301\n",
      "  R: -0.8306\n",
      "  Directional Accuracy: 0.3450\n",
      "    - For actual positive returns: 0.0270\n",
      "    - For actual negative returns: 0.9837\n",
      "  Spearman Correlation: 0.2972 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3450\n",
      "    - Profit Factor: 0.5329\n",
      "    - Sharpe Ratio: -3.5970\n",
      "    - Mean Return: -0.000677\n",
      "\n",
      "Best model for 10-day horizon: xgb\n",
      "  Directional Accuracy: 0.6748\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "## TRAINING MODELS FOR 20-DAY HORIZON ##\n",
      "----------------------------------------------------------------------\n",
      "Training traditional models...\n",
      "  Training Ridge regression...\n",
      "    Top 5 Ridge coefficients (absolute):\n",
      "      hy_ig_oas_ratio_20d: -0.011630\n",
      "      curve_slope_ewm10: -0.009296\n",
      "      curve_slope_ewm60: 0.007663\n",
      "      hy_ig_oas_ratio_120d: 0.006206\n",
      "      hy_ig_oas_ratio_10d: 0.005185\n",
      "  Training ElasticNet...\n",
      "    ElasticNet selected 2 non-zero features\n",
      "    Top 5 ElasticNet coefficients (absolute):\n",
      "      us_ig_er_index_mom3: 0.000188\n",
      "      us_ig_er_index_mom5: 0.000020\n",
      "  Training Random Forest...\n",
      "    Top 5 Random Forest important features:\n",
      "      er_vol_250d: 0.266944\n",
      "      hy_ig_oas_ratio_120d: 0.190557\n",
      "      cad_us_oas_diff_250d: 0.145515\n",
      "      hy_ig_oas_ratio_10d: 0.034028\n",
      "      vix_ma30: 0.033476\n",
      "  Training Gradient Boosting...\n",
      "    Top 5 Gradient Boosting important features:\n",
      "      er_vol_250d: 0.227896\n",
      "      cad_us_oas_diff_250d: 0.203589\n",
      "      hy_ig_oas_ratio_120d: 0.174593\n",
      "      cad_oas_ma120: 0.078023\n",
      "      us_ig_er_index_mom3: 0.047949\n",
      "  Training XGBoost...\n",
      "    Top 5 XGBoost important features:\n",
      "      vix_ma20: 0.077970\n",
      "      er_vol_250d: 0.073136\n",
      "      us_ig_oas_mom10: 0.067149\n",
      "      cad_oas_ma120: 0.064934\n",
      "      us_hy_oas_mom10: 0.052505\n",
      "  Trained 5 traditional models\n",
      "Training regime-specific models...\n",
      "  Training models for 'neutral' regime (3555 samples)...\n",
      "    Training Ridge model for neutral regime...\n",
      "      Top 3 Ridge coefficients for neutral regime:\n",
      "        curve_slope_ewm10: -0.008812\n",
      "        hy_ig_oas_ratio_20d: -0.008454\n",
      "        curve_slope_ewm60: 0.008035\n",
      "    Training Random Forest model for neutral regime...\n",
      "      Top 3 Random Forest important features for neutral regime:\n",
      "        cad_us_oas_diff_250d: 0.276267\n",
      "        hy_ig_oas_ratio_120d: 0.249187\n",
      "        vix_ma60: 0.100192\n",
      "  Training models for 'favorable' regime (102 samples)...\n",
      "    Training Ridge model for favorable regime...\n",
      "      Top 3 Ridge coefficients for favorable regime:\n",
      "        us_hy_oas_mom20: -0.001123\n",
      "        cad_er_rsi_14: 0.001009\n",
      "        cad_ig_er_index_mom5: -0.000733\n",
      "    Training Random Forest model for favorable regime...\n",
      "      Top 3 Random Forest important features for favorable regime:\n",
      "        hy_ig_oas_ratio_250d: 0.495772\n",
      "        hy_ig_oas_ratio_120d: 0.361502\n",
      "        curve_slope_ewm10: 0.015331\n",
      "  Training models for 'stressed' regime (779 samples)...\n",
      "    Training Ridge model for stressed regime...\n",
      "      Top 3 Ridge coefficients for stressed regime:\n",
      "        cad_oas_ma20: 0.013193\n",
      "        hy_ig_oas_ratio_20d: -0.011612\n",
      "        cad_oas_mom60: 0.010631\n",
      "    Training Random Forest model for stressed regime...\n",
      "      Top 3 Random Forest important features for stressed regime:\n",
      "        er_vol_250d: 0.383776\n",
      "        cad_us_oas_diff_250d: 0.369747\n",
      "        hy_ig_oas_ratio_120d: 0.156279\n",
      "  Trained 6 regime-specific models\n",
      "Training advanced models...\n",
      "  Training stacking ensemble model...\n",
      "    Stacking ensemble model trained successfully\n",
      "  Training neural network model...\n",
      "Iteration 1, loss = 0.03909701\n",
      "Validation score: -386.119886\n",
      "Iteration 2, loss = 0.00658396\n",
      "Validation score: -221.511918\n",
      "Iteration 3, loss = 0.00637405\n",
      "Validation score: -597.307871\n",
      "Iteration 4, loss = 0.00483828\n",
      "Validation score: -150.013754\n",
      "Iteration 5, loss = 0.00629911\n",
      "Validation score: -206.519041\n",
      "Iteration 6, loss = 0.00227166\n",
      "Validation score: -88.761283\n",
      "Iteration 7, loss = 0.00151329\n",
      "Validation score: -75.130852\n",
      "Iteration 8, loss = 0.00181529\n",
      "Validation score: -74.115058\n",
      "Iteration 9, loss = 0.00134542\n",
      "Validation score: -79.610562\n",
      "Iteration 10, loss = 0.00115216\n",
      "Validation score: -56.211236\n",
      "Iteration 11, loss = 0.00097717\n",
      "Validation score: -46.192803\n",
      "Iteration 12, loss = 0.00087879\n",
      "Validation score: -92.015675\n",
      "Iteration 13, loss = 0.00110635\n",
      "Validation score: -53.723477\n",
      "Iteration 14, loss = 0.00178785\n",
      "Validation score: -44.395644\n",
      "Iteration 15, loss = 0.00367084\n",
      "Validation score: -47.936326\n",
      "Iteration 16, loss = 0.00101512\n",
      "Validation score: -33.555120\n",
      "Iteration 17, loss = 0.00062106\n",
      "Validation score: -24.143109\n",
      "Iteration 18, loss = 0.00058227\n",
      "Validation score: -21.346320\n",
      "Iteration 19, loss = 0.00050708\n",
      "Validation score: -20.512969\n",
      "Iteration 20, loss = 0.00045174\n",
      "Validation score: -19.591659\n",
      "Iteration 21, loss = 0.00042231\n",
      "Validation score: -16.615919\n",
      "Iteration 22, loss = 0.00038893\n",
      "Validation score: -15.046118\n",
      "Iteration 23, loss = 0.00038328\n",
      "Validation score: -15.070260\n",
      "Iteration 24, loss = 0.00040094\n",
      "Validation score: -15.741208\n",
      "Iteration 25, loss = 0.00036093\n",
      "Validation score: -14.377545\n",
      "Iteration 26, loss = 0.00043982\n",
      "Validation score: -13.089284\n",
      "Iteration 27, loss = 0.00031723\n",
      "Validation score: -10.303356\n",
      "Iteration 28, loss = 0.00031149\n",
      "Validation score: -11.307203\n",
      "Iteration 29, loss = 0.00034134\n",
      "Validation score: -20.552503\n",
      "Iteration 30, loss = 0.00030882\n",
      "Validation score: -10.771145\n",
      "Iteration 31, loss = 0.00028817\n",
      "Validation score: -9.072768\n",
      "Iteration 32, loss = 0.00026159\n",
      "Validation score: -8.364184\n",
      "Iteration 33, loss = 0.00025721\n",
      "Validation score: -7.770921\n",
      "Iteration 34, loss = 0.00024273\n",
      "Validation score: -8.509055\n",
      "Iteration 35, loss = 0.00023466\n",
      "Validation score: -8.315134\n",
      "Iteration 36, loss = 0.00023331\n",
      "Validation score: -7.947056\n",
      "Iteration 37, loss = 0.00022743\n",
      "Validation score: -6.375587\n",
      "Iteration 38, loss = 0.00022093\n",
      "Validation score: -6.606876\n",
      "Iteration 39, loss = 0.00023439\n",
      "Validation score: -7.708201\n",
      "Iteration 40, loss = 0.00023034\n",
      "Validation score: -6.536279\n",
      "Iteration 41, loss = 0.00027901\n",
      "Validation score: -5.967204\n",
      "Iteration 42, loss = 0.00027184\n",
      "Validation score: -5.968637\n",
      "Iteration 43, loss = 0.00049101\n",
      "Validation score: -14.426537\n",
      "Iteration 44, loss = 0.00036060\n",
      "Validation score: -4.850582\n",
      "Iteration 45, loss = 0.00023382\n",
      "Validation score: -3.861516\n",
      "Iteration 46, loss = 0.00021017\n",
      "Validation score: -6.362944\n",
      "Iteration 47, loss = 0.00019000\n",
      "Validation score: -3.576368\n",
      "Iteration 48, loss = 0.00017847\n",
      "Validation score: -3.578437\n",
      "Iteration 49, loss = 0.00017495\n",
      "Validation score: -3.412048\n",
      "Iteration 50, loss = 0.00016735\n",
      "Validation score: -2.842844\n",
      "Iteration 51, loss = 0.00016048\n",
      "Validation score: -2.682826\n",
      "Iteration 52, loss = 0.00015743\n",
      "Validation score: -2.460021\n",
      "Iteration 53, loss = 0.00015368\n",
      "Validation score: -2.415419\n",
      "Iteration 54, loss = 0.00015578\n",
      "Validation score: -2.170427\n",
      "Iteration 55, loss = 0.00014835\n",
      "Validation score: -1.907157\n",
      "Iteration 56, loss = 0.00015259\n",
      "Validation score: -2.425098\n",
      "Iteration 57, loss = 0.00016696\n",
      "Validation score: -2.290151\n",
      "Iteration 58, loss = 0.00016049\n",
      "Validation score: -2.638667\n",
      "Iteration 59, loss = 0.00014777\n",
      "Validation score: -1.469570\n",
      "Iteration 60, loss = 0.00014463\n",
      "Validation score: -1.945093\n",
      "Iteration 61, loss = 0.00014826\n",
      "Validation score: -2.992923\n",
      "Iteration 62, loss = 0.00013426\n",
      "Validation score: -1.694918\n",
      "Iteration 63, loss = 0.00013234\n",
      "Validation score: -1.725003\n",
      "Iteration 64, loss = 0.00012759\n",
      "Validation score: -1.661704\n",
      "Iteration 65, loss = 0.00012572\n",
      "Validation score: -3.143469\n",
      "Iteration 66, loss = 0.00012730\n",
      "Validation score: -1.794341\n",
      "Iteration 67, loss = 0.00012396\n",
      "Validation score: -0.948209\n",
      "Iteration 68, loss = 0.00015300\n",
      "Validation score: -6.251850\n",
      "Iteration 69, loss = 0.00017416\n",
      "Validation score: -1.003467\n",
      "Iteration 70, loss = 0.00013839\n",
      "Validation score: -2.592224\n",
      "Iteration 71, loss = 0.00012742\n",
      "Validation score: -0.931617\n",
      "Iteration 72, loss = 0.00011037\n",
      "Validation score: -0.585290\n",
      "Iteration 73, loss = 0.00010668\n",
      "Validation score: -0.925845\n",
      "Iteration 74, loss = 0.00010617\n",
      "Validation score: -0.796296\n",
      "Iteration 75, loss = 0.00010480\n",
      "Validation score: -0.341728\n",
      "Iteration 76, loss = 0.00010306\n",
      "Validation score: -0.422145\n",
      "Iteration 77, loss = 0.00009865\n",
      "Validation score: -0.334508\n",
      "Iteration 78, loss = 0.00009687\n",
      "Validation score: -0.355398\n",
      "Iteration 79, loss = 0.00009408\n",
      "Validation score: -0.372204\n",
      "Iteration 80, loss = 0.00009804\n",
      "Validation score: -0.955305\n",
      "Iteration 81, loss = 0.00009297\n",
      "Validation score: 0.039123\n",
      "Iteration 82, loss = 0.00009108\n",
      "Validation score: -0.245769\n",
      "Iteration 83, loss = 0.00009130\n",
      "Validation score: -1.059148\n",
      "Iteration 84, loss = 0.00009093\n",
      "Validation score: -0.028490\n",
      "Iteration 85, loss = 0.00008861\n",
      "Validation score: -0.212667\n",
      "Iteration 86, loss = 0.00008410\n",
      "Validation score: 0.162672\n",
      "Iteration 87, loss = 0.00008279\n",
      "Validation score: -0.025148\n",
      "Iteration 88, loss = 0.00007776\n",
      "Validation score: -0.122395\n",
      "Iteration 89, loss = 0.00007596\n",
      "Validation score: 0.060584\n",
      "Iteration 90, loss = 0.00008704\n",
      "Validation score: 0.268740\n",
      "Iteration 91, loss = 0.00008202\n",
      "Validation score: -0.016432\n",
      "Iteration 92, loss = 0.00007159\n",
      "Validation score: 0.445851\n",
      "Iteration 93, loss = 0.00007177\n",
      "Validation score: -0.002617\n",
      "Iteration 94, loss = 0.00006837\n",
      "Validation score: 0.477442\n",
      "Iteration 95, loss = 0.00006472\n",
      "Validation score: 0.480492\n",
      "Iteration 96, loss = 0.00006545\n",
      "Validation score: 0.347497\n",
      "Iteration 97, loss = 0.00006072\n",
      "Validation score: 0.536635\n",
      "Iteration 98, loss = 0.00005743\n",
      "Validation score: 0.598085\n",
      "Iteration 99, loss = 0.00005565\n",
      "Validation score: 0.567872\n",
      "Iteration 100, loss = 0.00005551\n",
      "Validation score: 0.530122\n",
      "Iteration 101, loss = 0.00005458\n",
      "Validation score: 0.510820\n",
      "Iteration 102, loss = 0.00005249\n",
      "Validation score: 0.373867\n",
      "Iteration 103, loss = 0.00005691\n",
      "Validation score: 0.393626\n",
      "Iteration 104, loss = 0.00006176\n",
      "Validation score: 0.394369\n",
      "Iteration 105, loss = 0.00005114\n",
      "Validation score: 0.492484\n",
      "Iteration 106, loss = 0.00004736\n",
      "Validation score: 0.609292\n",
      "Iteration 107, loss = 0.00004439\n",
      "Validation score: 0.653455\n",
      "Iteration 108, loss = 0.00004292\n",
      "Validation score: 0.589142\n",
      "Iteration 109, loss = 0.00004178\n",
      "Validation score: 0.459677\n",
      "Iteration 110, loss = 0.00003954\n",
      "Validation score: 0.672486\n",
      "Iteration 111, loss = 0.00003923\n",
      "Validation score: 0.682855\n",
      "Iteration 112, loss = 0.00003691\n",
      "Validation score: 0.688243\n",
      "Iteration 113, loss = 0.00003644\n",
      "Validation score: 0.689337\n",
      "Iteration 114, loss = 0.00004987\n",
      "Validation score: 0.285805\n",
      "Iteration 115, loss = 0.00006690\n",
      "Validation score: 0.387306\n",
      "Iteration 116, loss = 0.00004274\n",
      "Validation score: 0.605430\n",
      "Iteration 117, loss = 0.00003846\n",
      "Validation score: 0.672467\n",
      "Iteration 118, loss = 0.00003451\n",
      "Validation score: 0.538096\n",
      "Iteration 119, loss = 0.00003177\n",
      "Validation score: 0.738319\n",
      "Iteration 120, loss = 0.00003135\n",
      "Validation score: 0.730215\n",
      "Iteration 121, loss = 0.00002898\n",
      "Validation score: 0.736598\n",
      "Iteration 122, loss = 0.00002948\n",
      "Validation score: 0.552168\n",
      "Iteration 123, loss = 0.00002801\n",
      "Validation score: 0.698840\n",
      "Iteration 124, loss = 0.00003019\n",
      "Validation score: 0.675763\n",
      "Iteration 125, loss = 0.00003131\n",
      "Validation score: 0.764427\n",
      "Iteration 126, loss = 0.00002580\n",
      "Validation score: 0.679737\n",
      "Iteration 127, loss = 0.00002354\n",
      "Validation score: 0.738350\n",
      "Iteration 128, loss = 0.00002247\n",
      "Validation score: 0.657284\n",
      "Iteration 129, loss = 0.00002221\n",
      "Validation score: 0.717883\n",
      "Iteration 130, loss = 0.00002231\n",
      "Validation score: 0.703765\n",
      "Iteration 131, loss = 0.00002372\n",
      "Validation score: 0.554101\n",
      "Iteration 132, loss = 0.00002086\n",
      "Validation score: 0.743345\n",
      "Iteration 133, loss = 0.00002034\n",
      "Validation score: 0.771103\n",
      "Iteration 134, loss = 0.00001905\n",
      "Validation score: 0.793265\n",
      "Iteration 135, loss = 0.00001866\n",
      "Validation score: 0.785633\n",
      "Iteration 136, loss = 0.00001645\n",
      "Validation score: 0.704747\n",
      "Iteration 137, loss = 0.00001737\n",
      "Validation score: 0.706723\n",
      "Iteration 138, loss = 0.00001677\n",
      "Validation score: 0.803147\n",
      "Iteration 139, loss = 0.00001770\n",
      "Validation score: 0.571384\n",
      "Iteration 140, loss = 0.00002252\n",
      "Validation score: 0.581327\n",
      "Iteration 141, loss = 0.00001886\n",
      "Validation score: 0.747117\n",
      "Iteration 142, loss = 0.00001558\n",
      "Validation score: 0.786252\n",
      "Iteration 143, loss = 0.00001405\n",
      "Validation score: 0.850272\n",
      "Iteration 144, loss = 0.00001378\n",
      "Validation score: 0.738511\n",
      "Iteration 145, loss = 0.00001383\n",
      "Validation score: 0.782608\n",
      "Iteration 146, loss = 0.00001360\n",
      "Validation score: 0.847953\n",
      "Iteration 147, loss = 0.00001366\n",
      "Validation score: 0.806800\n",
      "Iteration 148, loss = 0.00001314\n",
      "Validation score: 0.834315\n",
      "Iteration 149, loss = 0.00001618\n",
      "Validation score: 0.661736\n",
      "Iteration 150, loss = 0.00002062\n",
      "Validation score: 0.743522\n",
      "Iteration 151, loss = 0.00001456\n",
      "Validation score: 0.850550\n",
      "Iteration 152, loss = 0.00001191\n",
      "Validation score: 0.853133\n",
      "Iteration 153, loss = 0.00001150\n",
      "Validation score: 0.839706\n",
      "Iteration 154, loss = 0.00001153\n",
      "Validation score: 0.779909\n",
      "Iteration 155, loss = 0.00001236\n",
      "Validation score: 0.797936\n",
      "Iteration 156, loss = 0.00001354\n",
      "Validation score: 0.802934\n",
      "Iteration 157, loss = 0.00001137\n",
      "Validation score: 0.842628\n",
      "Iteration 158, loss = 0.00000996\n",
      "Validation score: 0.830770\n",
      "Iteration 159, loss = 0.00000940\n",
      "Validation score: 0.820459\n",
      "Iteration 160, loss = 0.00002324\n",
      "Validation score: 0.483716\n",
      "Iteration 161, loss = 0.00001974\n",
      "Validation score: 0.752918\n",
      "Iteration 162, loss = 0.00001691\n",
      "Validation score: 0.712750\n",
      "Iteration 163, loss = 0.00001495\n",
      "Validation score: 0.744424\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "    Neural network model trained successfully\n",
      "    Training iterations: 163\n",
      "    Final loss: 0.000015\n",
      "  Training SVR model...\n",
      "    SVR model trained successfully\n",
      "    Support vectors: 0\n",
      "  Trained 3 advanced models\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "## EVALUATING MODELS FOR 20-DAY HORIZON ##\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Generating predictions for model: ridge\n",
      "\n",
      "Detailed evaluation for 20d ridge:\n",
      "  MSE: 0.00002955\n",
      "  RMSE: 0.005436\n",
      "  MAE: 0.004202\n",
      "  R: -0.4612\n",
      "  Directional Accuracy: 0.6333\n",
      "    - For actual positive returns: 0.7818\n",
      "    - For actual negative returns: 0.2971\n",
      "  Spearman Correlation: 0.1677 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6333\n",
      "    - Profit Factor: 1.8716\n",
      "    - Sharpe Ratio: 3.8014\n",
      "    - Mean Return: 0.001109\n",
      "\n",
      "Generating predictions for model: elastic\n",
      "\n",
      "Detailed evaluation for 20d elastic:\n",
      "  MSE: 0.00002025\n",
      "  RMSE: 0.004500\n",
      "  MAE: 0.003352\n",
      "  R: -0.0012\n",
      "  Directional Accuracy: 0.6937\n",
      "    - For actual positive returns: 0.9948\n",
      "    - For actual negative returns: 0.0118\n",
      "  Spearman Correlation: 0.2597 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6937\n",
      "    - Profit Factor: 2.4649\n",
      "    - Sharpe Ratio: 5.4441\n",
      "    - Mean Return: 0.001545\n",
      "\n",
      "Generating predictions for model: rf\n",
      "\n",
      "Detailed evaluation for 20d rf:\n",
      "  MSE: 0.00061551\n",
      "  RMSE: 0.024810\n",
      "  MAE: 0.023151\n",
      "  R: -29.4329\n",
      "  Directional Accuracy: 0.3369\n",
      "    - For actual positive returns: 0.0948\n",
      "    - For actual negative returns: 0.8853\n",
      "  Spearman Correlation: 0.0349 (p=0.2458)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3369\n",
      "    - Profit Factor: 0.5716\n",
      "    - Sharpe Ratio: -3.3954\n",
      "    - Mean Return: -0.000996\n",
      "\n",
      "Generating predictions for model: gbr\n",
      "\n",
      "Detailed evaluation for 20d gbr:\n",
      "  MSE: 0.00016125\n",
      "  RMSE: 0.012698\n",
      "  MAE: 0.011349\n",
      "  R: -6.9725\n",
      "  Directional Accuracy: 0.3369\n",
      "    - For actual positive returns: 0.0948\n",
      "    - For actual negative returns: 0.8853\n",
      "  Spearman Correlation: 0.1533 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3369\n",
      "    - Profit Factor: 0.5716\n",
      "    - Sharpe Ratio: -3.3954\n",
      "    - Mean Return: -0.000996\n",
      "\n",
      "Generating predictions for model: xgb\n",
      "\n",
      "Detailed evaluation for 20d xgb:\n",
      "  MSE: 0.00012444\n",
      "  RMSE: 0.011155\n",
      "  MAE: 0.009584\n",
      "  R: -5.1527\n",
      "  Directional Accuracy: 0.3369\n",
      "    - For actual positive returns: 0.0948\n",
      "    - For actual negative returns: 0.8853\n",
      "  Spearman Correlation: 0.1761 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3369\n",
      "    - Profit Factor: 0.5716\n",
      "    - Sharpe Ratio: -3.3954\n",
      "    - Mean Return: -0.000996\n",
      "\n",
      "Generating predictions for model: stacking\n",
      "\n",
      "Detailed evaluation for 20d stacking:\n",
      "  MSE: 0.00002078\n",
      "  RMSE: 0.004559\n",
      "  MAE: 0.003434\n",
      "  R: -0.0277\n",
      "  Directional Accuracy: 0.6937\n",
      "    - For actual positive returns: 0.9961\n",
      "    - For actual negative returns: 0.0088\n",
      "  Spearman Correlation: 0.1353 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6937\n",
      "    - Profit Factor: 2.4498\n",
      "    - Sharpe Ratio: 5.4078\n",
      "    - Mean Return: 0.001536\n",
      "\n",
      "Generating predictions for model: nn\n",
      "\n",
      "Detailed evaluation for 20d nn:\n",
      "  MSE: 0.00022125\n",
      "  RMSE: 0.014875\n",
      "  MAE: 0.009254\n",
      "  R: -9.9394\n",
      "  Directional Accuracy: 0.5910\n",
      "    - For actual positive returns: 0.6727\n",
      "    - For actual negative returns: 0.4059\n",
      "  Spearman Correlation: 0.1007 (p=0.0008)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.5910\n",
      "    - Profit Factor: 1.7402\n",
      "    - Sharpe Ratio: 3.3633\n",
      "    - Mean Return: 0.000987\n",
      "\n",
      "Generating predictions for model: svr\n",
      "\n",
      "Detailed evaluation for 20d svr:\n",
      "  MSE: 0.00065486\n",
      "  RMSE: 0.025590\n",
      "  MAE: 0.025192\n",
      "  R: -31.3782\n",
      "  Directional Accuracy: 0.3063\n",
      "    - For actual positive returns: 0.0000\n",
      "    - For actual negative returns: 1.0000\n",
      "  Spearman Correlation: nan (p=nan)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3063\n",
      "    - Profit Factor: 0.4017\n",
      "    - Sharpe Ratio: -5.5023\n",
      "    - Mean Return: -0.001559\n",
      "\n",
      "Generating regime-aware ensemble predictions\n",
      "Creating regime-based ensemble predictions...\n",
      "  Test set contains 3 regimes: neutral, stressed, favorable\n",
      "  Regime model usage:\n",
      "    neutral: 973 samples (87.7%)\n",
      "    stressed: 116 samples (10.5%)\n",
      "    favorable: 21 samples (1.9%)\n",
      "  Prediction statistics: mean=-0.001342, std=0.003370\n",
      "  Prediction range: [-0.028013, 0.011459]\n",
      "\n",
      "Detailed evaluation for 20d regime_ensemble:\n",
      "  MSE: 0.00003719\n",
      "  RMSE: 0.006098\n",
      "  MAE: 0.004673\n",
      "  R: -0.8385\n",
      "  Directional Accuracy: 0.4477\n",
      "    - For actual positive returns: 0.3234\n",
      "    - For actual negative returns: 0.7294\n",
      "  Spearman Correlation: 0.0780 (p=0.0093)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.4477\n",
      "    - Profit Factor: 0.9697\n",
      "    - Sharpe Ratio: -0.1875\n",
      "    - Mean Return: -0.000056\n",
      "\n",
      "Generating model ensemble predictions (average of all models)\n",
      "\n",
      "Detailed evaluation for 20d model_ensemble:\n",
      "  MSE: 0.00010588\n",
      "  RMSE: 0.010290\n",
      "  MAE: 0.009155\n",
      "  R: -4.2350\n",
      "  Directional Accuracy: 0.3351\n",
      "    - For actual positive returns: 0.0896\n",
      "    - For actual negative returns: 0.8912\n",
      "  Spearman Correlation: 0.1503 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3351\n",
      "    - Profit Factor: 0.5635\n",
      "    - Sharpe Ratio: -3.4808\n",
      "    - Mean Return: -0.001020\n",
      "\n",
      "Best model for 20-day horizon: elastic\n",
      "  Directional Accuracy: 0.6937\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "## TRAINING MODELS FOR 60-DAY HORIZON ##\n",
      "----------------------------------------------------------------------\n",
      "Training traditional models...\n",
      "  Training Ridge regression...\n",
      "    Top 5 Ridge coefficients (absolute):\n",
      "      hy_ig_oas_ratio_90d: 0.043360\n",
      "      hy_ig_oas_ratio_60d: -0.026199\n",
      "      hy_ig_oas_ratio_120d: -0.018017\n",
      "      curve_slope_ewm10: -0.015969\n",
      "      cad_oas_ma120: 0.014132\n",
      "  Training ElasticNet...\n",
      "    ElasticNet selected 4 non-zero features\n",
      "    Top 5 ElasticNet coefficients (absolute):\n",
      "      cad_oas_ma120: 0.002125\n",
      "      us_ig_er_index_mom10: 0.000327\n",
      "      us_ig_er_index_mom5: 0.000259\n",
      "      us_ig_er_index_mom3: 0.000189\n",
      "  Training Random Forest...\n",
      "    Top 5 Random Forest important features:\n",
      "      er_vol_250d: 0.358361\n",
      "      hy_ig_oas_ratio_90d: 0.274470\n",
      "      cad_us_oas_diff_250d: 0.216161\n",
      "      vix_ma30: 0.080334\n",
      "      cad_oas_ma120: 0.027093\n",
      "  Training Gradient Boosting...\n",
      "    Top 5 Gradient Boosting important features:\n",
      "      er_vol_250d: 0.362934\n",
      "      hy_ig_oas_ratio_90d: 0.228802\n",
      "      cad_us_oas_diff_250d: 0.202615\n",
      "      cad_oas_ma120: 0.055195\n",
      "      vix_ma30: 0.050674\n",
      "  Training XGBoost...\n",
      "    Top 5 XGBoost important features:\n",
      "      er_vol_250d: 0.167701\n",
      "      hy_ig_oas_ratio_60d: 0.130355\n",
      "      cad_us_oas_diff_250d: 0.111614\n",
      "      hy_ig_oas_ratio_30d: 0.111210\n",
      "      regime_oas: 0.102144\n",
      "  Trained 5 traditional models\n",
      "Training regime-specific models...\n",
      "  Training models for 'neutral' regime (3555 samples)...\n",
      "    Training Ridge model for neutral regime...\n",
      "      Top 3 Ridge coefficients for neutral regime:\n",
      "        curve_slope_ewm10: -0.023647\n",
      "        curve_slope_ewm60: 0.021835\n",
      "        hy_ig_oas_ratio_90d: 0.018732\n",
      "    Training Random Forest model for neutral regime...\n",
      "      Top 3 Random Forest important features for neutral regime:\n",
      "        cad_us_oas_diff_250d: 0.546597\n",
      "        hy_ig_oas_ratio_90d: 0.218081\n",
      "        cad_oas_ma120: 0.083917\n",
      "  Training models for 'favorable' regime (102 samples)...\n",
      "    Training Ridge model for favorable regime...\n",
      "      Top 3 Ridge coefficients for favorable regime:\n",
      "        cad_er_rsi_14: 0.003228\n",
      "        cad_oas_ma120: 0.002732\n",
      "        us_ig_er_index_mom15: 0.002576\n",
      "    Training Random Forest model for favorable regime...\n",
      "      Top 3 Random Forest important features for favorable regime:\n",
      "        hy_ig_oas_ratio_120d: 0.218398\n",
      "        hy_ig_oas_ratio_90d: 0.151506\n",
      "        hy_ig_oas_ratio_60d: 0.139824\n",
      "  Training models for 'stressed' regime (779 samples)...\n",
      "    Training Ridge model for stressed regime...\n",
      "      Top 3 Ridge coefficients for stressed regime:\n",
      "        hy_ig_oas_ratio_90d: 0.046033\n",
      "        curve_slope_ewm60: -0.042520\n",
      "        curve_slope_ewm10: 0.026431\n",
      "    Training Random Forest model for stressed regime...\n",
      "      Top 3 Random Forest important features for stressed regime:\n",
      "        er_vol_250d: 0.618006\n",
      "        hy_ig_oas_ratio_60d: 0.145883\n",
      "        cad_oas_ma120: 0.117007\n",
      "  Trained 6 regime-specific models\n",
      "Training advanced models...\n",
      "  Training stacking ensemble model...\n",
      "    Stacking ensemble model trained successfully\n",
      "  Training neural network model...\n",
      "Iteration 1, loss = 0.03767246\n",
      "Validation score: -79.382309\n",
      "Iteration 2, loss = 0.00982398\n",
      "Validation score: -37.458261\n",
      "Iteration 3, loss = 0.00461871\n",
      "Validation score: -32.979266\n",
      "Iteration 4, loss = 0.00371633\n",
      "Validation score: -18.836369\n",
      "Iteration 5, loss = 0.00279181\n",
      "Validation score: -18.847569\n",
      "Iteration 6, loss = 0.00197645\n",
      "Validation score: -14.051653\n",
      "Iteration 7, loss = 0.00221355\n",
      "Validation score: -13.858903\n",
      "Iteration 8, loss = 0.00232769\n",
      "Validation score: -10.432705\n",
      "Iteration 9, loss = 0.00126833\n",
      "Validation score: -10.630727\n",
      "Iteration 10, loss = 0.00142041\n",
      "Validation score: -10.227643\n",
      "Iteration 11, loss = 0.00157050\n",
      "Validation score: -11.161152\n",
      "Iteration 12, loss = 0.00169477\n",
      "Validation score: -9.673328\n",
      "Iteration 13, loss = 0.00085057\n",
      "Validation score: -5.810945\n",
      "Iteration 14, loss = 0.00071959\n",
      "Validation score: -4.606992\n",
      "Iteration 15, loss = 0.00065806\n",
      "Validation score: -4.828885\n",
      "Iteration 16, loss = 0.00068823\n",
      "Validation score: -4.045283\n",
      "Iteration 17, loss = 0.00079090\n",
      "Validation score: -7.435278\n",
      "Iteration 18, loss = 0.00117412\n",
      "Validation score: -4.994755\n",
      "Iteration 19, loss = 0.00069652\n",
      "Validation score: -2.334596\n",
      "Iteration 20, loss = 0.00055491\n",
      "Validation score: -2.497966\n",
      "Iteration 21, loss = 0.00050994\n",
      "Validation score: -1.949170\n",
      "Iteration 22, loss = 0.00050761\n",
      "Validation score: -2.489224\n",
      "Iteration 23, loss = 0.00040640\n",
      "Validation score: -1.946392\n",
      "Iteration 24, loss = 0.00054096\n",
      "Validation score: -3.109835\n",
      "Iteration 25, loss = 0.00051308\n",
      "Validation score: -1.795364\n",
      "Iteration 26, loss = 0.00041079\n",
      "Validation score: -1.597655\n",
      "Iteration 27, loss = 0.00035540\n",
      "Validation score: -1.077426\n",
      "Iteration 28, loss = 0.00030732\n",
      "Validation score: -1.277155\n",
      "Iteration 29, loss = 0.00030482\n",
      "Validation score: -1.202886\n",
      "Iteration 30, loss = 0.00027996\n",
      "Validation score: -0.943755\n",
      "Iteration 31, loss = 0.00029614\n",
      "Validation score: -0.833061\n",
      "Iteration 32, loss = 0.00027450\n",
      "Validation score: -0.795140\n",
      "Iteration 33, loss = 0.00028408\n",
      "Validation score: -0.767326\n",
      "Iteration 34, loss = 0.00026070\n",
      "Validation score: -1.210635\n",
      "Iteration 35, loss = 0.00024315\n",
      "Validation score: -0.483172\n",
      "Iteration 36, loss = 0.00028782\n",
      "Validation score: -0.934014\n",
      "Iteration 37, loss = 0.00024563\n",
      "Validation score: -0.465969\n",
      "Iteration 38, loss = 0.00024646\n",
      "Validation score: -0.446801\n",
      "Iteration 39, loss = 0.00025836\n",
      "Validation score: -0.741771\n",
      "Iteration 40, loss = 0.00027889\n",
      "Validation score: -0.677816\n",
      "Iteration 41, loss = 0.00031808\n",
      "Validation score: -0.527655\n",
      "Iteration 42, loss = 0.00028025\n",
      "Validation score: -0.321358\n",
      "Iteration 43, loss = 0.00023131\n",
      "Validation score: -0.154245\n",
      "Iteration 44, loss = 0.00021416\n",
      "Validation score: -0.498641\n",
      "Iteration 45, loss = 0.00021689\n",
      "Validation score: -0.158986\n",
      "Iteration 46, loss = 0.00029061\n",
      "Validation score: -1.864422\n",
      "Iteration 47, loss = 0.00029743\n",
      "Validation score: 0.156784\n",
      "Iteration 48, loss = 0.00020301\n",
      "Validation score: -0.004256\n",
      "Iteration 49, loss = 0.00020914\n",
      "Validation score: 0.235067\n",
      "Iteration 50, loss = 0.00021444\n",
      "Validation score: -0.216871\n",
      "Iteration 51, loss = 0.00024767\n",
      "Validation score: 0.115341\n",
      "Iteration 52, loss = 0.00020191\n",
      "Validation score: 0.325223\n",
      "Iteration 53, loss = 0.00018195\n",
      "Validation score: 0.350817\n",
      "Iteration 54, loss = 0.00017216\n",
      "Validation score: 0.353756\n",
      "Iteration 55, loss = 0.00016786\n",
      "Validation score: 0.159884\n",
      "Iteration 56, loss = 0.00016038\n",
      "Validation score: 0.324225\n",
      "Iteration 57, loss = 0.00015644\n",
      "Validation score: 0.381567\n",
      "Iteration 58, loss = 0.00016202\n",
      "Validation score: 0.475542\n",
      "Iteration 59, loss = 0.00016739\n",
      "Validation score: 0.504340\n",
      "Iteration 60, loss = 0.00015601\n",
      "Validation score: 0.561773\n",
      "Iteration 61, loss = 0.00015134\n",
      "Validation score: 0.481774\n",
      "Iteration 62, loss = 0.00017113\n",
      "Validation score: 0.442798\n",
      "Iteration 63, loss = 0.00016096\n",
      "Validation score: 0.426708\n",
      "Iteration 64, loss = 0.00017298\n",
      "Validation score: 0.571651\n",
      "Iteration 65, loss = 0.00016418\n",
      "Validation score: 0.516344\n",
      "Iteration 66, loss = 0.00017342\n",
      "Validation score: 0.319300\n",
      "Iteration 67, loss = 0.00014549\n",
      "Validation score: 0.665646\n",
      "Iteration 68, loss = 0.00014675\n",
      "Validation score: 0.471661\n",
      "Iteration 69, loss = 0.00013664\n",
      "Validation score: 0.690785\n",
      "Iteration 70, loss = 0.00012909\n",
      "Validation score: 0.633563\n",
      "Iteration 71, loss = 0.00012825\n",
      "Validation score: 0.702621\n",
      "Iteration 72, loss = 0.00012292\n",
      "Validation score: 0.705094\n",
      "Iteration 73, loss = 0.00012660\n",
      "Validation score: 0.685297\n",
      "Iteration 74, loss = 0.00019230\n",
      "Validation score: -0.677244\n",
      "Iteration 75, loss = 0.00018597\n",
      "Validation score: 0.651267\n",
      "Iteration 76, loss = 0.00012038\n",
      "Validation score: 0.695597\n",
      "Iteration 77, loss = 0.00011116\n",
      "Validation score: 0.730365\n",
      "Iteration 78, loss = 0.00010608\n",
      "Validation score: 0.725598\n",
      "Iteration 79, loss = 0.00010915\n",
      "Validation score: 0.736640\n",
      "Iteration 80, loss = 0.00011448\n",
      "Validation score: 0.751643\n",
      "Iteration 81, loss = 0.00010831\n",
      "Validation score: 0.705635\n",
      "Iteration 82, loss = 0.00012391\n",
      "Validation score: 0.717544\n",
      "Iteration 83, loss = 0.00011235\n",
      "Validation score: 0.719201\n",
      "Iteration 84, loss = 0.00009797\n",
      "Validation score: 0.786850\n",
      "Iteration 85, loss = 0.00009107\n",
      "Validation score: 0.790347\n",
      "Iteration 86, loss = 0.00009023\n",
      "Validation score: 0.785795\n",
      "Iteration 87, loss = 0.00008567\n",
      "Validation score: 0.822245\n",
      "Iteration 88, loss = 0.00008409\n",
      "Validation score: 0.794269\n",
      "Iteration 89, loss = 0.00008408\n",
      "Validation score: 0.771949\n",
      "Iteration 90, loss = 0.00009127\n",
      "Validation score: 0.708833\n",
      "Iteration 91, loss = 0.00013647\n",
      "Validation score: 0.754338\n",
      "Iteration 92, loss = 0.00008742\n",
      "Validation score: 0.843780\n",
      "Iteration 93, loss = 0.00007807\n",
      "Validation score: 0.880667\n",
      "Iteration 94, loss = 0.00007485\n",
      "Validation score: 0.892810\n",
      "Iteration 95, loss = 0.00007228\n",
      "Validation score: 0.843334\n",
      "Iteration 96, loss = 0.00006983\n",
      "Validation score: 0.886963\n",
      "Iteration 97, loss = 0.00006810\n",
      "Validation score: 0.882052\n",
      "Iteration 98, loss = 0.00007282\n",
      "Validation score: 0.826737\n",
      "Iteration 99, loss = 0.00006959\n",
      "Validation score: 0.864658\n",
      "Iteration 100, loss = 0.00006350\n",
      "Validation score: 0.875918\n",
      "Iteration 101, loss = 0.00006175\n",
      "Validation score: 0.888206\n",
      "Iteration 102, loss = 0.00005960\n",
      "Validation score: 0.887034\n",
      "Iteration 103, loss = 0.00005729\n",
      "Validation score: 0.889042\n",
      "Iteration 104, loss = 0.00005707\n",
      "Validation score: 0.740110\n",
      "Iteration 105, loss = 0.00007133\n",
      "Validation score: 0.827654\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "    Neural network model trained successfully\n",
      "    Training iterations: 105\n",
      "    Final loss: 0.000071\n",
      "  Training SVR model...\n",
      "    SVR model trained successfully\n",
      "    Support vectors: 0\n",
      "  Trained 3 advanced models\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "## EVALUATING MODELS FOR 60-DAY HORIZON ##\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Generating predictions for model: ridge\n",
      "\n",
      "Detailed evaluation for 60d ridge:\n",
      "  MSE: 0.00010001\n",
      "  RMSE: 0.010000\n",
      "  MAE: 0.008175\n",
      "  R: -0.5527\n",
      "  Directional Accuracy: 0.8126\n",
      "    - For actual positive returns: 0.9859\n",
      "    - For actual negative returns: 0.2432\n",
      "  Spearman Correlation: 0.4230 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.8126\n",
      "    - Profit Factor: 9.5121\n",
      "    - Sharpe Ratio: 13.0431\n",
      "    - Mean Return: 0.005977\n",
      "\n",
      "Generating predictions for model: elastic\n",
      "\n",
      "Detailed evaluation for 60d elastic:\n",
      "  MSE: 0.00005949\n",
      "  RMSE: 0.007713\n",
      "  MAE: 0.005938\n",
      "  R: 0.0763\n",
      "  Directional Accuracy: 0.7658\n",
      "    - For actual positive returns: 0.9976\n",
      "    - For actual negative returns: 0.0039\n",
      "  Spearman Correlation: 0.3928 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.7658\n",
      "    - Profit Factor: 4.9665\n",
      "    - Sharpe Ratio: 9.6939\n",
      "    - Mean Return: 0.004907\n",
      "\n",
      "Generating predictions for model: rf\n",
      "\n",
      "Detailed evaluation for 60d rf:\n",
      "  MSE: 0.00366495\n",
      "  RMSE: 0.060539\n",
      "  MAE: 0.057743\n",
      "  R: -55.9010\n",
      "  Directional Accuracy: 0.3045\n",
      "    - For actual positive returns: 0.1128\n",
      "    - For actual negative returns: 0.9344\n",
      "  Spearman Correlation: 0.4166 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3045\n",
      "    - Profit Factor: 0.4720\n",
      "    - Sharpe Ratio: -4.6517\n",
      "    - Mean Return: -0.002648\n",
      "\n",
      "Generating predictions for model: gbr\n",
      "\n",
      "Detailed evaluation for 60d gbr:\n",
      "  MSE: 0.00143435\n",
      "  RMSE: 0.037873\n",
      "  MAE: 0.035743\n",
      "  R: -21.2693\n",
      "  Directional Accuracy: 0.3045\n",
      "    - For actual positive returns: 0.1128\n",
      "    - For actual negative returns: 0.9344\n",
      "  Spearman Correlation: -0.0361 (p=0.2297)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3045\n",
      "    - Profit Factor: 0.4720\n",
      "    - Sharpe Ratio: -4.6517\n",
      "    - Mean Return: -0.002648\n",
      "\n",
      "Generating predictions for model: xgb\n",
      "\n",
      "Detailed evaluation for 60d xgb:\n",
      "  MSE: 0.00103514\n",
      "  RMSE: 0.032174\n",
      "  MAE: 0.030310\n",
      "  R: -15.0713\n",
      "  Directional Accuracy: 0.3045\n",
      "    - For actual positive returns: 0.1128\n",
      "    - For actual negative returns: 0.9344\n",
      "  Spearman Correlation: 0.0751 (p=0.0123)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3045\n",
      "    - Profit Factor: 0.4720\n",
      "    - Sharpe Ratio: -4.6517\n",
      "    - Mean Return: -0.002648\n",
      "\n",
      "Generating predictions for model: stacking\n",
      "\n",
      "Detailed evaluation for 60d stacking:\n",
      "  MSE: 0.00008819\n",
      "  RMSE: 0.009391\n",
      "  MAE: 0.007640\n",
      "  R: -0.3693\n",
      "  Directional Accuracy: 0.3090\n",
      "    - For actual positive returns: 0.1187\n",
      "    - For actual negative returns: 0.9344\n",
      "  Spearman Correlation: 0.4423 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3090\n",
      "    - Profit Factor: 0.4949\n",
      "    - Sharpe Ratio: -4.3600\n",
      "    - Mean Return: -0.002494\n",
      "\n",
      "Generating predictions for model: nn\n",
      "\n",
      "Detailed evaluation for 60d nn:\n",
      "  MSE: 0.00101257\n",
      "  RMSE: 0.031821\n",
      "  MAE: 0.021374\n",
      "  R: -14.7209\n",
      "  Directional Accuracy: 0.5928\n",
      "    - For actual positive returns: 0.5746\n",
      "    - For actual negative returns: 0.6525\n",
      "  Spearman Correlation: 0.3219 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.5928\n",
      "    - Profit Factor: 2.0740\n",
      "    - Sharpe Ratio: 4.5206\n",
      "    - Mean Return: 0.002579\n",
      "\n",
      "Generating predictions for model: svr\n",
      "\n",
      "Detailed evaluation for 60d svr:\n",
      "  MSE: 0.00016497\n",
      "  RMSE: 0.012844\n",
      "  MAE: 0.011005\n",
      "  R: -1.5612\n",
      "  Directional Accuracy: 0.2333\n",
      "    - For actual positive returns: 0.0000\n",
      "    - For actual negative returns: 1.0000\n",
      "  Spearman Correlation: nan (p=nan)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.2333\n",
      "    - Profit Factor: 0.2004\n",
      "    - Sharpe Ratio: -9.7203\n",
      "    - Mean Return: -0.004916\n",
      "\n",
      "Generating regime-aware ensemble predictions\n",
      "Creating regime-based ensemble predictions...\n",
      "  Test set contains 3 regimes: neutral, stressed, favorable\n",
      "  Regime model usage:\n",
      "    neutral: 973 samples (87.7%)\n",
      "    stressed: 116 samples (10.5%)\n",
      "    favorable: 21 samples (1.9%)\n",
      "  Prediction statistics: mean=-0.006842, std=0.010378\n",
      "  Prediction range: [-0.032744, 0.033034]\n",
      "\n",
      "Detailed evaluation for 60d regime_ensemble:\n",
      "  MSE: 0.00028748\n",
      "  RMSE: 0.016955\n",
      "  MAE: 0.013737\n",
      "  R: -3.4633\n",
      "  Directional Accuracy: 0.3640\n",
      "    - For actual positive returns: 0.2021\n",
      "    - For actual negative returns: 0.8958\n",
      "  Spearman Correlation: 0.0164 (p=0.5847)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3640\n",
      "    - Profit Factor: 0.6860\n",
      "    - Sharpe Ratio: -2.3426\n",
      "    - Mean Return: -0.001375\n",
      "\n",
      "Generating model ensemble predictions (average of all models)\n",
      "\n",
      "Detailed evaluation for 60d model_ensemble:\n",
      "  MSE: 0.00041800\n",
      "  RMSE: 0.020445\n",
      "  MAE: 0.018652\n",
      "  R: -5.4898\n",
      "  Directional Accuracy: 0.3045\n",
      "    - For actual positive returns: 0.1128\n",
      "    - For actual negative returns: 0.9344\n",
      "  Spearman Correlation: 0.3788 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.3045\n",
      "    - Profit Factor: 0.4720\n",
      "    - Sharpe Ratio: -4.6517\n",
      "    - Mean Return: -0.002648\n",
      "\n",
      "Best model for 60-day horizon: ridge\n",
      "  Directional Accuracy: 0.8126\n",
      "\n",
      "Skipping LSTM model (Keras not available)\n",
      "\n",
      "================================================================================\n",
      "### CREATING ADAPTIVE MULTI-HORIZON ENSEMBLE ###\n",
      "================================================================================\n",
      "\n",
      "Best model for each horizon:\n",
      "  5-day horizon: gbr\n",
      "    Directional Accuracy: 0.6784\n",
      "    R: 0.0623\n",
      "    Profit Factor: 2.6921\n",
      "  10-day horizon: xgb\n",
      "    Directional Accuracy: 0.6748\n",
      "    R: -0.0873\n",
      "    Profit Factor: 2.6201\n",
      "  20-day horizon: elastic\n",
      "    Directional Accuracy: 0.6937\n",
      "    R: -0.0012\n",
      "    Profit Factor: 2.4649\n",
      "  60-day horizon: ridge\n",
      "    Directional Accuracy: 0.8126\n",
      "    R: -0.5527\n",
      "    Profit Factor: 9.5121\n",
      "Creating adaptive multi-horizon ensemble...\n",
      "\n",
      "Calculating model weights based on performance:\n",
      "  Using gbr for 5-day horizon\n",
      "    Directional accuracy: 0.6784\n",
      "    Raw weight: 0.1357\n",
      "  Using xgb for 10-day horizon\n",
      "    Directional accuracy: 0.6748\n",
      "    Raw weight: 0.0675\n",
      "  Using elastic for 20-day horizon\n",
      "    Directional accuracy: 0.6937\n",
      "    Raw weight: 0.0347\n",
      "  Using ridge for 60-day horizon\n",
      "    Directional accuracy: 0.8126\n",
      "    Raw weight: 0.0135\n",
      "\n",
      "Final model weights in ensemble:\n",
      "  5d_gbr: 0.5397 (54.0%)\n",
      "  10d_xgb: 0.2684 (26.8%)\n",
      "  20d_elastic: 0.1380 (13.8%)\n",
      "  60d_ridge: 0.0539 (5.4%)\n",
      "\n",
      "Ensemble prediction statistics:\n",
      "  Mean: 0.000888\n",
      "  Std Dev: 0.000959\n",
      "  Range: [-0.005643, 0.004064]\n",
      "  Positive predictions: 88.5%\n",
      "\n",
      "Adaptive ensemble weights:\n",
      "  5d_gbr: 0.5397\n",
      "  10d_xgb: 0.2684\n",
      "  20d_elastic: 0.1380\n",
      "  60d_ridge: 0.0539\n",
      "\n",
      "Evaluating adaptive multi-horizon ensemble:\n",
      "\n",
      "Detailed evaluation for Adaptive multi-horizon ensemble:\n",
      "  MSE: 0.00000352\n",
      "  RMSE: 0.001875\n",
      "  MAE: 0.001292\n",
      "  R: -0.0674\n",
      "  Directional Accuracy: 0.6883\n",
      "    - For actual positive returns: 0.9404\n",
      "    - For actual negative returns: 0.2191\n",
      "  Spearman Correlation: 0.2935 (p=0.0000)\n",
      "  Trading Performance:\n",
      "    - Win Rate: 0.6883\n",
      "    - Profit Factor: 2.8317\n",
      "    - Sharpe Ratio: 5.7349\n",
      "    - Mean Return: 0.000631\n",
      "\n",
      "================================================================================\n",
      "### FEATURE IMPORTANCE ANALYSIS ###\n",
      "================================================================================\n",
      "\n",
      "Top 15 features from Random Forest:\n",
      "  1. us_ig_er_index_mom10: 0.1698\n",
      "  2. er_vol_250d: 0.0937\n",
      "  3. us_hy_oas_mom10: 0.0784\n",
      "  4. vix_ewm_divergence: 0.0676\n",
      "  5. hy_ig_oas_ratio_250d: 0.0520\n",
      "  6. us_ig_oas_mom3: 0.0499\n",
      "  7. us_hy_oas_zscore_60d: 0.0471\n",
      "  8. vix_zscore_250d: 0.0417\n",
      "  9. us_ig_oas_mom10: 0.0367\n",
      "  10. hy_ig_oas_ratio_120d: 0.0343\n",
      "  11. us_ig_er_index_mom3: 0.0300\n",
      "  12. er_vol_5d: 0.0284\n",
      "  13. us_ig_oas_mom15: 0.0253\n",
      "  14. vix_ma30: 0.0250\n",
      "  15. us_hy_oas_zscore_250d: 0.0203\n",
      "\n",
      "Top 15 features from Gradient Boosting:\n",
      "  1. us_ig_er_index_mom10: 0.1289\n",
      "  2. er_vol_5d: 0.1223\n",
      "  3. cad_us_oas_diff_250d: 0.0946\n",
      "  4. vix_ewm_divergence: 0.0803\n",
      "  5. er_vol_250d: 0.0747\n",
      "  6. hy_ig_oas_ratio_250d: 0.0474\n",
      "  7. us_hy_oas_zscore_60d: 0.0384\n",
      "  8. us_ig_oas_mom10: 0.0333\n",
      "  9. hy_ig_oas_ratio_90d: 0.0277\n",
      "  10. us_ig_oas_mom15: 0.0271\n",
      "  11. er_trend_divergence: 0.0265\n",
      "  12. vix_ma30: 0.0227\n",
      "  13. curve_slope_ewm10: 0.0171\n",
      "  14. cad_er_rsi_14: 0.0164\n",
      "  15. hy_ig_oas_ratio_60d: 0.0160\n",
      "\n",
      "Top 15 features from XGBoost:\n",
      "  1. us_ig_oas_mom10: 0.0508\n",
      "  2. cad_us_oas_diff_250d: 0.0482\n",
      "  3. vix_zscore_250d: 0.0461\n",
      "  4. hy_ig_oas_ratio_120d: 0.0457\n",
      "  5. us_ig_oas_mom15: 0.0433\n",
      "  6. us_ig_er_index_mom10: 0.0416\n",
      "  7. er_vol_250d: 0.0393\n",
      "  8. cad_ig_er_index_mom10: 0.0369\n",
      "  9. hy_ig_oas_ratio_60d: 0.0364\n",
      "  10. hy_ig_oas_ratio_90d: 0.0318\n",
      "  11. regime_oas: 0.0315\n",
      "  12. cad_er_rsi_14: 0.0294\n",
      "  13. us_ig_er_index_mom5: 0.0271\n",
      "  14. us_hy_oas_zscore_60d: 0.0263\n",
      "  15. hy_ig_oas_ratio_250d: 0.0252\n",
      "\n",
      "Top 15 features from ElasticNet:\n",
      "  1. us_ig_er_index_mom10: 0.000000\n",
      "  2. cad_oas_mom60: -0.000000\n",
      "  3. er_vol_3d: -0.000000\n",
      "  4. vix_ewm30_std: -0.000000\n",
      "  5. cad_er_rsi_14: 0.000000\n",
      "  6. oas_rsi_14: -0.000000\n",
      "  7. vix_ewm10_std: -0.000000\n",
      "  8. er_vol_10d: -0.000000\n",
      "  9. cad_ig_er_index_mom10: 0.000000\n",
      "  10. cad_us_oas_diff_250d: -0.000000\n",
      "  11. us_hy_oas_ewm10_std: -0.000000\n",
      "  12. cad_ig_er_index_pct_lag3: 0.000000\n",
      "  13. vix_ma20: 0.000000\n",
      "  14. us_hy_oas_mom10: -0.000000\n",
      "  15. regime_oas: 0.000000\n",
      "\n",
      "Consensus feature importance (top 15):\n",
      "  1. us_ig_er_index_mom10: 0.1134 (RF: 0.1698, GB: 0.1289, XGB: 0.0416)\n",
      "  2. er_vol_250d: 0.0692 (RF: 0.0937, GB: 0.0747, XGB: 0.0393)\n",
      "  3. er_vol_5d: 0.0526 (RF: 0.0284, GB: 0.1223, XGB: 0.0069)\n",
      "  4. cad_us_oas_diff_250d: 0.0520 (RF: 0.0134, GB: 0.0946, XGB: 0.0482)\n",
      "  5. vix_ewm_divergence: 0.0513 (RF: 0.0676, GB: 0.0803, XGB: 0.0061)\n",
      "  6. hy_ig_oas_ratio_250d: 0.0415 (RF: 0.0520, GB: 0.0474, XGB: 0.0252)\n",
      "  7. us_ig_oas_mom10: 0.0403 (RF: 0.0367, GB: 0.0333, XGB: 0.0508)\n",
      "  8. us_hy_oas_mom10: 0.0381 (RF: 0.0784, GB: 0.0158, XGB: 0.0202)\n",
      "  9. us_hy_oas_zscore_60d: 0.0373 (RF: 0.0471, GB: 0.0384, XGB: 0.0263)\n",
      "  10. vix_zscore_250d: 0.0332 (RF: 0.0417, GB: 0.0120, XGB: 0.0461)\n",
      "  11. us_ig_oas_mom15: 0.0319 (RF: 0.0253, GB: 0.0271, XGB: 0.0433)\n",
      "  12. hy_ig_oas_ratio_120d: 0.0309 (RF: 0.0343, GB: 0.0128, XGB: 0.0457)\n",
      "  13. us_ig_oas_mom3: 0.0263 (RF: 0.0499, GB: 0.0150, XGB: 0.0139)\n",
      "  14. hy_ig_oas_ratio_90d: 0.0215 (RF: 0.0050, GB: 0.0277, XGB: 0.0318)\n",
      "  15. vix_ma30: 0.0208 (RF: 0.0250, GB: 0.0227, XGB: 0.0147)\n",
      "\n",
      "================================================================================\n",
      "### FINAL PERFORMANCE SUMMARY ###\n",
      "================================================================================\n",
      "\n",
      "Performance metrics for 5-day horizon models:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model                Dir. Acc.  R         RMSE       MAE        Profit Factor   Sharpe    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "ridge                0.5982     -0.1194    0.001921   0.001417   2.0726          4.0659    \n",
      "elastic              0.6505     -0.0105    0.001825   0.001272   1.8262          3.3721    \n",
      "rf                   0.6640     0.0963     0.001726   0.001206   2.8318          5.7351    \n",
      "gbr                  0.6784     0.0623     0.001758   0.001218   2.6921          5.4695    \n",
      "xgb                  0.6676     0.0927     0.001729   0.001204   3.0311          6.0891    \n",
      "stacking             0.6505     -0.0070    0.001822   0.001270   1.8262          3.3721    \n",
      "nn                   0.6694     -0.5127    0.002233   0.001632   2.9476          5.9442    \n",
      "svr                  0.3495     -29.1752   0.009972   0.009807   0.5476          -3.3721   \n",
      "regime_ensemble      0.6279     -0.1496    0.001946   0.001351   2.2092          4.4124    \n",
      "model_ensemble       0.3739     -0.3708    0.002125   0.001694   0.6630          -2.3107   \n",
      "adaptive_ensemble    0.6883     -0.0674    0.001875   0.001292   2.8317          5.7349    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Directional accuracy comparison across horizons:\n",
      "----------------------------------------------------------------------\n",
      "Model                5-day       10-day      20-day      60-day      \n",
      "----------------------------------------------------------------------\n",
      "ridge                0.5982      0.6000      0.6333      0.8126      \n",
      "elastic              0.6505      0.6676      0.6937      0.7658      \n",
      "rf                   0.6640      0.6685      0.3369      0.3045      \n",
      "xgb                  0.6676      0.6748      0.3369      0.3045      \n",
      "model_ensemble       0.3739      0.3450      0.3351      0.3045      \n",
      "regime_ensemble      0.6279      0.6027      0.4477      0.3640      \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Adaptive Multi-Horizon Ensemble Final Performance:\n",
      "R: -0.0674\n",
      "RMSE: 0.001875\n",
      "Directional Accuracy: 0.6883\n",
      "Profit Factor: 2.8317\n",
      "Sharpe Ratio: 5.7349\n",
      "\n",
      "================================================================================\n",
      "### EXECUTION COMPLETE ###\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Execution \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_models, all_predictions, adaptive_ensemble_preds, model_metrics = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tajana-0X0hpMbN-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
